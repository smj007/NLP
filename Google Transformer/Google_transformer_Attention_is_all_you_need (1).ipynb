{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Google transformer - Attention is all you need",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4e5_trC_LhX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import re\n",
        "import os\n",
        "import time\n",
        "from google.colab import drive\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQL1qk7v_-S8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQX5NGCwAbxx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "9a47c86c-36e6-4e08-f3fd-6f5b1d8cce82"
      },
      "source": [
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jptZOwMAxeb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open (\"/content/drive/My Drive/transformer/data/europarl-v7.fr-en.en\",\n",
        "           mode = \"r\",\n",
        "           encoding = \"utf-8\") as f:\n",
        "  europarl_en = f.read()\n",
        "\n",
        "with open (\"/content/drive/My Drive/transformer/data/europarl-v7.fr-en.fr\",\n",
        "           mode = \"r\",\n",
        "           encoding = \"utf-8\") as f:\n",
        "  europarl_fr = f.read()\n",
        "\n",
        "\n",
        "with open (\"/content/drive/My Drive/transformer/data/nonbreaking_prefix.en\",\n",
        "           mode = \"r\",\n",
        "           encoding = \"utf-8\") as f:\n",
        "  non_breaking_prefix_en = f.read()    \n",
        "\n",
        "\n",
        "with open (\"/content/drive/My Drive/transformer/data/nonbreaking_prefix.fr\",\n",
        "           mode = \"r\",\n",
        "           encoding = \"utf-8\") as f:\n",
        "  non_breaking_prefix_fr = f.read()  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7xSMtN1BwWW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "06ce9227-42d9-4733-e276-6c362e2fc0d7"
      },
      "source": [
        "europarl_en[:100]"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Resumption of the session\\nI declare resumed the session of the European Parliament adjourned on Frid'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6W-dF5EGtOG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "427e581a-f7bb-4af2-dd1a-fb3b721415f6"
      },
      "source": [
        "non_breaking_prefix_en[:5]"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'a\\nb\\nc'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_9ZGeMVylMl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bb3bb31e-7f14-41aa-98a4-4a08d6cc8861"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrfcAgg1B3ml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "non_breaking_prefix_en = non_breaking_prefix_en.split(\"\\n\")\n",
        "non_breaking_prefix_en = [' ' + pref + '.' for pref in non_breaking_prefix_en]\n",
        "non_breaking_prefix_fr = non_breaking_prefix_fr.split(\"\\n\")\n",
        "non_breaking_prefix_fr = [' ' + pref + '.' for pref in non_breaking_prefix_fr]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2M2vZbFKq9O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "62e9a57b-f8b8-47e6-a106-9c6583d03e74"
      },
      "source": [
        "non_breaking_prefix_en[:5]"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' a.', ' b.', ' c.', ' d.', ' e.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SBJr8asK46m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_en = europarl_en  #Here we distinguish the above NBPrefixes from the fullstops by adding a ### after the prefix's dot, and removing the .###\n",
        "for prefix in non_breaking_prefix_en:\n",
        "  corpus_en = corpus_en.replace(prefix, prefix + \"###\")\n",
        "corpus_en = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".###\" , corpus_en)  #In regex, whatever is in () are not removed, only occurences are searched, and | is for or\n",
        "corpus_en = re.sub(r\"\\.###\", '', corpus_en)\n",
        "corpus_en = re.sub(r\"  +\", ' ', corpus_en)\n",
        "corpus_en = corpus_en.split(\"\\n\")    #To make the huge list into a normal one with spaces\n",
        "\n",
        "\n",
        "corpus_fr = europarl_fr\n",
        "for prefix in non_breaking_prefix_fr:\n",
        "  corpus_fr = corpus_fr.replace(prefix, prefix + \"###\")\n",
        "corpus_fr = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".###\" , corpus_fr)\n",
        "corpus_fr = re.sub(r\"\\.###\", '', corpus_fr)\n",
        "corpus_fr = re.sub(r\"  +\", ' ', corpus_fr)\n",
        "corpus_fr = corpus_fr.split(\"\\n\")  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YMRBSppNgpO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    corpus_en, target_vocab_size=2**13)\n",
        "tokenizer_fr = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    corpus_fr, target_vocab_size=2**13)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVI6fO4_TTNy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCAB_SIZE_EN = tokenizer_en.vocab_size + 2 #2 extra for the start and end token\n",
        "VOCAB_SIZE_FR = tokenizer_fr.vocab_size + 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQ_iOle_TTR9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Prepare the inputs and target op for training like - <start> text <end>\n",
        "inputs = [[VOCAB_SIZE_EN - 2] + tokenizer_en.encode(sentence) + [VOCAB_SIZE_EN - 1]\n",
        "          for sentence in corpus_en]\n",
        "\n",
        "outputs = [[VOCAB_SIZE_FR - 2] + tokenizer_fr.encode(sentence) + [VOCAB_SIZE_FR - 1] \n",
        "           for sentence in corpus_fr]          "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pU4Q_f0UwCB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Removing extra long sentences to prevent delay in preprocessing of the corpus\n",
        "MAXLEN = 20\n",
        "\n",
        "idx_remove = [count for count, sentence in enumerate(inputs)\n",
        "              if len(sentence) > MAXLEN]  #If we pass the entire corpus at once, RAM may exhaust, hence pass one by one\n",
        "\n",
        "for idx in reversed(idx_remove): #if no reversal, one deletion will push the index forward by 1, we will delete the wrong words\n",
        "  del inputs[idx]   #Since the ip and op text need to correspond to each other, we need to truncate the same idx\n",
        "  del outputs[idx]    \n",
        "\n",
        "idx_remove = [count for count, sentence in enumerate(outputs)\n",
        "              if len(sentence) > MAXLEN]\n",
        "\n",
        "for idx in reversed(idx_remove):\n",
        "  del inputs[idx]\n",
        "  del outputs[idx]                          "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JglqakT2UwQt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Padding\n",
        "#0 is a value not used by the tokenizer, hence safe to pad\n",
        "#The algo used by the transformer (MASKING) will anyway not touch padded data\n",
        "\n",
        "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
        "                                                       value = 0,\n",
        "                                                       padding = 'post',\n",
        "                                                       maxlen = MAXLEN)\n",
        "\n",
        "outputs = tf.keras.preprocessing.sequence.pad_sequences(outputs,\n",
        "                                                        value = 0,\n",
        "                                                        padding = 'post',\n",
        "                                                        maxlen = MAXLEN)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4H3RPhRtZ_YW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 20000 #For shuffling acc to BufSize and then batching\n",
        "\n",
        "#Cache stores dataset in local storage, faster loading\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n",
        "dataset = dataset.cache()\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE) #This starts requesting data before the execution of stmt is initialized"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVOjWV_ygo5Q",
        "colab_type": "text"
      },
      "source": [
        "Positional encoding formulae:\n",
        "\n",
        "$PE_{(pos,2i)} =\\sin(pos/10000^{2i/dmodel})$\n",
        "\n",
        "$PE_{(pos,2i+1)} =\\cos(pos/10000^{2i/dmodel})$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EC2Zg13rZ_bo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionalEncoding(layers.Layer):\n",
        "  def __init__(self):\n",
        "    super(PositionalEncoding, self).__init__()  #This will initialize the Layer class properties and complete inheritance\n",
        "\n",
        "  def get_angles(self, pos, i, d_model):\n",
        "    #pos refers to the index positions of the sequence, to be encoded\n",
        "    #embedding size of model is dmodel, specific iterable denoted by i\n",
        "    # pos: (seq_length, 1) and #i : (1, d_model)\n",
        "\n",
        "    angles = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model)) #// takes care of even and odd formula\n",
        "    return pos * angles #shape is seq_length, d_model\n",
        "\n",
        "  def call(self, inputs):\n",
        "      #to return inputs + encoding\n",
        "      seq_length = inputs.shape.as_list()[-2]\n",
        "      d_model = inputs.shape.as_list()[-1]\n",
        "      angles = self.get_angles(np.arange(seq_length)[:, np.newaxis], \n",
        "                               np.arange(d_model)[np.newaxis, :],\n",
        "                               d_model)   #adding new dims to get # pos: (seq_length, 1) and #i : (1, d_model)\n",
        "      angles[:, 0::2] = np.sin(angles[:, 0::2]) #from 0, step 2 - even\n",
        "      angles[:, 1::2] = np.cos(angles[:, 1::2]) #from 1, step 2 - odd\n",
        "      pos_encoding = angles[np.newaxis, ...] #This is for a batch dimension\n",
        "      return inputs + tf.cast(pos_encoding, tf.float32)  #This is to cast as a tensor object\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKYcIekWHzdF",
        "colab_type": "text"
      },
      "source": [
        "$Attention(Q, K, V ) = \\text{softmax}\\left(\\dfrac{QK^T}{\\sqrt{d_k}}\\right)V $"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdrjUHdCHnVg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scaled_dot_product_attention(queries, keys, values, mask):\n",
        "  product = tf.matmul(queries, keys, transpose_b = True)\n",
        "  keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32)  #The last axis corresponds to the embedding dimension\n",
        "  scaled_product = product / tf.math.sqrt(keys_dim)\n",
        "\n",
        "  if mask is not None:\n",
        "    scaled_product += (mask * -1e9)  #if the value of the scaled product is zero, it will not have any effect on final ans (padded zeros) * -infinite gives 0 after softmax (NO CONTRIBUTION)\n",
        "\n",
        "  attention = tf.matmul(tf.nn.softmax(scaled_product, axis = -1), values)  #Softmax applied across the last dimension which corr to weights of V, has to sum up to 1 (final op matrix)\n",
        "\n",
        "  return attention"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aH5-FhKxcUVs",
        "colab_type": "text"
      },
      "source": [
        "### Multi-head attention sublayer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ja7WiPHoHnck",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttention(layers.Layer):\n",
        "\n",
        "  def __init__(self, nb_proj):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.nb_proj = nb_proj\n",
        "\n",
        "  def build(self, input_shape): #Input shape has all required dims, build is a function called when the MHA layer is called the first time, as we need d_model for the linear layers which is not accessible from init, as the layer class attributes are called, so build is used\n",
        "    self.d_model = input_shape[-1]\n",
        "    assert self.d_model % self.nb_proj == 0 #make sure it is divisible into subspaces\n",
        "\n",
        "    self.d_proj = self.d_model // self.nb_proj\n",
        "\n",
        "    self.query_lin = layers.Dense(units = self.d_model)\n",
        "    self.key_lin = layers.Dense(units = self.d_model)\n",
        "    self.value_lin = layers.Dense(units = self.d_model)\n",
        "\n",
        "    self.final_lin = layers.Dense(units = self.d_model)\n",
        "\n",
        "\n",
        "\n",
        "  def split_proj(self, inputs, batch_size): # inputs: (batch_size, seq_length, d_model)\n",
        "    shape = (batch_size,\n",
        "             -1,\n",
        "             self.nb_proj,\n",
        "             self.d_proj)\n",
        "    split_inputs = tf.reshape(inputs, shape = shape)  # (batch_size, seq_length, nb_proj, d_proj)\n",
        "    return tf.transpose(split_inputs, perm = [0, 2, 1, 3]) # (batch_size, nb_proj, seq_length, d_proj)\n",
        "\n",
        "\n",
        "  def call(self, queries, keys, values, mask):\n",
        "\n",
        "    batch_size = tf.shape(queries)[0]\n",
        "    queries = self.query_lin(queries) \n",
        "    keys = self.key_lin(keys)\n",
        "    values = self.value_lin(values) \n",
        "\n",
        "    queries = self.split_proj(queries, batch_size)\n",
        "    keys = self.split_proj(keys, batch_size)\n",
        "    values = self.split_proj(values, batch_size)\n",
        "\n",
        "    attention = scaled_dot_product_attention(queries, keys, values, mask)\n",
        "\n",
        "    #Recombine back into original space dims\n",
        "\n",
        "    attention = tf.transpose(attention, perm = [0, 2, 1, 3])\n",
        "\n",
        "    concat_attention = tf.reshape(attention,\n",
        "                                  shape = (batch_size, -1, self.d_model))\n",
        "    \n",
        "    outputs = self.final_lin(concat_attention)\n",
        "\n",
        "    return outputs\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ny0nXUp-cPU-",
        "colab_type": "text"
      },
      "source": [
        "## Encoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49gg9SBCcoyJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(layers.Layer):\n",
        "    \n",
        "    def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
        "      super(EncoderLayer, self).__init__()\n",
        "      self.FFN_units = FFN_units\n",
        "      self.nb_proj = nb_proj\n",
        "      self.dropout_rate = dropout_rate\n",
        "\n",
        "    def build(self, input_shape):  \n",
        "      self.d_model = input_shape[-1]\n",
        "\n",
        "      self.multi_head_attention = MultiHeadAttention(self.nb_proj)\n",
        "      self.dropout_1 = layers.Dropout(rate = self.dropout_rate)\n",
        "      self.normal_1 = layers.LayerNormalization(epsilon = 1e-6)\n",
        "      self.dense_1 = layers.Dense(units = self.FFN_units, activation = 'relu')\n",
        "      self.dense_2 = layers.Dense(units = self.d_model)\n",
        "      self.dropout_2 = layers.Dropout(rate = self.dropout_rate)\n",
        "      self.normal_2 = layers.LayerNormalization(epsilon = 1e-6)\n",
        "\n",
        "    def call(self, inputs, mask, training):\n",
        "      attention = self.multi_head_attention(inputs, inputs, inputs, mask)\n",
        "      attention = self.dropout_1(attention, training = training)\n",
        "      attention = self.normal_1(attention + inputs) #Residual connection to ease backprop\n",
        "\n",
        "      outputs = self.dense_1(attention)\n",
        "      outputs = self.dense_2(outputs)\n",
        "      outputs = self.dropout_2(outputs, training = training)\n",
        "      outputs = self.normal_2(outputs + attention)\n",
        "\n",
        "      return outputs\n",
        "\n",
        "      #How to build this : first define the call structure as per the process\n",
        "      #then go back to build and define each layer one by one\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyr8PTOAg4Me",
        "colab_type": "text"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "58qWJ8kecvKz",
        "colab": {}
      },
      "source": [
        "class Encoder(layers.Layer):\n",
        "\n",
        "  def __init__(self, \n",
        "               nb_layers,\n",
        "               FFN_units,\n",
        "               nb_proj,\n",
        "               dropout_rate,\n",
        "               vocab_size,\n",
        "               d_model,\n",
        "               name = \"encoder\"\n",
        "               ):\n",
        "    super(Encoder, self).__init__(name = name)\n",
        "    self.nb_layers = nb_layers\n",
        "    self.d_model = d_model\n",
        "\n",
        "    self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "    self.pos_encoding = PositionalEncoding()\n",
        "    self.dropout = layers.Dropout(rate = dropout_rate)\n",
        "    self.enc_layers = [EncoderLayer(FFN_units, nb_proj, dropout_rate)\n",
        "                      for _ in range(nb_layers)]\n",
        "  \n",
        "\n",
        "  def call(self, inputs, mask, training):\n",
        "    outputs = self.embedding(inputs)\n",
        "    outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    outputs = self.pos_encoding(outputs)\n",
        "    outputs = self.dropout(outputs, training)\n",
        "\n",
        "    for i in range(self.nb_layers):\n",
        "      outputs = self.enc_layers[i](outputs, mask, training)\n",
        "\n",
        "\n",
        "    return outputs  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCxekOO6omIv",
        "colab_type": "text"
      },
      "source": [
        "## Decoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UAZ1RwSHnjK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderLayer(layers.Layer):\n",
        "\n",
        "  def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "    self.FFN_units = FFN_units\n",
        "    self.nb_proj = nb_proj\n",
        "    self.dropout_rate = dropout_rate\n",
        "\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.d_model = input_shape[-1]\n",
        "\n",
        "    #Self multi-head attention\n",
        "    self.multi_head_attention_1 = MultiHeadAttention(self.nb_proj)\n",
        "    self.dropout_1 = layers.Dropout(rate = self.dropout_rate)\n",
        "    self.normal_1 = layers.LayerNormalization(epsilon = 1e-6)\n",
        "\n",
        "    # Multi head attention combined with encoder output    \n",
        "    self.multi_head_attention_2 = MultiHeadAttention(self.nb_proj)\n",
        "    self.dropout_2 = layers.Dropout(rate = self.dropout_rate)\n",
        "    self.normal_2 = layers.LayerNormalization(epsilon = 1e-6)  \n",
        "\n",
        "    #FFN\n",
        "    self.dense_1 = layers.Dense(units = self.FFN_units, activation = 'relu')\n",
        "    self.dense_2 = layers.Dense(units = self.d_model) \n",
        "    self.dropout_3 = layers.Dropout(rate = self.dropout_rate)\n",
        "    self.normal_3 = layers.LayerNormalization(epsilon = 1e-6)\n",
        "\n",
        "\n",
        "  def call(self, inputs, enc_outputs, mask_1, mask_2, training):  #bool train is to apply dropout or not\n",
        "    attention = self.multi_head_attention_1(inputs,\n",
        "                                            inputs,\n",
        "                                            inputs,  \n",
        "                                            mask_1)  #ip, ip, ip are queries, keys and values respectively\n",
        "    attention = self.dropout_1(attention, training)\n",
        "    attention = self.normal_1(attention + inputs)\n",
        "\n",
        "    attention_2 = self.multi_head_attention_2(attention,\n",
        "                                              enc_outputs, \n",
        "                                              enc_outputs,\n",
        "                                              mask_2)\n",
        "    attention_2 = self.dropout_2(attention_2, training)\n",
        "    attention_2 = self.normal_2(attention_2 + attention)\n",
        "\n",
        "    outputs = self.dense_1(attention_2)\n",
        "    outputs = self.dense_2(outputs)\n",
        "    outputs = self.dropout_3(outputs, training)\n",
        "    outputs = self.normal_3(outputs + attention_2)\n",
        "\n",
        "    return outputs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUq6XlkRuii8",
        "colab_type": "text"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RE7dFR_3uiGh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(layers.Layer):\n",
        "\n",
        "  def __init__(self, \n",
        "               nb_layers,\n",
        "               FFN_units,\n",
        "               nb_proj,\n",
        "               dropout_rate,\n",
        "               vocab_size,\n",
        "               d_model,\n",
        "               name = \"decoder\"):\n",
        "    super(Decoder, self).__init__(name = name)\n",
        "    self.d_model = d_model\n",
        "    self.nb_layers = nb_layers\n",
        "\n",
        "    self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "    self.pos_encoding = PositionalEncoding()\n",
        "    self.dropout = layers.Dropout(rate = dropout_rate)\n",
        "\n",
        "    self.dec_layers = [DecoderLayer(FFN_units,\n",
        "                                    nb_proj,\n",
        "                                    dropout_rate)\n",
        "                      for _ in range(nb_layers)]\n",
        "\n",
        "    \n",
        "  def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "    outputs = self.embedding(inputs)\n",
        "    outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    outputs = self.pos_encoding(outputs)\n",
        "    outputs = self.dropout(outputs, training)\n",
        "\n",
        "    for i in range(self.nb_layers):\n",
        "      outputs = self.dec_layers[i](outputs,\n",
        "                                   enc_outputs,\n",
        "                                   mask_1,\n",
        "                                   mask_2,\n",
        "                                   training)\n",
        "\n",
        "      return outputs  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRJQbTrHxI0J",
        "colab_type": "text"
      },
      "source": [
        "## Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEV31AvNcuRp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Here we also define the 2 masks used\n",
        "\n",
        "class Transformer(tf.keras.Model):\n",
        "\n",
        "  def __init__(self,\n",
        "               vocab_size_enc,\n",
        "               vocab_size_dec,\n",
        "               d_model,\n",
        "               nb_layers,\n",
        "               FFN_units,\n",
        "               nb_proj,\n",
        "               dropout_rate,\n",
        "               name = \"transformer\"):\n",
        "    super(Transformer, self).__init__(name = name)\n",
        "\n",
        "    self.encoder = Encoder(nb_layers,\n",
        "                           FFN_units,\n",
        "                           nb_proj,\n",
        "                           dropout_rate,\n",
        "                           vocab_size_enc,\n",
        "                           d_model)\n",
        "    \n",
        "    self.decoder = Decoder(nb_layers,\n",
        "                           FFN_units,\n",
        "                           nb_proj,\n",
        "                           dropout_rate,\n",
        "                           vocab_size_dec,\n",
        "                           d_model)    \n",
        "    \n",
        "    self.last_linear = layers.Dense(units = vocab_size_dec, name = \"lin_output\")\n",
        "\n",
        "  def create_padding_mask(self, seq):  #Seq here is tokenized, each word is a num, its not embedded yet\n",
        "\n",
        "    mask = tf.cast(tf.math.equal(seq, 0), tf.float32) #check for zeros for the padding token and place the zero for the mask there\n",
        "    return mask[:, tf.newaxis, tf.newaxis, :]   #first newaxis is for mask to be applied on each projection (dim = nb_proj) in the attention layer\n",
        "                                                #second newaxis is a broadcasting dimension, if mask is a row vector , the dim before it is empty, hence it broadcasts and we get the required masking matrix\n",
        "\n",
        "  def create_lookahead_mask(self, seq): #This mask is to prevent access of a future word j by word i, during train we provide the full word to the trans, but to predict a word n, it should not look beyond word n\n",
        "\n",
        "    seq_len = tf.shape(seq)[1]\n",
        "    lookahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0) #the -1 is the arg position for i < j, not used, and the 0 arg is for i > j, which is filled with 1 to get a UTMatrix (left)                                              \n",
        "                                                                                 #we convert this to a strict UT Right matrix so that word i does not have access to a future word j   \n",
        "    return lookahead_mask #tf.max will intelligently broadcast dims to the 4dim tensor\n",
        "\n",
        "  def call(self, enc_inputs, dec_inputs, training):\n",
        "    enc_mask = self.create_padding_mask(enc_inputs)\n",
        "    dec_mask_1 = tf.maximum(\n",
        "        self.create_padding_mask(dec_inputs),\n",
        "        self.create_lookahead_mask(dec_inputs)\n",
        "    )\n",
        "\n",
        "    #NOTE - a mask applied is a value of 1, We need to block the word i from seeing the future word j, so we use a URT matrix, where each 1 in the j > i part shows that the value is NOT CONSIDERED\n",
        "    #there are 3 matrices, Q, K and V\n",
        "    #Q is a matrix with the french translation in training phase\n",
        "    # here K = V, but the product between K and Q is a similarity norm, to find the dependance on output french words on the input\n",
        "    #This product acts as weights after softmax, for the V matrix, which is to simply reorganize all the text in V (english) for a better fit for the french translation\n",
        "    #This product is like a GLOBAL matrix which provides attention to the required input parts\n",
        "\n",
        "    dec_mask_2 = self.create_padding_mask(enc_inputs)  #For this mask, it is a mask applied to check the similarity between the french word matrix Q and english word matrix to group together the english sentence in such a way that the meaningful english words, wrt the french context word, is re-organized for a better fit\n",
        "                                                       #The shape of the output - a zero is applied by the mask wherever matrix A (english K) is zero due to the padding, and hence if n zeros, last n terms in Q are not touched (decoder_dims), hence enc_input matrix decides this mask\n",
        "    enc_outputs = self.encoder(enc_inputs, enc_mask, training)\n",
        "    dec_outputs = self.decoder(dec_inputs,\n",
        "                               enc_outputs,\n",
        "                               dec_mask_1,\n",
        "                               dec_mask_2,\n",
        "                               training)\n",
        "    \n",
        "    outputs = self.last_linear(dec_outputs)\n",
        "\n",
        "    return outputs\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tui9jVY5dt0z",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4JGlwEQcuTv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Hyper-parameters\n",
        "D_MODEL = 128 # 512  These second values are those used in the Google Research paper\n",
        "NB_LAYERS = 4 # 6 - Reduce all to ease computation\n",
        "FFN_UNITS = 512 # 2048\n",
        "NB_PROJ = 8 # 8\n",
        "DROPOUT_RATE = 0.1 # 0.1\n",
        "\n",
        "transformer = Transformer(vocab_size_enc=VOCAB_SIZE_EN,\n",
        "                          vocab_size_dec=VOCAB_SIZE_FR,\n",
        "                          d_model=D_MODEL,\n",
        "                          nb_layers=NB_LAYERS,\n",
        "                          FFN_units=FFN_UNITS,\n",
        "                          nb_proj=NB_PROJ,\n",
        "                          dropout_rate=DROPOUT_RATE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfvDqO13cuYG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True,\n",
        "                                                            reduction = \"none\") #indicating a softmax prediction and not to sum losses and reduce, keep each loss value instead\n",
        "\n",
        "def loss_function(target, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(target, 0))  \n",
        "  loss_ = loss_object(target, pred)   #This is to mask out all the padded zeros, to not evaluate those in the loss function\n",
        "  mask = tf.cast(mask, dtype = loss_.dtype)\n",
        "  loss_ *= mask  #calc each term in the loss function and mask out the padded terms\n",
        "\n",
        "  return tf.reduce_mean(loss_)\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name = \"train_loss\")\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5n54F--qNtS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This is the custom learning rate to be defined\n",
        "\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "\n",
        "  def __init__(self, d_model, warmup_steps = 4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "\n",
        "    self.d_model = tf.cast(d_model, tf.float32)\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)  \n",
        "\n",
        "learning_rate = CustomSchedule(D_MODEL)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
        "                                     beta_1 = 0.9,\n",
        "                                     beta_2 = 0.98,\n",
        "                                     epsilon = 1e-9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzToun1AqNj2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7a8d163d-a1aa-4d50-c7be-8588bf872bb1"
      },
      "source": [
        "checkpoint_path = \"./drive/My Drive/transformer/ckpt/\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer = transformer,\n",
        "                           optimizer = optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep = 5)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print(\"Latest Checkpoint restored!\")"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Latest Checkpoint restored!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9iKiX_1qNXG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "35163442-faa0-49f7-f072-2132059093dc"
      },
      "source": [
        "EPOCHS = 8\n",
        "for epoch in range(EPOCHS):\n",
        "  print(\"Start of epocch {}\".format(epoch + 1))\n",
        "  start = time.time()\n",
        "\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "\n",
        "  for (batch, (enc_inputs, targets)) in enumerate(dataset):\n",
        "    dec_inputs = targets[:, :-1]\n",
        "    dec_outputs_real = targets[:, 1:]\n",
        "    with tf.GradientTape() as tape:\n",
        "      predictions = transformer(enc_inputs, dec_inputs, True)\n",
        "      loss = loss_function(dec_outputs_real, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "    train_loss(loss)\n",
        "    train_accuracy(dec_outputs_real, predictions)\n",
        "\n",
        "    if batch % 50 == 0:\n",
        "      print(\"Epoch {} Batch {} Loss {:.4f} Accuracy {:.4}\".format(\n",
        "          epoch + 1, batch, train_loss.result(), train_accuracy.result()\n",
        "      )) \n",
        "\n",
        "  ckpt_save_path = ckpt_manager.save()\n",
        "  print(\"Saving checkpoint for epoch {} at {}\".format(epoch+1,\n",
        "                                                        ckpt_save_path))\n",
        "  print(\"Time taken for 1 epoch: {} secs\\n\".format(time.time() - start))   \n",
        "\n",
        "\n"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start of epocch 1\n",
            "Epoch 1 Batch 0 Loss 2.8019 Accuracy 0.2508\n",
            "Epoch 1 Batch 50 Loss 2.2186 Accuracy 0.2947\n",
            "Epoch 1 Batch 100 Loss 2.0657 Accuracy 0.3154\n",
            "Epoch 1 Batch 150 Loss 1.9729 Accuracy 0.3268\n",
            "Epoch 1 Batch 200 Loss 1.9109 Accuracy 0.3377\n",
            "Epoch 1 Batch 250 Loss 1.8662 Accuracy 0.3438\n",
            "Epoch 1 Batch 300 Loss 1.8287 Accuracy 0.3496\n",
            "Epoch 1 Batch 350 Loss 1.7995 Accuracy 0.3545\n",
            "Epoch 1 Batch 400 Loss 1.7659 Accuracy 0.3587\n",
            "Epoch 1 Batch 450 Loss 1.7400 Accuracy 0.3622\n",
            "Epoch 1 Batch 500 Loss 1.7191 Accuracy 0.3655\n",
            "Epoch 1 Batch 550 Loss 1.7007 Accuracy 0.3681\n",
            "Epoch 1 Batch 600 Loss 1.6860 Accuracy 0.3708\n",
            "Epoch 1 Batch 650 Loss 1.6687 Accuracy 0.3735\n",
            "Epoch 1 Batch 700 Loss 1.6549 Accuracy 0.376\n",
            "Epoch 1 Batch 750 Loss 1.6429 Accuracy 0.3781\n",
            "Epoch 1 Batch 800 Loss 1.6300 Accuracy 0.3801\n",
            "Epoch 1 Batch 850 Loss 1.6189 Accuracy 0.382\n",
            "Epoch 1 Batch 900 Loss 1.6080 Accuracy 0.3837\n",
            "Epoch 1 Batch 950 Loss 1.5983 Accuracy 0.3849\n",
            "Epoch 1 Batch 1000 Loss 1.5893 Accuracy 0.3861\n",
            "Epoch 1 Batch 1050 Loss 1.5814 Accuracy 0.387\n",
            "Epoch 1 Batch 1100 Loss 1.5737 Accuracy 0.3883\n",
            "Epoch 1 Batch 1150 Loss 1.5664 Accuracy 0.3894\n",
            "Epoch 1 Batch 1200 Loss 1.5589 Accuracy 0.3904\n",
            "Epoch 1 Batch 1250 Loss 1.5506 Accuracy 0.3915\n",
            "Epoch 1 Batch 1300 Loss 1.5425 Accuracy 0.3928\n",
            "Epoch 1 Batch 1350 Loss 1.5347 Accuracy 0.3942\n",
            "Epoch 1 Batch 1400 Loss 1.5277 Accuracy 0.3956\n",
            "Epoch 1 Batch 1450 Loss 1.5206 Accuracy 0.3969\n",
            "Epoch 1 Batch 1500 Loss 1.5141 Accuracy 0.3983\n",
            "Epoch 1 Batch 1550 Loss 1.5078 Accuracy 0.3998\n",
            "Epoch 1 Batch 1600 Loss 1.5021 Accuracy 0.4013\n",
            "Epoch 1 Batch 1650 Loss 1.4951 Accuracy 0.4026\n",
            "Epoch 1 Batch 1700 Loss 1.4879 Accuracy 0.404\n",
            "Epoch 1 Batch 1750 Loss 1.4820 Accuracy 0.4054\n",
            "Epoch 1 Batch 1800 Loss 1.4765 Accuracy 0.4067\n",
            "Epoch 1 Batch 1850 Loss 1.4705 Accuracy 0.4081\n",
            "Epoch 1 Batch 1900 Loss 1.4651 Accuracy 0.4092\n",
            "Epoch 1 Batch 1950 Loss 1.4593 Accuracy 0.4106\n",
            "Epoch 1 Batch 2000 Loss 1.4541 Accuracy 0.4117\n",
            "Epoch 1 Batch 2050 Loss 1.4493 Accuracy 0.4125\n",
            "Epoch 1 Batch 2100 Loss 1.4438 Accuracy 0.4134\n",
            "Epoch 1 Batch 2150 Loss 1.4383 Accuracy 0.4143\n",
            "Epoch 1 Batch 2200 Loss 1.4319 Accuracy 0.4151\n",
            "Epoch 1 Batch 2250 Loss 1.4257 Accuracy 0.4158\n",
            "Epoch 1 Batch 2300 Loss 1.4199 Accuracy 0.4165\n",
            "Epoch 1 Batch 2350 Loss 1.4141 Accuracy 0.4173\n",
            "Epoch 1 Batch 2400 Loss 1.4084 Accuracy 0.4179\n",
            "Epoch 1 Batch 2450 Loss 1.4025 Accuracy 0.4186\n",
            "Epoch 1 Batch 2500 Loss 1.3968 Accuracy 0.4192\n",
            "Epoch 1 Batch 2550 Loss 1.3917 Accuracy 0.42\n",
            "Epoch 1 Batch 2600 Loss 1.3866 Accuracy 0.4208\n",
            "Epoch 1 Batch 2650 Loss 1.3809 Accuracy 0.4215\n",
            "Epoch 1 Batch 2700 Loss 1.3760 Accuracy 0.4223\n",
            "Epoch 1 Batch 2750 Loss 1.3713 Accuracy 0.423\n",
            "Epoch 1 Batch 2800 Loss 1.3667 Accuracy 0.4237\n",
            "Epoch 1 Batch 2850 Loss 1.3623 Accuracy 0.4244\n",
            "Epoch 1 Batch 2900 Loss 1.3579 Accuracy 0.425\n",
            "Epoch 1 Batch 2950 Loss 1.3533 Accuracy 0.4257\n",
            "Epoch 1 Batch 3000 Loss 1.3493 Accuracy 0.4263\n",
            "Epoch 1 Batch 3050 Loss 1.3453 Accuracy 0.427\n",
            "Epoch 1 Batch 3100 Loss 1.3411 Accuracy 0.4275\n",
            "Epoch 1 Batch 3150 Loss 1.3372 Accuracy 0.4282\n",
            "Epoch 1 Batch 3200 Loss 1.3333 Accuracy 0.4287\n",
            "Epoch 1 Batch 3250 Loss 1.3293 Accuracy 0.4293\n",
            "Epoch 1 Batch 3300 Loss 1.3252 Accuracy 0.4299\n",
            "Epoch 1 Batch 3350 Loss 1.3211 Accuracy 0.4305\n",
            "Epoch 1 Batch 3400 Loss 1.3174 Accuracy 0.4311\n",
            "Epoch 1 Batch 3450 Loss 1.3138 Accuracy 0.4316\n",
            "Epoch 1 Batch 3500 Loss 1.3104 Accuracy 0.4322\n",
            "Epoch 1 Batch 3550 Loss 1.3071 Accuracy 0.4329\n",
            "Epoch 1 Batch 3600 Loss 1.3037 Accuracy 0.4335\n",
            "Epoch 1 Batch 3650 Loss 1.3004 Accuracy 0.434\n",
            "Epoch 1 Batch 3700 Loss 1.2973 Accuracy 0.4347\n",
            "Epoch 1 Batch 3750 Loss 1.2939 Accuracy 0.4353\n",
            "Epoch 1 Batch 3800 Loss 1.2907 Accuracy 0.4359\n",
            "Epoch 1 Batch 3850 Loss 1.2879 Accuracy 0.4364\n",
            "Epoch 1 Batch 3900 Loss 1.2850 Accuracy 0.437\n",
            "Epoch 1 Batch 3950 Loss 1.2823 Accuracy 0.4376\n",
            "Epoch 1 Batch 4000 Loss 1.2795 Accuracy 0.4381\n",
            "Epoch 1 Batch 4050 Loss 1.2768 Accuracy 0.4386\n",
            "Epoch 1 Batch 4100 Loss 1.2744 Accuracy 0.4391\n",
            "Epoch 1 Batch 4150 Loss 1.2726 Accuracy 0.4394\n",
            "Epoch 1 Batch 4200 Loss 1.2715 Accuracy 0.4397\n",
            "Epoch 1 Batch 4250 Loss 1.2708 Accuracy 0.44\n",
            "Epoch 1 Batch 4300 Loss 1.2702 Accuracy 0.4401\n",
            "Epoch 1 Batch 4350 Loss 1.2700 Accuracy 0.4402\n",
            "Epoch 1 Batch 4400 Loss 1.2697 Accuracy 0.4403\n",
            "Epoch 1 Batch 4450 Loss 1.2697 Accuracy 0.4403\n",
            "Epoch 1 Batch 4500 Loss 1.2699 Accuracy 0.4403\n",
            "Epoch 1 Batch 4550 Loss 1.2701 Accuracy 0.4403\n",
            "Epoch 1 Batch 4600 Loss 1.2705 Accuracy 0.4403\n",
            "Epoch 1 Batch 4650 Loss 1.2707 Accuracy 0.4403\n",
            "Epoch 1 Batch 4700 Loss 1.2708 Accuracy 0.4403\n",
            "Epoch 1 Batch 4750 Loss 1.2709 Accuracy 0.4403\n",
            "Epoch 1 Batch 4800 Loss 1.2712 Accuracy 0.4404\n",
            "Epoch 1 Batch 4850 Loss 1.2714 Accuracy 0.4404\n",
            "Epoch 1 Batch 4900 Loss 1.2718 Accuracy 0.4404\n",
            "Epoch 1 Batch 4950 Loss 1.2718 Accuracy 0.4404\n",
            "Epoch 1 Batch 5000 Loss 1.2724 Accuracy 0.4404\n",
            "Epoch 1 Batch 5050 Loss 1.2724 Accuracy 0.4404\n",
            "Epoch 1 Batch 5100 Loss 1.2727 Accuracy 0.4403\n",
            "Epoch 1 Batch 5150 Loss 1.2729 Accuracy 0.4403\n",
            "Epoch 1 Batch 5200 Loss 1.2731 Accuracy 0.4402\n",
            "Epoch 1 Batch 5250 Loss 1.2733 Accuracy 0.4401\n",
            "Epoch 1 Batch 5300 Loss 1.2732 Accuracy 0.44\n",
            "Epoch 1 Batch 5350 Loss 1.2735 Accuracy 0.4399\n",
            "Epoch 1 Batch 5400 Loss 1.2736 Accuracy 0.4399\n",
            "Epoch 1 Batch 5450 Loss 1.2736 Accuracy 0.4397\n",
            "Epoch 1 Batch 5500 Loss 1.2739 Accuracy 0.4396\n",
            "Epoch 1 Batch 5550 Loss 1.2742 Accuracy 0.4395\n",
            "Epoch 1 Batch 5600 Loss 1.2744 Accuracy 0.4394\n",
            "Epoch 1 Batch 5650 Loss 1.2742 Accuracy 0.4393\n",
            "Epoch 1 Batch 5700 Loss 1.2742 Accuracy 0.4393\n",
            "Saving checkpoint for epoch 1 at ./drive/My Drive/transformer/ckpt/ckpt-10\n",
            "Time taken for 1 epoch: 1332.249340057373 secs\n",
            "\n",
            "Start of epocch 2\n",
            "Epoch 2 Batch 0 Loss 1.3277 Accuracy 0.4186\n",
            "Epoch 2 Batch 50 Loss 1.2900 Accuracy 0.4365\n",
            "Epoch 2 Batch 100 Loss 1.2797 Accuracy 0.4394\n",
            "Epoch 2 Batch 150 Loss 1.2834 Accuracy 0.4393\n",
            "Epoch 2 Batch 200 Loss 1.2796 Accuracy 0.4398\n",
            "Epoch 2 Batch 250 Loss 1.2814 Accuracy 0.4405\n",
            "Epoch 2 Batch 300 Loss 1.2787 Accuracy 0.4403\n",
            "Epoch 2 Batch 350 Loss 1.2777 Accuracy 0.44\n",
            "Epoch 2 Batch 400 Loss 1.2744 Accuracy 0.4401\n",
            "Epoch 2 Batch 450 Loss 1.2711 Accuracy 0.4402\n",
            "Epoch 2 Batch 500 Loss 1.2701 Accuracy 0.4402\n",
            "Epoch 2 Batch 550 Loss 1.2684 Accuracy 0.44\n",
            "Epoch 2 Batch 600 Loss 1.2691 Accuracy 0.4399\n",
            "Epoch 2 Batch 650 Loss 1.2688 Accuracy 0.4401\n",
            "Epoch 2 Batch 700 Loss 1.2660 Accuracy 0.4403\n",
            "Epoch 2 Batch 750 Loss 1.2663 Accuracy 0.4405\n",
            "Epoch 2 Batch 800 Loss 1.2665 Accuracy 0.4408\n",
            "Epoch 2 Batch 850 Loss 1.2651 Accuracy 0.4412\n",
            "Epoch 2 Batch 900 Loss 1.2634 Accuracy 0.4411\n",
            "Epoch 2 Batch 950 Loss 1.2618 Accuracy 0.441\n",
            "Epoch 2 Batch 1000 Loss 1.2602 Accuracy 0.441\n",
            "Epoch 2 Batch 1050 Loss 1.2599 Accuracy 0.4412\n",
            "Epoch 2 Batch 1100 Loss 1.2589 Accuracy 0.4412\n",
            "Epoch 2 Batch 1150 Loss 1.2579 Accuracy 0.4415\n",
            "Epoch 2 Batch 1200 Loss 1.2570 Accuracy 0.4417\n",
            "Epoch 2 Batch 1250 Loss 1.2547 Accuracy 0.442\n",
            "Epoch 2 Batch 1300 Loss 1.2524 Accuracy 0.4424\n",
            "Epoch 2 Batch 1350 Loss 1.2512 Accuracy 0.4431\n",
            "Epoch 2 Batch 1400 Loss 1.2487 Accuracy 0.4436\n",
            "Epoch 2 Batch 1450 Loss 1.2460 Accuracy 0.4443\n",
            "Epoch 2 Batch 1500 Loss 1.2435 Accuracy 0.4451\n",
            "Epoch 2 Batch 1550 Loss 1.2416 Accuracy 0.4458\n",
            "Epoch 2 Batch 1600 Loss 1.2386 Accuracy 0.4466\n",
            "Epoch 2 Batch 1650 Loss 1.2362 Accuracy 0.4472\n",
            "Epoch 2 Batch 1700 Loss 1.2330 Accuracy 0.4478\n",
            "Epoch 2 Batch 1750 Loss 1.2310 Accuracy 0.4486\n",
            "Epoch 2 Batch 1800 Loss 1.2283 Accuracy 0.4493\n",
            "Epoch 2 Batch 1850 Loss 1.2258 Accuracy 0.4502\n",
            "Epoch 2 Batch 1900 Loss 1.2238 Accuracy 0.451\n",
            "Epoch 2 Batch 1950 Loss 1.2216 Accuracy 0.4517\n",
            "Epoch 2 Batch 2000 Loss 1.2196 Accuracy 0.4522\n",
            "Epoch 2 Batch 2050 Loss 1.2171 Accuracy 0.4527\n",
            "Epoch 2 Batch 2100 Loss 1.2141 Accuracy 0.4531\n",
            "Epoch 2 Batch 2150 Loss 1.2109 Accuracy 0.4535\n",
            "Epoch 2 Batch 2200 Loss 1.2076 Accuracy 0.4537\n",
            "Epoch 2 Batch 2250 Loss 1.2042 Accuracy 0.4538\n",
            "Epoch 2 Batch 2300 Loss 1.2012 Accuracy 0.454\n",
            "Epoch 2 Batch 2350 Loss 1.1981 Accuracy 0.4543\n",
            "Epoch 2 Batch 2400 Loss 1.1954 Accuracy 0.4546\n",
            "Epoch 2 Batch 2450 Loss 1.1917 Accuracy 0.455\n",
            "Epoch 2 Batch 2500 Loss 1.1886 Accuracy 0.4554\n",
            "Epoch 2 Batch 2550 Loss 1.1854 Accuracy 0.4556\n",
            "Epoch 2 Batch 2600 Loss 1.1823 Accuracy 0.4561\n",
            "Epoch 2 Batch 2650 Loss 1.1794 Accuracy 0.4566\n",
            "Epoch 2 Batch 2700 Loss 1.1765 Accuracy 0.4569\n",
            "Epoch 2 Batch 2750 Loss 1.1737 Accuracy 0.4573\n",
            "Epoch 2 Batch 2800 Loss 1.1712 Accuracy 0.4577\n",
            "Epoch 2 Batch 2850 Loss 1.1685 Accuracy 0.4581\n",
            "Epoch 2 Batch 2900 Loss 1.1663 Accuracy 0.4585\n",
            "Epoch 2 Batch 2950 Loss 1.1638 Accuracy 0.4588\n",
            "Epoch 2 Batch 3000 Loss 1.1620 Accuracy 0.459\n",
            "Epoch 2 Batch 3050 Loss 1.1601 Accuracy 0.4593\n",
            "Epoch 2 Batch 3100 Loss 1.1578 Accuracy 0.4595\n",
            "Epoch 2 Batch 3150 Loss 1.1557 Accuracy 0.4598\n",
            "Epoch 2 Batch 3200 Loss 1.1537 Accuracy 0.46\n",
            "Epoch 2 Batch 3250 Loss 1.1509 Accuracy 0.4603\n",
            "Epoch 2 Batch 3300 Loss 1.1488 Accuracy 0.4607\n",
            "Epoch 2 Batch 3350 Loss 1.1464 Accuracy 0.461\n",
            "Epoch 2 Batch 3400 Loss 1.1443 Accuracy 0.4613\n",
            "Epoch 2 Batch 3450 Loss 1.1423 Accuracy 0.4616\n",
            "Epoch 2 Batch 3500 Loss 1.1400 Accuracy 0.462\n",
            "Epoch 2 Batch 3550 Loss 1.1378 Accuracy 0.4623\n",
            "Epoch 2 Batch 3600 Loss 1.1360 Accuracy 0.4627\n",
            "Epoch 2 Batch 3650 Loss 1.1340 Accuracy 0.4631\n",
            "Epoch 2 Batch 3700 Loss 1.1323 Accuracy 0.4635\n",
            "Epoch 2 Batch 3750 Loss 1.1305 Accuracy 0.4639\n",
            "Epoch 2 Batch 3800 Loss 1.1287 Accuracy 0.4642\n",
            "Epoch 2 Batch 3850 Loss 1.1271 Accuracy 0.4646\n",
            "Epoch 2 Batch 3900 Loss 1.1256 Accuracy 0.4649\n",
            "Epoch 2 Batch 3950 Loss 1.1243 Accuracy 0.4652\n",
            "Epoch 2 Batch 4000 Loss 1.1227 Accuracy 0.4656\n",
            "Epoch 2 Batch 4050 Loss 1.1213 Accuracy 0.466\n",
            "Epoch 2 Batch 4100 Loss 1.1196 Accuracy 0.4663\n",
            "Epoch 2 Batch 4150 Loss 1.1192 Accuracy 0.4664\n",
            "Epoch 2 Batch 4200 Loss 1.1191 Accuracy 0.4664\n",
            "Epoch 2 Batch 4250 Loss 1.1196 Accuracy 0.4664\n",
            "Epoch 2 Batch 4300 Loss 1.1203 Accuracy 0.4663\n",
            "Epoch 2 Batch 4350 Loss 1.1211 Accuracy 0.4662\n",
            "Epoch 2 Batch 4400 Loss 1.1221 Accuracy 0.4661\n",
            "Epoch 2 Batch 4450 Loss 1.1231 Accuracy 0.4659\n",
            "Epoch 2 Batch 4500 Loss 1.1242 Accuracy 0.4657\n",
            "Epoch 2 Batch 4550 Loss 1.1254 Accuracy 0.4656\n",
            "Epoch 2 Batch 4600 Loss 1.1264 Accuracy 0.4654\n",
            "Epoch 2 Batch 4650 Loss 1.1274 Accuracy 0.4652\n",
            "Epoch 2 Batch 4700 Loss 1.1284 Accuracy 0.4651\n",
            "Epoch 2 Batch 4750 Loss 1.1295 Accuracy 0.4649\n",
            "Epoch 2 Batch 4800 Loss 1.1304 Accuracy 0.4648\n",
            "Epoch 2 Batch 4850 Loss 1.1315 Accuracy 0.4647\n",
            "Epoch 2 Batch 4900 Loss 1.1330 Accuracy 0.4645\n",
            "Epoch 2 Batch 4950 Loss 1.1343 Accuracy 0.4643\n",
            "Epoch 2 Batch 5000 Loss 1.1355 Accuracy 0.4641\n",
            "Epoch 2 Batch 5050 Loss 1.1368 Accuracy 0.4639\n",
            "Epoch 2 Batch 5100 Loss 1.1380 Accuracy 0.4637\n",
            "Epoch 2 Batch 5150 Loss 1.1393 Accuracy 0.4635\n",
            "Epoch 2 Batch 5200 Loss 1.1406 Accuracy 0.4633\n",
            "Epoch 2 Batch 5250 Loss 1.1415 Accuracy 0.463\n",
            "Epoch 2 Batch 5300 Loss 1.1422 Accuracy 0.4627\n",
            "Epoch 2 Batch 5350 Loss 1.1435 Accuracy 0.4625\n",
            "Epoch 2 Batch 5400 Loss 1.1443 Accuracy 0.4622\n",
            "Epoch 2 Batch 5450 Loss 1.1454 Accuracy 0.4621\n",
            "Epoch 2 Batch 5500 Loss 1.1462 Accuracy 0.4618\n",
            "Epoch 2 Batch 5550 Loss 1.1472 Accuracy 0.4616\n",
            "Epoch 2 Batch 5600 Loss 1.1480 Accuracy 0.4614\n",
            "Epoch 2 Batch 5650 Loss 1.1487 Accuracy 0.4612\n",
            "Epoch 2 Batch 5700 Loss 1.1494 Accuracy 0.461\n",
            "Saving checkpoint for epoch 2 at ./drive/My Drive/transformer/ckpt/ckpt-11\n",
            "Time taken for 1 epoch: 1346.3778386116028 secs\n",
            "\n",
            "Start of epocch 3\n",
            "Epoch 3 Batch 0 Loss 1.3660 Accuracy 0.4309\n",
            "Epoch 3 Batch 50 Loss 1.2490 Accuracy 0.4481\n",
            "Epoch 3 Batch 100 Loss 1.2338 Accuracy 0.4481\n",
            "Epoch 3 Batch 150 Loss 1.2397 Accuracy 0.4474\n",
            "Epoch 3 Batch 200 Loss 1.2421 Accuracy 0.4484\n",
            "Epoch 3 Batch 250 Loss 1.2396 Accuracy 0.4489\n",
            "Epoch 3 Batch 300 Loss 1.2340 Accuracy 0.4481\n",
            "Epoch 3 Batch 350 Loss 1.2362 Accuracy 0.4477\n",
            "Epoch 3 Batch 400 Loss 1.2342 Accuracy 0.4472\n",
            "Epoch 3 Batch 450 Loss 1.2298 Accuracy 0.4473\n",
            "Epoch 3 Batch 500 Loss 1.2275 Accuracy 0.4472\n",
            "Epoch 3 Batch 550 Loss 1.2276 Accuracy 0.4473\n",
            "Epoch 3 Batch 600 Loss 1.2249 Accuracy 0.4471\n",
            "Epoch 3 Batch 650 Loss 1.2237 Accuracy 0.4473\n",
            "Epoch 3 Batch 700 Loss 1.2229 Accuracy 0.4476\n",
            "Epoch 3 Batch 750 Loss 1.2237 Accuracy 0.4481\n",
            "Epoch 3 Batch 800 Loss 1.2236 Accuracy 0.4483\n",
            "Epoch 3 Batch 850 Loss 1.2224 Accuracy 0.4486\n",
            "Epoch 3 Batch 900 Loss 1.2210 Accuracy 0.4483\n",
            "Epoch 3 Batch 950 Loss 1.2211 Accuracy 0.4481\n",
            "Epoch 3 Batch 1000 Loss 1.2190 Accuracy 0.4481\n",
            "Epoch 3 Batch 1050 Loss 1.2186 Accuracy 0.4481\n",
            "Epoch 3 Batch 1100 Loss 1.2172 Accuracy 0.4482\n",
            "Epoch 3 Batch 1150 Loss 1.2158 Accuracy 0.4484\n",
            "Epoch 3 Batch 1200 Loss 1.2152 Accuracy 0.4486\n",
            "Epoch 3 Batch 1250 Loss 1.2133 Accuracy 0.4489\n",
            "Epoch 3 Batch 1300 Loss 1.2109 Accuracy 0.4494\n",
            "Epoch 3 Batch 1350 Loss 1.2084 Accuracy 0.4501\n",
            "Epoch 3 Batch 1400 Loss 1.2059 Accuracy 0.4509\n",
            "Epoch 3 Batch 1450 Loss 1.2043 Accuracy 0.4516\n",
            "Epoch 3 Batch 1500 Loss 1.2020 Accuracy 0.4523\n",
            "Epoch 3 Batch 1550 Loss 1.1997 Accuracy 0.4529\n",
            "Epoch 3 Batch 1600 Loss 1.1973 Accuracy 0.4537\n",
            "Epoch 3 Batch 1650 Loss 1.1943 Accuracy 0.4544\n",
            "Epoch 3 Batch 1700 Loss 1.1924 Accuracy 0.4552\n",
            "Epoch 3 Batch 1750 Loss 1.1904 Accuracy 0.4557\n",
            "Epoch 3 Batch 1800 Loss 1.1883 Accuracy 0.4566\n",
            "Epoch 3 Batch 1850 Loss 1.1864 Accuracy 0.4573\n",
            "Epoch 3 Batch 1900 Loss 1.1843 Accuracy 0.4581\n",
            "Epoch 3 Batch 1950 Loss 1.1825 Accuracy 0.4588\n",
            "Epoch 3 Batch 2000 Loss 1.1802 Accuracy 0.4595\n",
            "Epoch 3 Batch 2050 Loss 1.1779 Accuracy 0.4599\n",
            "Epoch 3 Batch 2100 Loss 1.1751 Accuracy 0.4603\n",
            "Epoch 3 Batch 2150 Loss 1.1721 Accuracy 0.4607\n",
            "Epoch 3 Batch 2200 Loss 1.1689 Accuracy 0.4609\n",
            "Epoch 3 Batch 2250 Loss 1.1653 Accuracy 0.4612\n",
            "Epoch 3 Batch 2300 Loss 1.1618 Accuracy 0.4613\n",
            "Epoch 3 Batch 2350 Loss 1.1589 Accuracy 0.4615\n",
            "Epoch 3 Batch 2400 Loss 1.1557 Accuracy 0.4617\n",
            "Epoch 3 Batch 2450 Loss 1.1524 Accuracy 0.462\n",
            "Epoch 3 Batch 2500 Loss 1.1493 Accuracy 0.4623\n",
            "Epoch 3 Batch 2550 Loss 1.1463 Accuracy 0.4626\n",
            "Epoch 3 Batch 2600 Loss 1.1438 Accuracy 0.463\n",
            "Epoch 3 Batch 2650 Loss 1.1410 Accuracy 0.4635\n",
            "Epoch 3 Batch 2700 Loss 1.1384 Accuracy 0.4638\n",
            "Epoch 3 Batch 2750 Loss 1.1362 Accuracy 0.4641\n",
            "Epoch 3 Batch 2800 Loss 1.1336 Accuracy 0.4644\n",
            "Epoch 3 Batch 2850 Loss 1.1312 Accuracy 0.4648\n",
            "Epoch 3 Batch 2900 Loss 1.1289 Accuracy 0.4651\n",
            "Epoch 3 Batch 2950 Loss 1.1269 Accuracy 0.4655\n",
            "Epoch 3 Batch 3000 Loss 1.1248 Accuracy 0.4657\n",
            "Epoch 3 Batch 3050 Loss 1.1228 Accuracy 0.466\n",
            "Epoch 3 Batch 3100 Loss 1.1204 Accuracy 0.4663\n",
            "Epoch 3 Batch 3150 Loss 1.1181 Accuracy 0.4666\n",
            "Epoch 3 Batch 3200 Loss 1.1160 Accuracy 0.4668\n",
            "Epoch 3 Batch 3250 Loss 1.1134 Accuracy 0.4671\n",
            "Epoch 3 Batch 3300 Loss 1.1112 Accuracy 0.4674\n",
            "Epoch 3 Batch 3350 Loss 1.1094 Accuracy 0.4677\n",
            "Epoch 3 Batch 3400 Loss 1.1075 Accuracy 0.4681\n",
            "Epoch 3 Batch 3450 Loss 1.1056 Accuracy 0.4684\n",
            "Epoch 3 Batch 3500 Loss 1.1035 Accuracy 0.4687\n",
            "Epoch 3 Batch 3550 Loss 1.1018 Accuracy 0.4691\n",
            "Epoch 3 Batch 3600 Loss 1.0998 Accuracy 0.4693\n",
            "Epoch 3 Batch 3650 Loss 1.0976 Accuracy 0.4697\n",
            "Epoch 3 Batch 3700 Loss 1.0956 Accuracy 0.4701\n",
            "Epoch 3 Batch 3750 Loss 1.0941 Accuracy 0.4703\n",
            "Epoch 3 Batch 3800 Loss 1.0929 Accuracy 0.4707\n",
            "Epoch 3 Batch 3850 Loss 1.0910 Accuracy 0.471\n",
            "Epoch 3 Batch 3900 Loss 1.0894 Accuracy 0.4713\n",
            "Epoch 3 Batch 3950 Loss 1.0881 Accuracy 0.4717\n",
            "Epoch 3 Batch 4000 Loss 1.0870 Accuracy 0.472\n",
            "Epoch 3 Batch 4050 Loss 1.0856 Accuracy 0.4723\n",
            "Epoch 3 Batch 4100 Loss 1.0846 Accuracy 0.4725\n",
            "Epoch 3 Batch 4150 Loss 1.0841 Accuracy 0.4727\n",
            "Epoch 3 Batch 4200 Loss 1.0840 Accuracy 0.4728\n",
            "Epoch 3 Batch 4250 Loss 1.0843 Accuracy 0.4727\n",
            "Epoch 3 Batch 4300 Loss 1.0848 Accuracy 0.4726\n",
            "Epoch 3 Batch 4350 Loss 1.0856 Accuracy 0.4726\n",
            "Epoch 3 Batch 4400 Loss 1.0867 Accuracy 0.4724\n",
            "Epoch 3 Batch 4450 Loss 1.0878 Accuracy 0.4722\n",
            "Epoch 3 Batch 4500 Loss 1.0890 Accuracy 0.472\n",
            "Epoch 3 Batch 4550 Loss 1.0899 Accuracy 0.4718\n",
            "Epoch 3 Batch 4600 Loss 1.0913 Accuracy 0.4717\n",
            "Epoch 3 Batch 4650 Loss 1.0929 Accuracy 0.4715\n",
            "Epoch 3 Batch 4700 Loss 1.0944 Accuracy 0.4713\n",
            "Epoch 3 Batch 4750 Loss 1.0956 Accuracy 0.4711\n",
            "Epoch 3 Batch 4800 Loss 1.0966 Accuracy 0.471\n",
            "Epoch 3 Batch 4850 Loss 1.0978 Accuracy 0.4708\n",
            "Epoch 3 Batch 4900 Loss 1.0989 Accuracy 0.4707\n",
            "Epoch 3 Batch 4950 Loss 1.1002 Accuracy 0.4705\n",
            "Epoch 3 Batch 5000 Loss 1.1014 Accuracy 0.4703\n",
            "Epoch 3 Batch 5050 Loss 1.1026 Accuracy 0.4701\n",
            "Epoch 3 Batch 5100 Loss 1.1039 Accuracy 0.4699\n",
            "Epoch 3 Batch 5150 Loss 1.1051 Accuracy 0.4697\n",
            "Epoch 3 Batch 5200 Loss 1.1063 Accuracy 0.4694\n",
            "Epoch 3 Batch 5250 Loss 1.1073 Accuracy 0.4692\n",
            "Epoch 3 Batch 5300 Loss 1.1084 Accuracy 0.4688\n",
            "Epoch 3 Batch 5350 Loss 1.1099 Accuracy 0.4686\n",
            "Epoch 3 Batch 5400 Loss 1.1107 Accuracy 0.4683\n",
            "Epoch 3 Batch 5450 Loss 1.1118 Accuracy 0.4681\n",
            "Epoch 3 Batch 5500 Loss 1.1129 Accuracy 0.4679\n",
            "Epoch 3 Batch 5550 Loss 1.1136 Accuracy 0.4676\n",
            "Epoch 3 Batch 5600 Loss 1.1145 Accuracy 0.4674\n",
            "Epoch 3 Batch 5650 Loss 1.1152 Accuracy 0.4672\n",
            "Epoch 3 Batch 5700 Loss 1.1161 Accuracy 0.467\n",
            "Saving checkpoint for epoch 3 at ./drive/My Drive/transformer/ckpt/ckpt-12\n",
            "Time taken for 1 epoch: 1335.4125123023987 secs\n",
            "\n",
            "Start of epocch 4\n",
            "Epoch 4 Batch 0 Loss 1.3348 Accuracy 0.4243\n",
            "Epoch 4 Batch 50 Loss 1.2216 Accuracy 0.4518\n",
            "Epoch 4 Batch 100 Loss 1.2248 Accuracy 0.4495\n",
            "Epoch 4 Batch 150 Loss 1.2199 Accuracy 0.4507\n",
            "Epoch 4 Batch 200 Loss 1.2268 Accuracy 0.4505\n",
            "Epoch 4 Batch 250 Loss 1.2205 Accuracy 0.4499\n",
            "Epoch 4 Batch 300 Loss 1.2203 Accuracy 0.4504\n",
            "Epoch 4 Batch 350 Loss 1.2132 Accuracy 0.4514\n",
            "Epoch 4 Batch 400 Loss 1.2124 Accuracy 0.4519\n",
            "Epoch 4 Batch 450 Loss 1.2087 Accuracy 0.4519\n",
            "Epoch 4 Batch 500 Loss 1.2061 Accuracy 0.4519\n",
            "Epoch 4 Batch 550 Loss 1.2024 Accuracy 0.4516\n",
            "Epoch 4 Batch 600 Loss 1.2018 Accuracy 0.4518\n",
            "Epoch 4 Batch 650 Loss 1.2014 Accuracy 0.452\n",
            "Epoch 4 Batch 700 Loss 1.2003 Accuracy 0.4523\n",
            "Epoch 4 Batch 750 Loss 1.1995 Accuracy 0.4524\n",
            "Epoch 4 Batch 800 Loss 1.1982 Accuracy 0.4529\n",
            "Epoch 4 Batch 850 Loss 1.1974 Accuracy 0.4531\n",
            "Epoch 4 Batch 900 Loss 1.1962 Accuracy 0.4531\n",
            "Epoch 4 Batch 950 Loss 1.1945 Accuracy 0.4532\n",
            "Epoch 4 Batch 1000 Loss 1.1924 Accuracy 0.4534\n",
            "Epoch 4 Batch 1050 Loss 1.1906 Accuracy 0.4535\n",
            "Epoch 4 Batch 1100 Loss 1.1894 Accuracy 0.4537\n",
            "Epoch 4 Batch 1150 Loss 1.1879 Accuracy 0.4538\n",
            "Epoch 4 Batch 1200 Loss 1.1875 Accuracy 0.454\n",
            "Epoch 4 Batch 1250 Loss 1.1854 Accuracy 0.454\n",
            "Epoch 4 Batch 1300 Loss 1.1832 Accuracy 0.4543\n",
            "Epoch 4 Batch 1350 Loss 1.1811 Accuracy 0.4549\n",
            "Epoch 4 Batch 1400 Loss 1.1783 Accuracy 0.4554\n",
            "Epoch 4 Batch 1450 Loss 1.1774 Accuracy 0.4559\n",
            "Epoch 4 Batch 1500 Loss 1.1748 Accuracy 0.4566\n",
            "Epoch 4 Batch 1550 Loss 1.1725 Accuracy 0.4574\n",
            "Epoch 4 Batch 1600 Loss 1.1699 Accuracy 0.4582\n",
            "Epoch 4 Batch 1650 Loss 1.1674 Accuracy 0.4589\n",
            "Epoch 4 Batch 1700 Loss 1.1661 Accuracy 0.4595\n",
            "Epoch 4 Batch 1750 Loss 1.1638 Accuracy 0.4603\n",
            "Epoch 4 Batch 1800 Loss 1.1613 Accuracy 0.461\n",
            "Epoch 4 Batch 1850 Loss 1.1592 Accuracy 0.4619\n",
            "Epoch 4 Batch 1900 Loss 1.1568 Accuracy 0.4626\n",
            "Epoch 4 Batch 1950 Loss 1.1547 Accuracy 0.4633\n",
            "Epoch 4 Batch 2000 Loss 1.1534 Accuracy 0.4638\n",
            "Epoch 4 Batch 2050 Loss 1.1511 Accuracy 0.4642\n",
            "Epoch 4 Batch 2100 Loss 1.1484 Accuracy 0.4646\n",
            "Epoch 4 Batch 2150 Loss 1.1458 Accuracy 0.4649\n",
            "Epoch 4 Batch 2200 Loss 1.1422 Accuracy 0.4651\n",
            "Epoch 4 Batch 2250 Loss 1.1390 Accuracy 0.4653\n",
            "Epoch 4 Batch 2300 Loss 1.1358 Accuracy 0.4656\n",
            "Epoch 4 Batch 2350 Loss 1.1328 Accuracy 0.4661\n",
            "Epoch 4 Batch 2400 Loss 1.1297 Accuracy 0.4662\n",
            "Epoch 4 Batch 2450 Loss 1.1265 Accuracy 0.4665\n",
            "Epoch 4 Batch 2500 Loss 1.1238 Accuracy 0.4667\n",
            "Epoch 4 Batch 2550 Loss 1.1209 Accuracy 0.4672\n",
            "Epoch 4 Batch 2600 Loss 1.1179 Accuracy 0.4674\n",
            "Epoch 4 Batch 2650 Loss 1.1155 Accuracy 0.4677\n",
            "Epoch 4 Batch 2700 Loss 1.1128 Accuracy 0.4681\n",
            "Epoch 4 Batch 2750 Loss 1.1105 Accuracy 0.4685\n",
            "Epoch 4 Batch 2800 Loss 1.1079 Accuracy 0.4687\n",
            "Epoch 4 Batch 2850 Loss 1.1058 Accuracy 0.4691\n",
            "Epoch 4 Batch 2900 Loss 1.1034 Accuracy 0.4694\n",
            "Epoch 4 Batch 2950 Loss 1.1015 Accuracy 0.4698\n",
            "Epoch 4 Batch 3000 Loss 1.0994 Accuracy 0.47\n",
            "Epoch 4 Batch 3050 Loss 1.0974 Accuracy 0.4703\n",
            "Epoch 4 Batch 3100 Loss 1.0955 Accuracy 0.4706\n",
            "Epoch 4 Batch 3150 Loss 1.0933 Accuracy 0.4709\n",
            "Epoch 4 Batch 3200 Loss 1.0911 Accuracy 0.4712\n",
            "Epoch 4 Batch 3250 Loss 1.0891 Accuracy 0.4716\n",
            "Epoch 4 Batch 3300 Loss 1.0869 Accuracy 0.4719\n",
            "Epoch 4 Batch 3350 Loss 1.0844 Accuracy 0.4722\n",
            "Epoch 4 Batch 3400 Loss 1.0826 Accuracy 0.4726\n",
            "Epoch 4 Batch 3450 Loss 1.0806 Accuracy 0.4728\n",
            "Epoch 4 Batch 3500 Loss 1.0787 Accuracy 0.4731\n",
            "Epoch 4 Batch 3550 Loss 1.0770 Accuracy 0.4734\n",
            "Epoch 4 Batch 3600 Loss 1.0754 Accuracy 0.4737\n",
            "Epoch 4 Batch 3650 Loss 1.0735 Accuracy 0.4741\n",
            "Epoch 4 Batch 3700 Loss 1.0718 Accuracy 0.4744\n",
            "Epoch 4 Batch 3750 Loss 1.0703 Accuracy 0.4748\n",
            "Epoch 4 Batch 3800 Loss 1.0688 Accuracy 0.4751\n",
            "Epoch 4 Batch 3850 Loss 1.0673 Accuracy 0.4755\n",
            "Epoch 4 Batch 3900 Loss 1.0658 Accuracy 0.4758\n",
            "Epoch 4 Batch 3950 Loss 1.0643 Accuracy 0.476\n",
            "Epoch 4 Batch 4000 Loss 1.0629 Accuracy 0.4763\n",
            "Epoch 4 Batch 4050 Loss 1.0618 Accuracy 0.4766\n",
            "Epoch 4 Batch 4100 Loss 1.0606 Accuracy 0.4767\n",
            "Epoch 4 Batch 4150 Loss 1.0601 Accuracy 0.4769\n",
            "Epoch 4 Batch 4200 Loss 1.0601 Accuracy 0.4769\n",
            "Epoch 4 Batch 4250 Loss 1.0607 Accuracy 0.4769\n",
            "Epoch 4 Batch 4300 Loss 1.0614 Accuracy 0.4768\n",
            "Epoch 4 Batch 4350 Loss 1.0623 Accuracy 0.4766\n",
            "Epoch 4 Batch 4400 Loss 1.0635 Accuracy 0.4765\n",
            "Epoch 4 Batch 4450 Loss 1.0649 Accuracy 0.4763\n",
            "Epoch 4 Batch 4500 Loss 1.0660 Accuracy 0.4762\n",
            "Epoch 4 Batch 4550 Loss 1.0676 Accuracy 0.476\n",
            "Epoch 4 Batch 4600 Loss 1.0688 Accuracy 0.4759\n",
            "Epoch 4 Batch 4650 Loss 1.0698 Accuracy 0.4757\n",
            "Epoch 4 Batch 4700 Loss 1.0712 Accuracy 0.4756\n",
            "Epoch 4 Batch 4750 Loss 1.0722 Accuracy 0.4754\n",
            "Epoch 4 Batch 4800 Loss 1.0735 Accuracy 0.4752\n",
            "Epoch 4 Batch 4850 Loss 1.0745 Accuracy 0.475\n",
            "Epoch 4 Batch 4900 Loss 1.0759 Accuracy 0.4748\n",
            "Epoch 4 Batch 4950 Loss 1.0774 Accuracy 0.4746\n",
            "Epoch 4 Batch 5000 Loss 1.0787 Accuracy 0.4744\n",
            "Epoch 4 Batch 5050 Loss 1.0800 Accuracy 0.4742\n",
            "Epoch 4 Batch 5100 Loss 1.0812 Accuracy 0.474\n",
            "Epoch 4 Batch 5150 Loss 1.0824 Accuracy 0.4737\n",
            "Epoch 4 Batch 5200 Loss 1.0836 Accuracy 0.4735\n",
            "Epoch 4 Batch 5250 Loss 1.0848 Accuracy 0.4732\n",
            "Epoch 4 Batch 5300 Loss 1.0860 Accuracy 0.4729\n",
            "Epoch 4 Batch 5350 Loss 1.0871 Accuracy 0.4726\n",
            "Epoch 4 Batch 5400 Loss 1.0882 Accuracy 0.4724\n",
            "Epoch 4 Batch 5450 Loss 1.0892 Accuracy 0.4722\n",
            "Epoch 4 Batch 5500 Loss 1.0904 Accuracy 0.4719\n",
            "Epoch 4 Batch 5550 Loss 1.0910 Accuracy 0.4717\n",
            "Epoch 4 Batch 5600 Loss 1.0921 Accuracy 0.4715\n",
            "Epoch 4 Batch 5650 Loss 1.0931 Accuracy 0.4712\n",
            "Epoch 4 Batch 5700 Loss 1.0939 Accuracy 0.471\n",
            "Saving checkpoint for epoch 4 at ./drive/My Drive/transformer/ckpt/ckpt-13\n",
            "Time taken for 1 epoch: 1335.3418955802917 secs\n",
            "\n",
            "Start of epocch 5\n",
            "Epoch 5 Batch 0 Loss 1.0435 Accuracy 0.4786\n",
            "Epoch 5 Batch 50 Loss 1.1987 Accuracy 0.4499\n",
            "Epoch 5 Batch 100 Loss 1.1960 Accuracy 0.4512\n",
            "Epoch 5 Batch 150 Loss 1.1900 Accuracy 0.4535\n",
            "Epoch 5 Batch 200 Loss 1.1862 Accuracy 0.4546\n",
            "Epoch 5 Batch 250 Loss 1.1911 Accuracy 0.4546\n",
            "Epoch 5 Batch 300 Loss 1.1912 Accuracy 0.4546\n",
            "Epoch 5 Batch 350 Loss 1.1879 Accuracy 0.4548\n",
            "Epoch 5 Batch 400 Loss 1.1851 Accuracy 0.4548\n",
            "Epoch 5 Batch 450 Loss 1.1823 Accuracy 0.4551\n",
            "Epoch 5 Batch 500 Loss 1.1806 Accuracy 0.455\n",
            "Epoch 5 Batch 550 Loss 1.1794 Accuracy 0.4551\n",
            "Epoch 5 Batch 600 Loss 1.1786 Accuracy 0.4551\n",
            "Epoch 5 Batch 650 Loss 1.1794 Accuracy 0.4555\n",
            "Epoch 5 Batch 700 Loss 1.1774 Accuracy 0.456\n",
            "Epoch 5 Batch 750 Loss 1.1782 Accuracy 0.4565\n",
            "Epoch 5 Batch 800 Loss 1.1776 Accuracy 0.4567\n",
            "Epoch 5 Batch 850 Loss 1.1762 Accuracy 0.4565\n",
            "Epoch 5 Batch 900 Loss 1.1766 Accuracy 0.4566\n",
            "Epoch 5 Batch 950 Loss 1.1740 Accuracy 0.4565\n",
            "Epoch 5 Batch 1000 Loss 1.1711 Accuracy 0.4565\n",
            "Epoch 5 Batch 1050 Loss 1.1710 Accuracy 0.4565\n",
            "Epoch 5 Batch 1100 Loss 1.1702 Accuracy 0.4565\n",
            "Epoch 5 Batch 1150 Loss 1.1700 Accuracy 0.4567\n",
            "Epoch 5 Batch 1200 Loss 1.1688 Accuracy 0.4567\n",
            "Epoch 5 Batch 1250 Loss 1.1668 Accuracy 0.457\n",
            "Epoch 5 Batch 1300 Loss 1.1646 Accuracy 0.4574\n",
            "Epoch 5 Batch 1350 Loss 1.1625 Accuracy 0.4578\n",
            "Epoch 5 Batch 1400 Loss 1.1605 Accuracy 0.4585\n",
            "Epoch 5 Batch 1450 Loss 1.1581 Accuracy 0.4592\n",
            "Epoch 5 Batch 1500 Loss 1.1555 Accuracy 0.46\n",
            "Epoch 5 Batch 1550 Loss 1.1532 Accuracy 0.4606\n",
            "Epoch 5 Batch 1600 Loss 1.1502 Accuracy 0.4615\n",
            "Epoch 5 Batch 1650 Loss 1.1483 Accuracy 0.4624\n",
            "Epoch 5 Batch 1700 Loss 1.1463 Accuracy 0.4631\n",
            "Epoch 5 Batch 1750 Loss 1.1443 Accuracy 0.4638\n",
            "Epoch 5 Batch 1800 Loss 1.1434 Accuracy 0.4645\n",
            "Epoch 5 Batch 1850 Loss 1.1403 Accuracy 0.4654\n",
            "Epoch 5 Batch 1900 Loss 1.1375 Accuracy 0.4662\n",
            "Epoch 5 Batch 1950 Loss 1.1351 Accuracy 0.4668\n",
            "Epoch 5 Batch 2000 Loss 1.1333 Accuracy 0.4672\n",
            "Epoch 5 Batch 2050 Loss 1.1314 Accuracy 0.4676\n",
            "Epoch 5 Batch 2100 Loss 1.1297 Accuracy 0.4679\n",
            "Epoch 5 Batch 2150 Loss 1.1270 Accuracy 0.4682\n",
            "Epoch 5 Batch 2200 Loss 1.1239 Accuracy 0.4685\n",
            "Epoch 5 Batch 2250 Loss 1.1204 Accuracy 0.4687\n",
            "Epoch 5 Batch 2300 Loss 1.1175 Accuracy 0.4689\n",
            "Epoch 5 Batch 2350 Loss 1.1144 Accuracy 0.4691\n",
            "Epoch 5 Batch 2400 Loss 1.1113 Accuracy 0.4694\n",
            "Epoch 5 Batch 2450 Loss 1.1086 Accuracy 0.4696\n",
            "Epoch 5 Batch 2500 Loss 1.1054 Accuracy 0.4699\n",
            "Epoch 5 Batch 2550 Loss 1.1026 Accuracy 0.4702\n",
            "Epoch 5 Batch 2600 Loss 1.1000 Accuracy 0.4705\n",
            "Epoch 5 Batch 2650 Loss 1.0970 Accuracy 0.4709\n",
            "Epoch 5 Batch 2700 Loss 1.0943 Accuracy 0.4713\n",
            "Epoch 5 Batch 2750 Loss 1.0918 Accuracy 0.4716\n",
            "Epoch 5 Batch 2800 Loss 1.0891 Accuracy 0.472\n",
            "Epoch 5 Batch 2850 Loss 1.0869 Accuracy 0.4722\n",
            "Epoch 5 Batch 2900 Loss 1.0845 Accuracy 0.4725\n",
            "Epoch 5 Batch 2950 Loss 1.0824 Accuracy 0.4729\n",
            "Epoch 5 Batch 3000 Loss 1.0805 Accuracy 0.4731\n",
            "Epoch 5 Batch 3050 Loss 1.0788 Accuracy 0.4733\n",
            "Epoch 5 Batch 3100 Loss 1.0772 Accuracy 0.4736\n",
            "Epoch 5 Batch 3150 Loss 1.0753 Accuracy 0.4739\n",
            "Epoch 5 Batch 3200 Loss 1.0736 Accuracy 0.4742\n",
            "Epoch 5 Batch 3250 Loss 1.0713 Accuracy 0.4744\n",
            "Epoch 5 Batch 3300 Loss 1.0694 Accuracy 0.4746\n",
            "Epoch 5 Batch 3350 Loss 1.0668 Accuracy 0.4749\n",
            "Epoch 5 Batch 3400 Loss 1.0648 Accuracy 0.4753\n",
            "Epoch 5 Batch 3450 Loss 1.0627 Accuracy 0.4757\n",
            "Epoch 5 Batch 3500 Loss 1.0609 Accuracy 0.4759\n",
            "Epoch 5 Batch 3550 Loss 1.0589 Accuracy 0.4763\n",
            "Epoch 5 Batch 3600 Loss 1.0572 Accuracy 0.4766\n",
            "Epoch 5 Batch 3650 Loss 1.0554 Accuracy 0.477\n",
            "Epoch 5 Batch 3700 Loss 1.0535 Accuracy 0.4774\n",
            "Epoch 5 Batch 3750 Loss 1.0520 Accuracy 0.4777\n",
            "Epoch 5 Batch 3800 Loss 1.0505 Accuracy 0.478\n",
            "Epoch 5 Batch 3850 Loss 1.0492 Accuracy 0.4784\n",
            "Epoch 5 Batch 3900 Loss 1.0480 Accuracy 0.4786\n",
            "Epoch 5 Batch 3950 Loss 1.0467 Accuracy 0.4789\n",
            "Epoch 5 Batch 4000 Loss 1.0456 Accuracy 0.4792\n",
            "Epoch 5 Batch 4050 Loss 1.0440 Accuracy 0.4796\n",
            "Epoch 5 Batch 4100 Loss 1.0430 Accuracy 0.4799\n",
            "Epoch 5 Batch 4150 Loss 1.0426 Accuracy 0.48\n",
            "Epoch 5 Batch 4200 Loss 1.0425 Accuracy 0.48\n",
            "Epoch 5 Batch 4250 Loss 1.0433 Accuracy 0.48\n",
            "Epoch 5 Batch 4300 Loss 1.0441 Accuracy 0.4798\n",
            "Epoch 5 Batch 4350 Loss 1.0451 Accuracy 0.4797\n",
            "Epoch 5 Batch 4400 Loss 1.0461 Accuracy 0.4795\n",
            "Epoch 5 Batch 4450 Loss 1.0475 Accuracy 0.4793\n",
            "Epoch 5 Batch 4500 Loss 1.0486 Accuracy 0.4791\n",
            "Epoch 5 Batch 4550 Loss 1.0498 Accuracy 0.4789\n",
            "Epoch 5 Batch 4600 Loss 1.0514 Accuracy 0.4787\n",
            "Epoch 5 Batch 4650 Loss 1.0527 Accuracy 0.4786\n",
            "Epoch 5 Batch 4700 Loss 1.0540 Accuracy 0.4784\n",
            "Epoch 5 Batch 4750 Loss 1.0552 Accuracy 0.4782\n",
            "Epoch 5 Batch 4800 Loss 1.0564 Accuracy 0.478\n",
            "Epoch 5 Batch 4850 Loss 1.0577 Accuracy 0.4779\n",
            "Epoch 5 Batch 4900 Loss 1.0588 Accuracy 0.4777\n",
            "Epoch 5 Batch 4950 Loss 1.0602 Accuracy 0.4776\n",
            "Epoch 5 Batch 5000 Loss 1.0611 Accuracy 0.4774\n",
            "Epoch 5 Batch 5050 Loss 1.0622 Accuracy 0.4772\n",
            "Epoch 5 Batch 5100 Loss 1.0637 Accuracy 0.477\n",
            "Epoch 5 Batch 5150 Loss 1.0649 Accuracy 0.4768\n",
            "Epoch 5 Batch 5200 Loss 1.0661 Accuracy 0.4765\n",
            "Epoch 5 Batch 5250 Loss 1.0675 Accuracy 0.4762\n",
            "Epoch 5 Batch 5300 Loss 1.0687 Accuracy 0.4759\n",
            "Epoch 5 Batch 5350 Loss 1.0699 Accuracy 0.4756\n",
            "Epoch 5 Batch 5400 Loss 1.0710 Accuracy 0.4754\n",
            "Epoch 5 Batch 5450 Loss 1.0722 Accuracy 0.4751\n",
            "Epoch 5 Batch 5500 Loss 1.0732 Accuracy 0.4748\n",
            "Epoch 5 Batch 5550 Loss 1.0741 Accuracy 0.4746\n",
            "Epoch 5 Batch 5600 Loss 1.0752 Accuracy 0.4743\n",
            "Epoch 5 Batch 5650 Loss 1.0759 Accuracy 0.4741\n",
            "Epoch 5 Batch 5700 Loss 1.0769 Accuracy 0.4738\n",
            "Saving checkpoint for epoch 5 at ./drive/My Drive/transformer/ckpt/ckpt-14\n",
            "Time taken for 1 epoch: 1340.5273025035858 secs\n",
            "\n",
            "Start of epocch 6\n",
            "Epoch 6 Batch 0 Loss 1.1935 Accuracy 0.4597\n",
            "Epoch 6 Batch 50 Loss 1.1856 Accuracy 0.4587\n",
            "Epoch 6 Batch 100 Loss 1.1750 Accuracy 0.4563\n",
            "Epoch 6 Batch 150 Loss 1.1723 Accuracy 0.4562\n",
            "Epoch 6 Batch 200 Loss 1.1759 Accuracy 0.4563\n",
            "Epoch 6 Batch 250 Loss 1.1764 Accuracy 0.4569\n",
            "Epoch 6 Batch 300 Loss 1.1749 Accuracy 0.4576\n",
            "Epoch 6 Batch 350 Loss 1.1733 Accuracy 0.457\n",
            "Epoch 6 Batch 400 Loss 1.1729 Accuracy 0.4575\n",
            "Epoch 6 Batch 450 Loss 1.1708 Accuracy 0.4575\n",
            "Epoch 6 Batch 500 Loss 1.1703 Accuracy 0.4573\n",
            "Epoch 6 Batch 550 Loss 1.1673 Accuracy 0.4572\n",
            "Epoch 6 Batch 600 Loss 1.1653 Accuracy 0.4573\n",
            "Epoch 6 Batch 650 Loss 1.1646 Accuracy 0.4576\n",
            "Epoch 6 Batch 700 Loss 1.1636 Accuracy 0.458\n",
            "Epoch 6 Batch 750 Loss 1.1642 Accuracy 0.458\n",
            "Epoch 6 Batch 800 Loss 1.1639 Accuracy 0.4583\n",
            "Epoch 6 Batch 850 Loss 1.1619 Accuracy 0.4584\n",
            "Epoch 6 Batch 900 Loss 1.1607 Accuracy 0.4587\n",
            "Epoch 6 Batch 950 Loss 1.1598 Accuracy 0.4588\n",
            "Epoch 6 Batch 1000 Loss 1.1584 Accuracy 0.4589\n",
            "Epoch 6 Batch 1050 Loss 1.1576 Accuracy 0.4588\n",
            "Epoch 6 Batch 1100 Loss 1.1562 Accuracy 0.4589\n",
            "Epoch 6 Batch 1150 Loss 1.1555 Accuracy 0.4592\n",
            "Epoch 6 Batch 1200 Loss 1.1542 Accuracy 0.4593\n",
            "Epoch 6 Batch 1250 Loss 1.1518 Accuracy 0.4597\n",
            "Epoch 6 Batch 1300 Loss 1.1497 Accuracy 0.4601\n",
            "Epoch 6 Batch 1350 Loss 1.1470 Accuracy 0.4604\n",
            "Epoch 6 Batch 1400 Loss 1.1453 Accuracy 0.4611\n",
            "Epoch 6 Batch 1450 Loss 1.1425 Accuracy 0.4617\n",
            "Epoch 6 Batch 1500 Loss 1.1407 Accuracy 0.4625\n",
            "Epoch 6 Batch 1550 Loss 1.1378 Accuracy 0.4633\n",
            "Epoch 6 Batch 1600 Loss 1.1360 Accuracy 0.4642\n",
            "Epoch 6 Batch 1650 Loss 1.1341 Accuracy 0.465\n",
            "Epoch 6 Batch 1700 Loss 1.1322 Accuracy 0.4657\n",
            "Epoch 6 Batch 1750 Loss 1.1298 Accuracy 0.4665\n",
            "Epoch 6 Batch 1800 Loss 1.1277 Accuracy 0.4672\n",
            "Epoch 6 Batch 1850 Loss 1.1247 Accuracy 0.468\n",
            "Epoch 6 Batch 1900 Loss 1.1220 Accuracy 0.4687\n",
            "Epoch 6 Batch 1950 Loss 1.1201 Accuracy 0.4695\n",
            "Epoch 6 Batch 2000 Loss 1.1186 Accuracy 0.4701\n",
            "Epoch 6 Batch 2050 Loss 1.1166 Accuracy 0.4706\n",
            "Epoch 6 Batch 2100 Loss 1.1145 Accuracy 0.4709\n",
            "Epoch 6 Batch 2150 Loss 1.1113 Accuracy 0.4713\n",
            "Epoch 6 Batch 2200 Loss 1.1082 Accuracy 0.4716\n",
            "Epoch 6 Batch 2250 Loss 1.1045 Accuracy 0.4718\n",
            "Epoch 6 Batch 2300 Loss 1.1012 Accuracy 0.472\n",
            "Epoch 6 Batch 2350 Loss 1.0979 Accuracy 0.4722\n",
            "Epoch 6 Batch 2400 Loss 1.0948 Accuracy 0.4725\n",
            "Epoch 6 Batch 2450 Loss 1.0918 Accuracy 0.4728\n",
            "Epoch 6 Batch 2500 Loss 1.0892 Accuracy 0.4732\n",
            "Epoch 6 Batch 2550 Loss 1.0868 Accuracy 0.4735\n",
            "Epoch 6 Batch 2600 Loss 1.0844 Accuracy 0.4739\n",
            "Epoch 6 Batch 2650 Loss 1.0819 Accuracy 0.4741\n",
            "Epoch 6 Batch 2700 Loss 1.0790 Accuracy 0.4745\n",
            "Epoch 6 Batch 2750 Loss 1.0765 Accuracy 0.4748\n",
            "Epoch 6 Batch 2800 Loss 1.0740 Accuracy 0.475\n",
            "Epoch 6 Batch 2850 Loss 1.0718 Accuracy 0.4754\n",
            "Epoch 6 Batch 2900 Loss 1.0697 Accuracy 0.4756\n",
            "Epoch 6 Batch 2950 Loss 1.0675 Accuracy 0.4759\n",
            "Epoch 6 Batch 3000 Loss 1.0658 Accuracy 0.4761\n",
            "Epoch 6 Batch 3050 Loss 1.0640 Accuracy 0.4764\n",
            "Epoch 6 Batch 3100 Loss 1.0617 Accuracy 0.4767\n",
            "Epoch 6 Batch 3150 Loss 1.0597 Accuracy 0.4769\n",
            "Epoch 6 Batch 3200 Loss 1.0575 Accuracy 0.4772\n",
            "Epoch 6 Batch 3250 Loss 1.0558 Accuracy 0.4774\n",
            "Epoch 6 Batch 3300 Loss 1.0537 Accuracy 0.4776\n",
            "Epoch 6 Batch 3350 Loss 1.0519 Accuracy 0.4779\n",
            "Epoch 6 Batch 3400 Loss 1.0497 Accuracy 0.4782\n",
            "Epoch 6 Batch 3450 Loss 1.0476 Accuracy 0.4785\n",
            "Epoch 6 Batch 3500 Loss 1.0459 Accuracy 0.4789\n",
            "Epoch 6 Batch 3550 Loss 1.0443 Accuracy 0.4792\n",
            "Epoch 6 Batch 3600 Loss 1.0426 Accuracy 0.4795\n",
            "Epoch 6 Batch 3650 Loss 1.0406 Accuracy 0.4798\n",
            "Epoch 6 Batch 3700 Loss 1.0389 Accuracy 0.4801\n",
            "Epoch 6 Batch 3750 Loss 1.0371 Accuracy 0.4804\n",
            "Epoch 6 Batch 3800 Loss 1.0357 Accuracy 0.4808\n",
            "Epoch 6 Batch 3850 Loss 1.0343 Accuracy 0.4811\n",
            "Epoch 6 Batch 3900 Loss 1.0325 Accuracy 0.4815\n",
            "Epoch 6 Batch 3950 Loss 1.0312 Accuracy 0.4818\n",
            "Epoch 6 Batch 4000 Loss 1.0299 Accuracy 0.4821\n",
            "Epoch 6 Batch 4050 Loss 1.0287 Accuracy 0.4824\n",
            "Epoch 6 Batch 4100 Loss 1.0274 Accuracy 0.4826\n",
            "Epoch 6 Batch 4150 Loss 1.0271 Accuracy 0.4827\n",
            "Epoch 6 Batch 4200 Loss 1.0272 Accuracy 0.4828\n",
            "Epoch 6 Batch 4250 Loss 1.0279 Accuracy 0.4827\n",
            "Epoch 6 Batch 4300 Loss 1.0289 Accuracy 0.4826\n",
            "Epoch 6 Batch 4350 Loss 1.0299 Accuracy 0.4825\n",
            "Epoch 6 Batch 4400 Loss 1.0309 Accuracy 0.4824\n",
            "Epoch 6 Batch 4450 Loss 1.0320 Accuracy 0.4822\n",
            "Epoch 6 Batch 4500 Loss 1.0334 Accuracy 0.482\n",
            "Epoch 6 Batch 4550 Loss 1.0347 Accuracy 0.4819\n",
            "Epoch 6 Batch 4600 Loss 1.0364 Accuracy 0.4817\n",
            "Epoch 6 Batch 4650 Loss 1.0377 Accuracy 0.4815\n",
            "Epoch 6 Batch 4700 Loss 1.0392 Accuracy 0.4813\n",
            "Epoch 6 Batch 4750 Loss 1.0403 Accuracy 0.4811\n",
            "Epoch 6 Batch 4800 Loss 1.0415 Accuracy 0.481\n",
            "Epoch 6 Batch 4850 Loss 1.0427 Accuracy 0.4808\n",
            "Epoch 6 Batch 4900 Loss 1.0437 Accuracy 0.4806\n",
            "Epoch 6 Batch 4950 Loss 1.0449 Accuracy 0.4804\n",
            "Epoch 6 Batch 5000 Loss 1.0462 Accuracy 0.4802\n",
            "Epoch 6 Batch 5050 Loss 1.0475 Accuracy 0.48\n",
            "Epoch 6 Batch 5100 Loss 1.0489 Accuracy 0.4797\n",
            "Epoch 6 Batch 5150 Loss 1.0502 Accuracy 0.4795\n",
            "Epoch 6 Batch 5200 Loss 1.0514 Accuracy 0.4792\n",
            "Epoch 6 Batch 5250 Loss 1.0524 Accuracy 0.4789\n",
            "Epoch 6 Batch 5300 Loss 1.0536 Accuracy 0.4786\n",
            "Epoch 6 Batch 5350 Loss 1.0548 Accuracy 0.4783\n",
            "Epoch 6 Batch 5400 Loss 1.0560 Accuracy 0.478\n",
            "Epoch 6 Batch 5450 Loss 1.0570 Accuracy 0.4777\n",
            "Epoch 6 Batch 5500 Loss 1.0581 Accuracy 0.4775\n",
            "Epoch 6 Batch 5550 Loss 1.0590 Accuracy 0.4772\n",
            "Epoch 6 Batch 5600 Loss 1.0602 Accuracy 0.477\n",
            "Epoch 6 Batch 5650 Loss 1.0614 Accuracy 0.4768\n",
            "Epoch 6 Batch 5700 Loss 1.0623 Accuracy 0.4766\n",
            "Saving checkpoint for epoch 6 at ./drive/My Drive/transformer/ckpt/ckpt-15\n",
            "Time taken for 1 epoch: 1312.5267169475555 secs\n",
            "\n",
            "Start of epocch 7\n",
            "Epoch 7 Batch 0 Loss 1.1740 Accuracy 0.4605\n",
            "Epoch 7 Batch 50 Loss 1.1761 Accuracy 0.4613\n",
            "Epoch 7 Batch 100 Loss 1.1633 Accuracy 0.4576\n",
            "Epoch 7 Batch 150 Loss 1.1616 Accuracy 0.4588\n",
            "Epoch 7 Batch 200 Loss 1.1624 Accuracy 0.4585\n",
            "Epoch 7 Batch 250 Loss 1.1552 Accuracy 0.4594\n",
            "Epoch 7 Batch 300 Loss 1.1558 Accuracy 0.4591\n",
            "Epoch 7 Batch 350 Loss 1.1548 Accuracy 0.4595\n",
            "Epoch 7 Batch 400 Loss 1.1557 Accuracy 0.4598\n",
            "Epoch 7 Batch 450 Loss 1.1523 Accuracy 0.4602\n",
            "Epoch 7 Batch 500 Loss 1.1510 Accuracy 0.4603\n",
            "Epoch 7 Batch 550 Loss 1.1498 Accuracy 0.4599\n",
            "Epoch 7 Batch 600 Loss 1.1519 Accuracy 0.46\n",
            "Epoch 7 Batch 650 Loss 1.1516 Accuracy 0.4602\n",
            "Epoch 7 Batch 700 Loss 1.1504 Accuracy 0.4606\n",
            "Epoch 7 Batch 750 Loss 1.1489 Accuracy 0.461\n",
            "Epoch 7 Batch 800 Loss 1.1470 Accuracy 0.4612\n",
            "Epoch 7 Batch 850 Loss 1.1469 Accuracy 0.4613\n",
            "Epoch 7 Batch 900 Loss 1.1457 Accuracy 0.4611\n",
            "Epoch 7 Batch 950 Loss 1.1458 Accuracy 0.4614\n",
            "Epoch 7 Batch 1000 Loss 1.1433 Accuracy 0.4614\n",
            "Epoch 7 Batch 1050 Loss 1.1437 Accuracy 0.4614\n",
            "Epoch 7 Batch 1100 Loss 1.1421 Accuracy 0.4617\n",
            "Epoch 7 Batch 1150 Loss 1.1394 Accuracy 0.4618\n",
            "Epoch 7 Batch 1200 Loss 1.1384 Accuracy 0.4619\n",
            "Epoch 7 Batch 1250 Loss 1.1363 Accuracy 0.4623\n",
            "Epoch 7 Batch 1300 Loss 1.1346 Accuracy 0.4626\n",
            "Epoch 7 Batch 1350 Loss 1.1329 Accuracy 0.4632\n",
            "Epoch 7 Batch 1400 Loss 1.1308 Accuracy 0.4638\n",
            "Epoch 7 Batch 1450 Loss 1.1280 Accuracy 0.4645\n",
            "Epoch 7 Batch 1500 Loss 1.1253 Accuracy 0.4652\n",
            "Epoch 7 Batch 1550 Loss 1.1220 Accuracy 0.4659\n",
            "Epoch 7 Batch 1600 Loss 1.1203 Accuracy 0.4667\n",
            "Epoch 7 Batch 1650 Loss 1.1183 Accuracy 0.4673\n",
            "Epoch 7 Batch 1700 Loss 1.1159 Accuracy 0.468\n",
            "Epoch 7 Batch 1750 Loss 1.1134 Accuracy 0.4688\n",
            "Epoch 7 Batch 1800 Loss 1.1114 Accuracy 0.4696\n",
            "Epoch 7 Batch 1850 Loss 1.1099 Accuracy 0.4704\n",
            "Epoch 7 Batch 1900 Loss 1.1083 Accuracy 0.4712\n",
            "Epoch 7 Batch 1950 Loss 1.1067 Accuracy 0.4719\n",
            "Epoch 7 Batch 2000 Loss 1.1044 Accuracy 0.4726\n",
            "Epoch 7 Batch 2050 Loss 1.1023 Accuracy 0.473\n",
            "Epoch 7 Batch 2100 Loss 1.0998 Accuracy 0.4734\n",
            "Epoch 7 Batch 2150 Loss 1.0969 Accuracy 0.4736\n",
            "Epoch 7 Batch 2200 Loss 1.0940 Accuracy 0.4738\n",
            "Epoch 7 Batch 2250 Loss 1.0911 Accuracy 0.474\n",
            "Epoch 7 Batch 2300 Loss 1.0883 Accuracy 0.4742\n",
            "Epoch 7 Batch 2350 Loss 1.0853 Accuracy 0.4744\n",
            "Epoch 7 Batch 2400 Loss 1.0820 Accuracy 0.4746\n",
            "Epoch 7 Batch 2450 Loss 1.0790 Accuracy 0.4749\n",
            "Epoch 7 Batch 2500 Loss 1.0763 Accuracy 0.4751\n",
            "Epoch 7 Batch 2550 Loss 1.0733 Accuracy 0.4755\n",
            "Epoch 7 Batch 2600 Loss 1.0705 Accuracy 0.4758\n",
            "Epoch 7 Batch 2650 Loss 1.0675 Accuracy 0.4762\n",
            "Epoch 7 Batch 2700 Loss 1.0652 Accuracy 0.4766\n",
            "Epoch 7 Batch 2750 Loss 1.0631 Accuracy 0.477\n",
            "Epoch 7 Batch 2800 Loss 1.0605 Accuracy 0.4772\n",
            "Epoch 7 Batch 2850 Loss 1.0583 Accuracy 0.4775\n",
            "Epoch 7 Batch 2900 Loss 1.0565 Accuracy 0.4778\n",
            "Epoch 7 Batch 2950 Loss 1.0544 Accuracy 0.4781\n",
            "Epoch 7 Batch 3000 Loss 1.0522 Accuracy 0.4784\n",
            "Epoch 7 Batch 3050 Loss 1.0504 Accuracy 0.4786\n",
            "Epoch 7 Batch 3100 Loss 1.0487 Accuracy 0.4789\n",
            "Epoch 7 Batch 3150 Loss 1.0467 Accuracy 0.4792\n",
            "Epoch 7 Batch 3200 Loss 1.0447 Accuracy 0.4794\n",
            "Epoch 7 Batch 3250 Loss 1.0427 Accuracy 0.4796\n",
            "Epoch 7 Batch 3300 Loss 1.0404 Accuracy 0.4799\n",
            "Epoch 7 Batch 3350 Loss 1.0381 Accuracy 0.4803\n",
            "Epoch 7 Batch 3400 Loss 1.0362 Accuracy 0.4805\n",
            "Epoch 7 Batch 3450 Loss 1.0347 Accuracy 0.4808\n",
            "Epoch 7 Batch 3500 Loss 1.0327 Accuracy 0.481\n",
            "Epoch 7 Batch 3550 Loss 1.0311 Accuracy 0.4814\n",
            "Epoch 7 Batch 3600 Loss 1.0292 Accuracy 0.4817\n",
            "Epoch 7 Batch 3650 Loss 1.0275 Accuracy 0.482\n",
            "Epoch 7 Batch 3700 Loss 1.0260 Accuracy 0.4823\n",
            "Epoch 7 Batch 3750 Loss 1.0243 Accuracy 0.4827\n",
            "Epoch 7 Batch 3800 Loss 1.0229 Accuracy 0.483\n",
            "Epoch 7 Batch 3850 Loss 1.0212 Accuracy 0.4834\n",
            "Epoch 7 Batch 3900 Loss 1.0196 Accuracy 0.4837\n",
            "Epoch 7 Batch 3950 Loss 1.0186 Accuracy 0.484\n",
            "Epoch 7 Batch 4000 Loss 1.0173 Accuracy 0.4843\n",
            "Epoch 7 Batch 4050 Loss 1.0158 Accuracy 0.4846\n",
            "Epoch 7 Batch 4100 Loss 1.0149 Accuracy 0.4848\n",
            "Epoch 7 Batch 4150 Loss 1.0143 Accuracy 0.4849\n",
            "Epoch 7 Batch 4200 Loss 1.0145 Accuracy 0.4849\n",
            "Epoch 7 Batch 4250 Loss 1.0151 Accuracy 0.4848\n",
            "Epoch 7 Batch 4300 Loss 1.0158 Accuracy 0.4848\n",
            "Epoch 7 Batch 4350 Loss 1.0169 Accuracy 0.4847\n",
            "Epoch 7 Batch 4400 Loss 1.0181 Accuracy 0.4845\n",
            "Epoch 7 Batch 4450 Loss 1.0191 Accuracy 0.4843\n",
            "Epoch 7 Batch 4500 Loss 1.0202 Accuracy 0.4841\n",
            "Epoch 7 Batch 4550 Loss 1.0214 Accuracy 0.4839\n",
            "Epoch 7 Batch 4600 Loss 1.0227 Accuracy 0.4837\n",
            "Epoch 7 Batch 4650 Loss 1.0243 Accuracy 0.4834\n",
            "Epoch 7 Batch 4700 Loss 1.0257 Accuracy 0.4832\n",
            "Epoch 7 Batch 4750 Loss 1.0272 Accuracy 0.4831\n",
            "Epoch 7 Batch 4800 Loss 1.0285 Accuracy 0.4829\n",
            "Epoch 7 Batch 4850 Loss 1.0301 Accuracy 0.4828\n",
            "Epoch 7 Batch 4900 Loss 1.0312 Accuracy 0.4826\n",
            "Epoch 7 Batch 4950 Loss 1.0325 Accuracy 0.4824\n",
            "Epoch 7 Batch 5000 Loss 1.0337 Accuracy 0.4822\n",
            "Epoch 7 Batch 5050 Loss 1.0353 Accuracy 0.482\n",
            "Epoch 7 Batch 5100 Loss 1.0365 Accuracy 0.4818\n",
            "Epoch 7 Batch 5150 Loss 1.0378 Accuracy 0.4815\n",
            "Epoch 7 Batch 5200 Loss 1.0391 Accuracy 0.4812\n",
            "Epoch 7 Batch 5250 Loss 1.0403 Accuracy 0.4809\n",
            "Epoch 7 Batch 5300 Loss 1.0414 Accuracy 0.4807\n",
            "Epoch 7 Batch 5350 Loss 1.0428 Accuracy 0.4804\n",
            "Epoch 7 Batch 5400 Loss 1.0441 Accuracy 0.4802\n",
            "Epoch 7 Batch 5450 Loss 1.0451 Accuracy 0.4799\n",
            "Epoch 7 Batch 5500 Loss 1.0460 Accuracy 0.4796\n",
            "Epoch 7 Batch 5550 Loss 1.0469 Accuracy 0.4794\n",
            "Epoch 7 Batch 5600 Loss 1.0477 Accuracy 0.4792\n",
            "Epoch 7 Batch 5650 Loss 1.0487 Accuracy 0.4789\n",
            "Epoch 7 Batch 5700 Loss 1.0498 Accuracy 0.4787\n",
            "Saving checkpoint for epoch 7 at ./drive/My Drive/transformer/ckpt/ckpt-16\n",
            "Time taken for 1 epoch: 1323.9645717144012 secs\n",
            "\n",
            "Start of epocch 8\n",
            "Epoch 8 Batch 0 Loss 1.1034 Accuracy 0.4474\n",
            "Epoch 8 Batch 50 Loss 1.1767 Accuracy 0.4569\n",
            "Epoch 8 Batch 100 Loss 1.1691 Accuracy 0.4607\n",
            "Epoch 8 Batch 150 Loss 1.1639 Accuracy 0.4614\n",
            "Epoch 8 Batch 200 Loss 1.1600 Accuracy 0.4612\n",
            "Epoch 8 Batch 250 Loss 1.1539 Accuracy 0.4611\n",
            "Epoch 8 Batch 300 Loss 1.1520 Accuracy 0.4613\n",
            "Epoch 8 Batch 350 Loss 1.1532 Accuracy 0.4612\n",
            "Epoch 8 Batch 400 Loss 1.1493 Accuracy 0.462\n",
            "Epoch 8 Batch 450 Loss 1.1471 Accuracy 0.4623\n",
            "Epoch 8 Batch 500 Loss 1.1446 Accuracy 0.4622\n",
            "Epoch 8 Batch 550 Loss 1.1426 Accuracy 0.462\n",
            "Epoch 8 Batch 600 Loss 1.1402 Accuracy 0.4619\n",
            "Epoch 8 Batch 650 Loss 1.1406 Accuracy 0.4621\n",
            "Epoch 8 Batch 700 Loss 1.1403 Accuracy 0.4624\n",
            "Epoch 8 Batch 750 Loss 1.1400 Accuracy 0.4628\n",
            "Epoch 8 Batch 800 Loss 1.1392 Accuracy 0.4631\n",
            "Epoch 8 Batch 850 Loss 1.1397 Accuracy 0.4633\n",
            "Epoch 8 Batch 900 Loss 1.1383 Accuracy 0.4632\n",
            "Epoch 8 Batch 950 Loss 1.1373 Accuracy 0.4629\n",
            "Epoch 8 Batch 1000 Loss 1.1339 Accuracy 0.463\n",
            "Epoch 8 Batch 1050 Loss 1.1331 Accuracy 0.4632\n",
            "Epoch 8 Batch 1100 Loss 1.1311 Accuracy 0.4633\n",
            "Epoch 8 Batch 1150 Loss 1.1299 Accuracy 0.4636\n",
            "Epoch 8 Batch 1200 Loss 1.1282 Accuracy 0.4636\n",
            "Epoch 8 Batch 1250 Loss 1.1264 Accuracy 0.4639\n",
            "Epoch 8 Batch 1300 Loss 1.1237 Accuracy 0.4643\n",
            "Epoch 8 Batch 1350 Loss 1.1211 Accuracy 0.4647\n",
            "Epoch 8 Batch 1400 Loss 1.1194 Accuracy 0.4651\n",
            "Epoch 8 Batch 1450 Loss 1.1174 Accuracy 0.4659\n",
            "Epoch 8 Batch 1500 Loss 1.1155 Accuracy 0.4667\n",
            "Epoch 8 Batch 1550 Loss 1.1128 Accuracy 0.4676\n",
            "Epoch 8 Batch 1600 Loss 1.1105 Accuracy 0.4683\n",
            "Epoch 8 Batch 1650 Loss 1.1084 Accuracy 0.469\n",
            "Epoch 8 Batch 1700 Loss 1.1062 Accuracy 0.4698\n",
            "Epoch 8 Batch 1750 Loss 1.1033 Accuracy 0.4705\n",
            "Epoch 8 Batch 1800 Loss 1.1009 Accuracy 0.4713\n",
            "Epoch 8 Batch 1850 Loss 1.0990 Accuracy 0.4721\n",
            "Epoch 8 Batch 1900 Loss 1.0972 Accuracy 0.4729\n",
            "Epoch 8 Batch 1950 Loss 1.0954 Accuracy 0.4735\n",
            "Epoch 8 Batch 2000 Loss 1.0939 Accuracy 0.4741\n",
            "Epoch 8 Batch 2050 Loss 1.0919 Accuracy 0.4745\n",
            "Epoch 8 Batch 2100 Loss 1.0895 Accuracy 0.4747\n",
            "Epoch 8 Batch 2150 Loss 1.0869 Accuracy 0.475\n",
            "Epoch 8 Batch 2200 Loss 1.0843 Accuracy 0.4753\n",
            "Epoch 8 Batch 2250 Loss 1.0816 Accuracy 0.4755\n",
            "Epoch 8 Batch 2300 Loss 1.0790 Accuracy 0.4757\n",
            "Epoch 8 Batch 2350 Loss 1.0756 Accuracy 0.476\n",
            "Epoch 8 Batch 2400 Loss 1.0727 Accuracy 0.4763\n",
            "Epoch 8 Batch 2450 Loss 1.0692 Accuracy 0.4766\n",
            "Epoch 8 Batch 2500 Loss 1.0656 Accuracy 0.4769\n",
            "Epoch 8 Batch 2550 Loss 1.0626 Accuracy 0.4773\n",
            "Epoch 8 Batch 2600 Loss 1.0600 Accuracy 0.4777\n",
            "Epoch 8 Batch 2650 Loss 1.0571 Accuracy 0.4781\n",
            "Epoch 8 Batch 2700 Loss 1.0548 Accuracy 0.4784\n",
            "Epoch 8 Batch 2750 Loss 1.0524 Accuracy 0.4787\n",
            "Epoch 8 Batch 2800 Loss 1.0501 Accuracy 0.4789\n",
            "Epoch 8 Batch 2850 Loss 1.0479 Accuracy 0.4792\n",
            "Epoch 8 Batch 2900 Loss 1.0459 Accuracy 0.4795\n",
            "Epoch 8 Batch 2950 Loss 1.0439 Accuracy 0.4798\n",
            "Epoch 8 Batch 3000 Loss 1.0418 Accuracy 0.4801\n",
            "Epoch 8 Batch 3050 Loss 1.0398 Accuracy 0.4803\n",
            "Epoch 8 Batch 3100 Loss 1.0378 Accuracy 0.4806\n",
            "Epoch 8 Batch 3150 Loss 1.0359 Accuracy 0.481\n",
            "Epoch 8 Batch 3200 Loss 1.0339 Accuracy 0.4812\n",
            "Epoch 8 Batch 3250 Loss 1.0322 Accuracy 0.4815\n",
            "Epoch 8 Batch 3300 Loss 1.0298 Accuracy 0.4818\n",
            "Epoch 8 Batch 3350 Loss 1.0278 Accuracy 0.482\n",
            "Epoch 8 Batch 3400 Loss 1.0255 Accuracy 0.4822\n",
            "Epoch 8 Batch 3450 Loss 1.0236 Accuracy 0.4825\n",
            "Epoch 8 Batch 3500 Loss 1.0217 Accuracy 0.4829\n",
            "Epoch 8 Batch 3550 Loss 1.0199 Accuracy 0.4832\n",
            "Epoch 8 Batch 3600 Loss 1.0180 Accuracy 0.4835\n",
            "Epoch 8 Batch 3650 Loss 1.0164 Accuracy 0.4838\n",
            "Epoch 8 Batch 3700 Loss 1.0148 Accuracy 0.4842\n",
            "Epoch 8 Batch 3750 Loss 1.0132 Accuracy 0.4845\n",
            "Epoch 8 Batch 3800 Loss 1.0120 Accuracy 0.4848\n",
            "Epoch 8 Batch 3850 Loss 1.0105 Accuracy 0.4851\n",
            "Epoch 8 Batch 3900 Loss 1.0091 Accuracy 0.4854\n",
            "Epoch 8 Batch 3950 Loss 1.0078 Accuracy 0.4858\n",
            "Epoch 8 Batch 4000 Loss 1.0067 Accuracy 0.4862\n",
            "Epoch 8 Batch 4050 Loss 1.0056 Accuracy 0.4865\n",
            "Epoch 8 Batch 4100 Loss 1.0045 Accuracy 0.4867\n",
            "Epoch 8 Batch 4150 Loss 1.0038 Accuracy 0.4868\n",
            "Epoch 8 Batch 4200 Loss 1.0040 Accuracy 0.4868\n",
            "Epoch 8 Batch 4250 Loss 1.0045 Accuracy 0.4868\n",
            "Epoch 8 Batch 4300 Loss 1.0053 Accuracy 0.4867\n",
            "Epoch 8 Batch 4350 Loss 1.0061 Accuracy 0.4866\n",
            "Epoch 8 Batch 4400 Loss 1.0073 Accuracy 0.4864\n",
            "Epoch 8 Batch 4450 Loss 1.0088 Accuracy 0.4862\n",
            "Epoch 8 Batch 4500 Loss 1.0098 Accuracy 0.486\n",
            "Epoch 8 Batch 4550 Loss 1.0114 Accuracy 0.4858\n",
            "Epoch 8 Batch 4600 Loss 1.0129 Accuracy 0.4856\n",
            "Epoch 8 Batch 4650 Loss 1.0145 Accuracy 0.4853\n",
            "Epoch 8 Batch 4700 Loss 1.0159 Accuracy 0.4851\n",
            "Epoch 8 Batch 4750 Loss 1.0168 Accuracy 0.4849\n",
            "Epoch 8 Batch 4800 Loss 1.0180 Accuracy 0.4847\n",
            "Epoch 8 Batch 4850 Loss 1.0192 Accuracy 0.4846\n",
            "Epoch 8 Batch 4900 Loss 1.0205 Accuracy 0.4844\n",
            "Epoch 8 Batch 4950 Loss 1.0219 Accuracy 0.4843\n",
            "Epoch 8 Batch 5000 Loss 1.0234 Accuracy 0.484\n",
            "Epoch 8 Batch 5050 Loss 1.0244 Accuracy 0.4838\n",
            "Epoch 8 Batch 5100 Loss 1.0258 Accuracy 0.4835\n",
            "Epoch 8 Batch 5150 Loss 1.0271 Accuracy 0.4833\n",
            "Epoch 8 Batch 5200 Loss 1.0285 Accuracy 0.483\n",
            "Epoch 8 Batch 5250 Loss 1.0298 Accuracy 0.4828\n",
            "Epoch 8 Batch 5300 Loss 1.0312 Accuracy 0.4825\n",
            "Epoch 8 Batch 5350 Loss 1.0323 Accuracy 0.4822\n",
            "Epoch 8 Batch 5400 Loss 1.0335 Accuracy 0.4819\n",
            "Epoch 8 Batch 5450 Loss 1.0344 Accuracy 0.4817\n",
            "Epoch 8 Batch 5500 Loss 1.0352 Accuracy 0.4814\n",
            "Epoch 8 Batch 5550 Loss 1.0363 Accuracy 0.4811\n",
            "Epoch 8 Batch 5600 Loss 1.0374 Accuracy 0.4809\n",
            "Epoch 8 Batch 5650 Loss 1.0383 Accuracy 0.4806\n",
            "Epoch 8 Batch 5700 Loss 1.0393 Accuracy 0.4804\n",
            "Saving checkpoint for epoch 8 at ./drive/My Drive/transformer/ckpt/ckpt-17\n",
            "Time taken for 1 epoch: 1328.253008365631 secs\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Phoub0kF5Yhf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(input_sentence):\n",
        "  input_sentence = \\\n",
        "        [VOCAB_SIZE_EN - 2] + tokenizer_en.encode(input_sentence) + [VOCAB_SIZE_EN - 1]\n",
        "\n",
        "  enc_input = tf.expand_dims(input_sentence, axis = 0) #Batch dims\n",
        "\n",
        "  output = tf.expand_dims([VOCAB_SIZE_FR - 2], axis = 0)  #we have only start tag now in predictions, enable batchDims\n",
        "\n",
        "  for _ in range(MAXLEN):\n",
        "    predictions = transformer(enc_input, output, False)  #Note here that the output is dec op which is also dec ip, but the dec ip has a start tag and text is shifted right, but dec op wll be shifted left rel to the dec ip and will have the extra final predicted word\n",
        "\n",
        "    prediction = predictions[:, -1:, :]   #size of predictions is (1, seq_length, vocab_size_fr), taking the last element from seq which is at each stage the predicted word, each word is a array of preds of size vocab_size_fr\n",
        "\n",
        "    predicted_id = tf.cast(tf.argmax(prediction, axis = -1), tf.int32)  #argmax along vocab_size arg to pull out highest of softmax preds\n",
        "\n",
        "    if predicted_id == VOCAB_SIZE_FR - 1:\n",
        "      return tf.squeeze(output, axis = 0)\n",
        "\n",
        "    output = tf.concat([output, predicted_id], axis = -1)\n",
        "\n",
        "  return tf.squeeze(output, axis = 0)   \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9CtXYbP5raG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(sentence):\n",
        "  output = evaluate(sentence).numpy()  #Numpy array is easier to handle than tensor\n",
        "\n",
        "  predicted_ans = tokenizer_fr.decode(\n",
        "      [i for i in output if i < VOCAB_SIZE_FR - 2]    #Do npt decode the start and end tag\n",
        "  )\n",
        "\n",
        "  print(\"Input : {}\".format(sentence))\n",
        "  print(\"Output: {}\".format(predicted_ans))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fi5j4gzg5rFV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ff949b48-47f7-4947-ad86-c7b263c15e8a"
      },
      "source": [
        "translate(\"Yesterday\")"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input : Yesterday\n",
            "Output: Hier, Hier\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4ERVrbL6ocT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0a927bbb-5d38-4e52-b671-93f9776224e7"
      },
      "source": [
        "translate(\"This is a really powerful tool!\")"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input : This is a really powerful tool!\n",
            "Output: C'est l un instrument de la plus puissant!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzXaxynJs3bk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "aa791838-5565-495e-e5ad-050477ab6604"
      },
      "source": [
        "translate(\"Good morning, how are you doing?\")"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input : Good morning, how are you doing?\n",
            "Output: Que faisons-nous ce matin, comment faisons-nous ?\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}