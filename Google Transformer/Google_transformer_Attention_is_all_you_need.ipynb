{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Google transformer - Attention is all you need",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4e5_trC_LhX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import re\n",
        "import os\n",
        "import time\n",
        "from google.colab import drive\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQL1qk7v_-S8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQX5NGCwAbxx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "61ac6171-3036-415e-9116-1d834b09c9d3"
      },
      "source": [
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jptZOwMAxeb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open (\"/content/drive/My Drive/transformer/data/europarl-v7.fr-en.en\",\n",
        "           mode = \"r\",\n",
        "           encoding = \"utf-8\") as f:\n",
        "  europarl_en = f.read()\n",
        "\n",
        "with open (\"/content/drive/My Drive/transformer/data/europarl-v7.fr-en.fr\",\n",
        "           mode = \"r\",\n",
        "           encoding = \"utf-8\") as f:\n",
        "  europarl_fr = f.read()\n",
        "\n",
        "\n",
        "with open (\"/content/drive/My Drive/transformer/data/nonbreaking_prefix.en\",\n",
        "           mode = \"r\",\n",
        "           encoding = \"utf-8\") as f:\n",
        "  non_breaking_prefix_en = f.read()    \n",
        "\n",
        "\n",
        "with open (\"/content/drive/My Drive/transformer/data/nonbreaking_prefix.fr\",\n",
        "           mode = \"r\",\n",
        "           encoding = \"utf-8\") as f:\n",
        "  non_breaking_prefix_fr = f.read()  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7xSMtN1BwWW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f4f54720-3bd4-4b15-dfe8-904193449217"
      },
      "source": [
        "europarl_en[:100]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Resumption of the session\\nI declare resumed the session of the European Parliament adjourned on Frid'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6W-dF5EGtOG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c2712422-2f35-4951-bd40-8180974575d7"
      },
      "source": [
        "non_breaking_prefix_en[:5]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'a\\nb\\nc'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_9ZGeMVylMl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a145f40b-6a58-4a34-c899-7e15797ce44d"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrfcAgg1B3ml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "non_breaking_prefix_en = non_breaking_prefix_en.split(\"\\n\")\n",
        "non_breaking_prefix_en = [' ' + pref + '.' for pref in non_breaking_prefix_en]\n",
        "non_breaking_prefix_fr = non_breaking_prefix_fr.split(\"\\n\")\n",
        "non_breaking_prefix_fr = [' ' + pref + '.' for pref in non_breaking_prefix_fr]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2M2vZbFKq9O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "83115b38-c693-40d8-c633-f05aaeb3eb1d"
      },
      "source": [
        "non_breaking_prefix_en[:5]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' a.', ' b.', ' c.', ' d.', ' e.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SBJr8asK46m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_en = europarl_en  #Here we distinguish the above NBPrefixes from the fullstops by adding a ### after the prefix's dot, and removing the .###\n",
        "for prefix in non_breaking_prefix_en:\n",
        "  corpus_en = corpus_en.replace(prefix, prefix + \"###\")\n",
        "corpus_en = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".###\" , corpus_en)  #In regex, whatever is in () are not removed, only occurences are searched, and | is for or\n",
        "corpus_en = re.sub(r\"\\.###\", '', corpus_en)\n",
        "corpus_en = re.sub(r\"  +\", ' ', corpus_en)\n",
        "corpus_en = corpus_en.split(\"\\n\")    #To make the huge list into a normal one with spaces\n",
        "\n",
        "\n",
        "corpus_fr = europarl_fr\n",
        "for prefix in non_breaking_prefix_fr:\n",
        "  corpus_fr = corpus_fr.replace(prefix, prefix + \"###\")\n",
        "corpus_fr = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".###\" , corpus_fr)\n",
        "corpus_fr = re.sub(r\"\\.###\", '', corpus_fr)\n",
        "corpus_fr = re.sub(r\"  +\", ' ', corpus_fr)\n",
        "corpus_fr = corpus_fr.split(\"\\n\")  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YMRBSppNgpO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    corpus_en, target_vocab_size=2**13)\n",
        "tokenizer_fr = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    corpus_fr, target_vocab_size=2**13)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVI6fO4_TTNy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCAB_SIZE_EN = tokenizer_en.vocab_size + 2 #2 extra for the start and end token\n",
        "VOCAB_SIZE_FR = tokenizer_fr.vocab_size + 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQ_iOle_TTR9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Prepare the inputs and target op for training like - <start> text <end>\n",
        "inputs = [[VOCAB_SIZE_EN - 2] + tokenizer_en.encode(sentence) + [VOCAB_SIZE_EN - 1]\n",
        "          for sentence in corpus_en]\n",
        "\n",
        "outputs = [[VOCAB_SIZE_FR - 2] + tokenizer_fr.encode(sentence) + [VOCAB_SIZE_FR - 1] \n",
        "           for sentence in corpus_fr]          "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pU4Q_f0UwCB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Removing extra long sentences to prevent delay in preprocessing of the corpus\n",
        "MAXLEN = 20\n",
        "\n",
        "idx_remove = [count for count, sentence in enumerate(inputs)\n",
        "              if len(sentence) > MAXLEN]  #If we pass the entire corpus at once, RAM may exhaust, hence pass one by one\n",
        "\n",
        "for idx in reversed(idx_remove): #if no reversal, one deletion will push the index forward by 1, we will delete the wrong words\n",
        "  del inputs[idx]   #Since the ip and op text need to correspond to each other, we need to truncate the same idx\n",
        "  del outputs[idx]    \n",
        "\n",
        "idx_remove = [count for count, sentence in enumerate(outputs)\n",
        "              if len(sentence) > MAXLEN]\n",
        "\n",
        "for idx in reversed(idx_remove):\n",
        "  del inputs[idx]\n",
        "  del outputs[idx]                          "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JglqakT2UwQt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Padding\n",
        "#0 is a value not used by the tokenizer, hence safe to pad\n",
        "#The algo used by the transformer (MASKING) will anyway not touch padded data\n",
        "\n",
        "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
        "                                                       value = 0,\n",
        "                                                       padding = 'post',\n",
        "                                                       maxlen = MAXLEN)\n",
        "\n",
        "outputs = tf.keras.preprocessing.sequence.pad_sequences(outputs,\n",
        "                                                        value = 0,\n",
        "                                                        padding = 'post',\n",
        "                                                        maxlen = MAXLEN)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4H3RPhRtZ_YW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 20000 #For shuffling acc to BufSize and then batching\n",
        "\n",
        "#Cache stores dataset in local storage, faster loading\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n",
        "dataset = dataset.cache()\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE) #This starts requesting data before the execution of stmt is initialized"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVOjWV_ygo5Q",
        "colab_type": "text"
      },
      "source": [
        "Positional encoding formulae:\n",
        "\n",
        "$PE_{(pos,2i)} =\\sin(pos/10000^{2i/dmodel})$\n",
        "\n",
        "$PE_{(pos,2i+1)} =\\cos(pos/10000^{2i/dmodel})$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EC2Zg13rZ_bo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionalEncoding(layers.Layer):\n",
        "  def __init__(self):\n",
        "    super(PositionalEncoding, self).__init__()  #This will initialize the Layer class properties and complete inheritance\n",
        "\n",
        "  def get_angles(self, pos, i, d_model):\n",
        "    #pos refers to the index positions of the sequence, to be encoded\n",
        "    #embedding size of model is dmodel, specific iterable denoted by i\n",
        "    # pos: (seq_length, 1) and #i : (1, d_model)\n",
        "\n",
        "    angles = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model)) #// takes care of even and odd formula\n",
        "    return pos * angles #shape is seq_length, d_model\n",
        "\n",
        "  def call(self, inputs):\n",
        "      #to return inputs + encoding\n",
        "      seq_length = inputs.shape.as_list()[-2]\n",
        "      d_model = inputs.shape.as_list()[-1]\n",
        "      angles = self.get_angles(np.arange(seq_length)[:, np.newaxis], \n",
        "                               np.arange(d_model)[np.newaxis, :],\n",
        "                               d_model)   #adding new dims to get # pos: (seq_length, 1) and #i : (1, d_model)\n",
        "      angles[:, 0::2] = np.sin(angles[:, 0::2]) #from 0, step 2 - even\n",
        "      angles[:, 1::2] = np.cos(angles[:, 1::2]) #from 1, step 2 - odd\n",
        "      pos_encoding = angles[np.newaxis, ...] #This is for a batch dimension\n",
        "      return inputs + tf.cast(pos_encoding, tf.float32)  #This is to cast as a tensor object\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKYcIekWHzdF",
        "colab_type": "text"
      },
      "source": [
        "$Attention(Q, K, V ) = \\text{softmax}\\left(\\dfrac{QK^T}{\\sqrt{d_k}}\\right)V $"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdrjUHdCHnVg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scaled_dot_product_attention(queries, keys, values, mask):\n",
        "  product = tf.matmul(queries, keys, transpose_b = True)\n",
        "  keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32)  #The last axis corresponds to the embedding dimension\n",
        "  scaled_product = product / tf.math.sqrt(keys_dim)\n",
        "\n",
        "  if mask is not None:\n",
        "    scaled_product += (mask * -1e9)  #if the value of the scaled product is zero, it will not have any effect on final ans (padded zeros) * -infinite gives 0 after softmax (NO CONTRIBUTION)\n",
        "\n",
        "  attention = tf.matmul(tf.nn.softmax(scaled_product, axis = -1), values)  #Softmax applied across the last dimension which corr to weights of V, has to sum up to 1 (final op matrix)\n",
        "\n",
        "  return attention"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aH5-FhKxcUVs",
        "colab_type": "text"
      },
      "source": [
        "### Multi-head attention sublayer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ja7WiPHoHnck",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttention(layers.Layer):\n",
        "\n",
        "  def __init__(self, nb_proj):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.nb_proj = nb_proj\n",
        "\n",
        "  def build(self, input_shape): #Input shape has all required dims, build is a function called when the MHA layer is called the first time, as we need d_model for the linear layers which is not accessible from init, as the layer class attributes are called, so build is used\n",
        "    self.d_model = input_shape[-1]\n",
        "    assert self.d_model % self.nb_proj == 0 #make sure it is divisible into subspaces\n",
        "\n",
        "    self.d_proj = self.d_model // self.nb_proj\n",
        "\n",
        "    self.query_lin = layers.Dense(units = self.d_model)\n",
        "    self.key_lin = layers.Dense(units = self.d_model)\n",
        "    self.value_lin = layers.Dense(units = self.d_model)\n",
        "\n",
        "    self.final_lin = layers.Dense(units = self.d_model)\n",
        "\n",
        "\n",
        "\n",
        "  def split_proj(self, inputs, batch_size): # inputs: (batch_size, seq_length, d_model)\n",
        "    shape = (batch_size,\n",
        "             -1,\n",
        "             self.nb_proj,\n",
        "             self.d_proj)\n",
        "    split_inputs = tf.reshape(inputs, shape = shape)  # (batch_size, seq_length, nb_proj, d_proj)\n",
        "    return tf.transpose(split_inputs, perm = [0, 2, 1, 3]) # (batch_size, nb_proj, seq_length, d_proj)\n",
        "\n",
        "\n",
        "  def call(self, queries, keys, values, mask):\n",
        "\n",
        "    batch_size = tf.shape(queries)[0]\n",
        "    queries = self.query_lin(queries) \n",
        "    keys = self.query_lin(keys)\n",
        "    values = self.query_lin(values) \n",
        "\n",
        "    queries = self.split_proj(queries, batch_size)\n",
        "    keys = self.split_proj(keys, batch_size)\n",
        "    values = self.split_proj(values, batch_size)\n",
        "\n",
        "    attention = scaled_dot_product_attention(queries, keys, values, mask)\n",
        "\n",
        "    #Recombine back into original space dims\n",
        "\n",
        "    attention = tf.transpose(attention, perm = [0, 2, 1, 3])\n",
        "\n",
        "    concat_attention = tf.reshape(attention,\n",
        "                                  shape = (batch_size, -1, self.d_model))\n",
        "    \n",
        "    outputs = self.final_lin(concat_attention)\n",
        "\n",
        "    return outputs\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ny0nXUp-cPU-",
        "colab_type": "text"
      },
      "source": [
        "## Encoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49gg9SBCcoyJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(layers.Layer):\n",
        "    \n",
        "    def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
        "      super(EncoderLayer, self).__init__()\n",
        "      self.FFN_units = FFN_units\n",
        "      self.nb_proj = nb_proj\n",
        "      self.dropout_rate = dropout_rate\n",
        "\n",
        "    def build(self, input_shape):  \n",
        "      self.d_model = input_shape[-1]\n",
        "\n",
        "      self.multi_head_attention = MultiHeadAttention(self.nb_proj)\n",
        "      self.dropout_1 = layers.Dropout(rate = self.dropout_rate)\n",
        "      self.normal_1 = layers.LayerNormalization(epsilon = 1e-6)\n",
        "      self.dense_1 = layers.Dense(units = self.FFN_units, activation = 'relu')\n",
        "      self.dense_2 = layers.Dense(units = self.d_model)\n",
        "      self.dropout_2 = layers.Dropout(rate = self.dropout_rate)\n",
        "      self.normal_2 = layers.LayerNormalization(epsilon = 1e-6)\n",
        "\n",
        "    def call(self, inputs, mask, training):\n",
        "      attention = self.multi_head_attention(inputs, inputs, inputs, mask)\n",
        "      attention = self.dropout_1(attention, training = training)\n",
        "      attention = self.normal_1(attention + inputs) #Residual connection to ease backprop\n",
        "\n",
        "      outputs = self.dense_1(attention)\n",
        "      outputs = self.dense_2(outputs)\n",
        "      outputs = self.dropout_2(outputs, training = training)\n",
        "      outputs = self.normal_2(outputs + attention)\n",
        "\n",
        "      return outputs\n",
        "\n",
        "      #How to build this : first define the call structure as per the process\n",
        "      #then go back to build and define each layer one by one\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyr8PTOAg4Me",
        "colab_type": "text"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "58qWJ8kecvKz",
        "colab": {}
      },
      "source": [
        "class Encoder(layers.Layer):\n",
        "\n",
        "  def __init__(self, \n",
        "               nb_layers,\n",
        "               FFN_units,\n",
        "               nb_proj,\n",
        "               dropout_rate,\n",
        "               vocab_size,\n",
        "               d_model,\n",
        "               name = \"encoder\"\n",
        "               ):\n",
        "    super(Encoder, self).__init__(name = name)\n",
        "    self.nb_layers = nb_layers\n",
        "    self.d_model = d_model\n",
        "\n",
        "    self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "    self.pos_encoding = PositionalEncoding()\n",
        "    self.dropout = layers.Dropout(rate = dropout_rate)\n",
        "    self.enc_layers = [EncoderLayer(FFN_units, nb_proj, dropout_rate)\n",
        "                      for _ in range(nb_layers)]\n",
        "  \n",
        "\n",
        "  def call(self, inputs, mask, training):\n",
        "    outputs = self.embedding(inputs)\n",
        "    outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    outputs = self.pos_encoding(outputs)\n",
        "    outputs = self.dropout(outputs, training)\n",
        "\n",
        "    for i in range(self.nb_layers):\n",
        "      outputs = self.enc_layers[i](outputs, mask, training)\n",
        "\n",
        "\n",
        "    return outputs  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCxekOO6omIv",
        "colab_type": "text"
      },
      "source": [
        "## Decoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UAZ1RwSHnjK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderLayer(layers.Layer):\n",
        "\n",
        "  def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "    self.FFN_units = FFN_units\n",
        "    self.nb_proj = nb_proj\n",
        "    self.dropout_rate = dropout_rate\n",
        "\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.d_model = input_shape[-1]\n",
        "\n",
        "    #Self multi-head attention\n",
        "    self.multi_head_attention_1 = MultiHeadAttention(self.nb_proj)\n",
        "    self.dropout_1 = layers.Dropout(rate = self.dropout_rate)\n",
        "    self.normal_1 = layers.LayerNormalization(epsilon = 1e-6)\n",
        "\n",
        "    # Multi head attention combined with encoder output    \n",
        "    self.multi_head_attention_2 = MultiHeadAttention(self.nb_proj)\n",
        "    self.dropout_2 = layers.Dropout(rate = self.dropout_rate)\n",
        "    self.normal_2 = layers.LayerNormalization(epsilon = 1e-6)  \n",
        "\n",
        "    #FFN\n",
        "    self.dense_1 = layers.Dense(units = self.FFN_units, activation = 'relu')\n",
        "    self.dense_2 = layers.Dense(units = self.d_model) \n",
        "    self.dropout_3 = layers.Dropout(rate = self.dropout_rate)\n",
        "    self.normal_3 = layers.LayerNormalization(epsilon = 1e-6)\n",
        "\n",
        "\n",
        "  def call(self, inputs, enc_outputs, mask_1, mask_2, training):  #bool train is to apply dropout or not\n",
        "    attention = self.multi_head_attention_1(inputs,\n",
        "                                            inputs,\n",
        "                                            inputs,  \n",
        "                                            mask_1)  #ip, ip, ip are queries, keys and values respectively\n",
        "    attention = self.dropout_1(attention, training)\n",
        "    attention = self.normal_1(attention + inputs)\n",
        "\n",
        "    attention_2 = self.multi_head_attention_2(attention,\n",
        "                                              enc_outputs, \n",
        "                                              enc_outputs,\n",
        "                                              mask_2)\n",
        "    attention_2 = self.dropout_2(attention_2, training)\n",
        "    attention_2 = self.normal_2(attention_2 + attention)\n",
        "\n",
        "    outputs = self.dense_1(attention_2)\n",
        "    outputs = self.dense_2(outputs)\n",
        "    outputs = self.dropout_3(outputs, training)\n",
        "    outputs = self.normal_3(outputs + attention_2)\n",
        "\n",
        "    return outputs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUq6XlkRuii8",
        "colab_type": "text"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RE7dFR_3uiGh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(layers.Layer):\n",
        "\n",
        "  def __init__(self, \n",
        "               nb_layers,\n",
        "               FFN_units,\n",
        "               nb_proj,\n",
        "               dropout_rate,\n",
        "               vocab_size,\n",
        "               d_model,\n",
        "               name = \"decoder\"):\n",
        "    super(Decoder, self).__init__(name = name)\n",
        "    self.d_model = d_model\n",
        "    self.nb_layers = nb_layers\n",
        "\n",
        "    self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "    self.pos_encoding = PositionalEncoding()\n",
        "    self.dropout = layers.Dropout(rate = dropout_rate)\n",
        "\n",
        "    self.dec_layers = [DecoderLayer(FFN_units,\n",
        "                                    nb_proj,\n",
        "                                    dropout_rate)\n",
        "                      for _ in range(nb_layers)]\n",
        "\n",
        "    \n",
        "  def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "    outputs = self.embedding(inputs)\n",
        "    outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    outputs = self.pos_encoding(outputs)\n",
        "    outputs = self.dropout(outputs, training)\n",
        "\n",
        "    for i in range(self.nb_layers):\n",
        "      outputs = self.dec_layers[i](outputs,\n",
        "                                   enc_outputs,\n",
        "                                   mask_1,\n",
        "                                   mask_2,\n",
        "                                   training)\n",
        "\n",
        "      return outputs  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRJQbTrHxI0J",
        "colab_type": "text"
      },
      "source": [
        "## Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEV31AvNcuRp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Here we also define the 2 masks used\n",
        "\n",
        "class Transformer(tf.keras.Model):\n",
        "\n",
        "  def __init__(self,\n",
        "               vocab_size_enc,\n",
        "               vocab_size_dec,\n",
        "               d_model,\n",
        "               nb_layers,\n",
        "               FFN_units,\n",
        "               nb_proj,\n",
        "               dropout_rate,\n",
        "               name = \"transformer\"):\n",
        "    super(Transformer, self).__init__(name = name)\n",
        "\n",
        "    self.encoder = Encoder(nb_layers,\n",
        "                           FFN_units,\n",
        "                           nb_proj,\n",
        "                           dropout_rate,\n",
        "                           vocab_size_enc,\n",
        "                           d_model)\n",
        "    \n",
        "    self.decoder = Decoder(nb_layers,\n",
        "                           FFN_units,\n",
        "                           nb_proj,\n",
        "                           dropout_rate,\n",
        "                           vocab_size_dec,\n",
        "                           d_model)    \n",
        "    \n",
        "    self.last_linear = layers.Dense(units = vocab_size_dec, name = \"lin_output\")\n",
        "\n",
        "  def create_padding_mask(self, seq):  #Seq here is tokenized, each word is a num, its not embedded yet\n",
        "\n",
        "    mask = tf.cast(tf.math.equal(seq, 0), tf.float32) #check for zeros for the padding token and place the zero for the mask there\n",
        "    return mask[:, tf.newaxis, tf.newaxis, :]   #first newaxis is for mask to be applied on each projection (dim = nb_proj) in the attention layer\n",
        "                                                #second newaxis is a broadcasting dimension, if mask is a row vector , the dim before it is empty, hence it broadcasts and we get the required masking matrix\n",
        "\n",
        "  def create_lookahead_mask(self, seq): #This mask is to prevent access of a future word j by word i, during train we provide the full word to the trans, but to predict a word n, it should not look beyond word n\n",
        "\n",
        "    seq_len = tf.shape(seq)[1]\n",
        "    lookahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0) #the -1 is the arg position for i < j, not used, and the 0 arg is for i > j, which is filled with 1 to get a UTMatrix (left)                                              \n",
        "                                                                                 #we convert this to a strict UT Right matrix so that word i does not have access to a future word j   \n",
        "    return lookahead_mask #tf.max will intelligently broadcast dims to the 4dim tensor\n",
        "\n",
        "  def call(self, enc_inputs, dec_inputs, training):\n",
        "    enc_mask = self.create_padding_mask(enc_inputs)\n",
        "    dec_mask_1 = tf.maximum(\n",
        "        self.create_padding_mask(dec_inputs),\n",
        "        self.create_lookahead_mask(dec_inputs)\n",
        "    )\n",
        "\n",
        "    #NOTE - a mask applied is a value of 1, We need to block the word i from seeing the future word j, so we use a URT matrix, where each 1 in the j > i part shows that the value is NOT CONSIDERED\n",
        "    #there are 3 matrices, Q, K and V\n",
        "    #Q is a matrix with the french translation in training phase\n",
        "    # here K = V, but the product between K and Q is a similarity norm, to find the dependance on output french words on the input\n",
        "    #This product acts as weights after softmax, for the V matrix, which is to simply reorganize all the text in V (english) for a better fit for the french translation\n",
        "    #This product is like a GLOBAL matrix which provides attention to the required input parts\n",
        "\n",
        "    dec_mask_2 = self.create_padding_mask(enc_inputs)  #For this mask, it is a mask applied to check the similarity between the french word matrix Q and english word matrix to group together the english sentence in such a way that the meaningful english words, wrt the french context word, is re-organized for a better fit\n",
        "                                                       #The shape of the output - a zero is applied by the mask wherever matrix A (english K) is zero due to the padding, and hence if n zeros, last n terms in Q are not touched (decoder_dims), hence enc_input matrix decides this mask\n",
        "    enc_outputs = self.encoder(enc_inputs, enc_mask, training)\n",
        "    dec_outputs = self.decoder(dec_inputs,\n",
        "                               enc_outputs,\n",
        "                               dec_mask_1,\n",
        "                               dec_mask_2,\n",
        "                               training)\n",
        "    \n",
        "    outputs = self.last_linear(dec_outputs)\n",
        "\n",
        "    return outputs\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tui9jVY5dt0z",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4JGlwEQcuTv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Hyper-parameters\n",
        "D_MODEL = 128 # 512  These second values are those used in the Google Research paper\n",
        "NB_LAYERS = 4 # 6 - Reduce all to ease computation\n",
        "FFN_UNITS = 512 # 2048\n",
        "NB_PROJ = 8 # 8\n",
        "DROPOUT_RATE = 0.1 # 0.1\n",
        "\n",
        "transformer = Transformer(vocab_size_enc=VOCAB_SIZE_EN,\n",
        "                          vocab_size_dec=VOCAB_SIZE_FR,\n",
        "                          d_model=D_MODEL,\n",
        "                          nb_layers=NB_LAYERS,\n",
        "                          FFN_units=FFN_UNITS,\n",
        "                          nb_proj=NB_PROJ,\n",
        "                          dropout_rate=DROPOUT_RATE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfvDqO13cuYG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True,\n",
        "                                                            reduction = \"none\") #indicating a softmax prediction and not to sum losses and reduce, keep each loss value instead\n",
        "\n",
        "def loss_function(target, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(target, 0))  \n",
        "  loss_ = loss_object(target, pred)   #This is to mask out all the padded zeros, to not evaluate those in the loss function\n",
        "  mask = tf.cast(mask, dtype = loss_.dtype)\n",
        "  loss_ *= mask  #calc each term in the loss function and mask out the padded terms\n",
        "\n",
        "  return tf.reduce_mean(loss_)\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name = \"train_loss\")\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5n54F--qNtS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This is the custom learning rate to be defined\n",
        "\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "\n",
        "  def __init__(self, d_model, warmup_steps = 4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "\n",
        "    self.d_model = tf.cast(d_model, tf.float32)\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)  \n",
        "\n",
        "learning_rate = CustomSchedule(D_MODEL)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
        "                                     beta_1 = 0.9,\n",
        "                                     beta_2 = 0.98,\n",
        "                                     epsilon = 1e-9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzToun1AqNj2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9ab77752-c0e3-4782-d598-04e656e53e0c"
      },
      "source": [
        "checkpoint_path = \"./drive/My Drive/transformer/ckpt/\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer = transformer,\n",
        "                           optimizer = optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep = 5)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print(\"Latest Checkpoint restored!\")"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Latest Checkpoint restored!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9iKiX_1qNXG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a36f3ffa-32a5-4cd1-af78-2d40e600e680"
      },
      "source": [
        "EPOCHS = 6\n",
        "for epoch in range(EPOCHS):\n",
        "  print(\"Start of epocch {}\".format(epoch + 1))\n",
        "  start = time.time()\n",
        "\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "\n",
        "  for (batch, (enc_inputs, targets)) in enumerate(dataset):\n",
        "    dec_inputs = targets[:, :-1]\n",
        "    dec_outputs_real = targets[:, 1:]\n",
        "    with tf.GradientTape() as tape:\n",
        "      predictions = transformer(enc_inputs, dec_inputs, True)\n",
        "      loss = loss_function(dec_outputs_real, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "    train_loss(loss)\n",
        "    train_accuracy(dec_outputs_real, predictions)\n",
        "\n",
        "    if batch % 50 == 0:\n",
        "      print(\"Epoch {} Batch {} Loss {:.4f} Accuracy {:.4}\".format(\n",
        "          epoch + 1, batch, train_loss.result(), train_accuracy.result()\n",
        "      )) \n",
        "\n",
        "  ckpt_save_path = ckpt_manager.save()\n",
        "  print(\"Saving checkpoint for epoch {} at {}\".format(epoch+1,\n",
        "                                                        ckpt_save_path))\n",
        "  print(\"Time taken for 1 epoch: {} secs\\n\".format(time.time() - start))   \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start of epocch 1\n",
            "Epoch 1 Batch 0 Loss 1.3769 Accuracy 0.44\n",
            "Epoch 1 Batch 50 Loss 1.4649 Accuracy 0.4147\n",
            "Epoch 1 Batch 100 Loss 1.4519 Accuracy 0.4165\n",
            "Epoch 1 Batch 150 Loss 1.4447 Accuracy 0.4161\n",
            "Epoch 1 Batch 200 Loss 1.4369 Accuracy 0.4153\n",
            "Epoch 1 Batch 250 Loss 1.4349 Accuracy 0.4159\n",
            "Epoch 1 Batch 300 Loss 1.4402 Accuracy 0.4152\n",
            "Epoch 1 Batch 350 Loss 1.4411 Accuracy 0.4153\n",
            "Epoch 1 Batch 400 Loss 1.4364 Accuracy 0.4157\n",
            "Epoch 1 Batch 450 Loss 1.4306 Accuracy 0.4155\n",
            "Epoch 1 Batch 500 Loss 1.4304 Accuracy 0.416\n",
            "Epoch 1 Batch 550 Loss 1.4293 Accuracy 0.4162\n",
            "Epoch 1 Batch 600 Loss 1.4276 Accuracy 0.4163\n",
            "Epoch 1 Batch 650 Loss 1.4267 Accuracy 0.4164\n",
            "Epoch 1 Batch 700 Loss 1.4265 Accuracy 0.4164\n",
            "Epoch 1 Batch 750 Loss 1.4247 Accuracy 0.4169\n",
            "Epoch 1 Batch 800 Loss 1.4248 Accuracy 0.4169\n",
            "Epoch 1 Batch 850 Loss 1.4233 Accuracy 0.4168\n",
            "Epoch 1 Batch 900 Loss 1.4217 Accuracy 0.4169\n",
            "Epoch 1 Batch 950 Loss 1.4203 Accuracy 0.4167\n",
            "Epoch 1 Batch 1000 Loss 1.4190 Accuracy 0.4166\n",
            "Epoch 1 Batch 1050 Loss 1.4175 Accuracy 0.4166\n",
            "Epoch 1 Batch 1100 Loss 1.4155 Accuracy 0.4165\n",
            "Epoch 1 Batch 1150 Loss 1.4150 Accuracy 0.4165\n",
            "Epoch 1 Batch 1200 Loss 1.4138 Accuracy 0.4167\n",
            "Epoch 1 Batch 1250 Loss 1.4133 Accuracy 0.4169\n",
            "Epoch 1 Batch 1300 Loss 1.4112 Accuracy 0.4173\n",
            "Epoch 1 Batch 1350 Loss 1.4095 Accuracy 0.4178\n",
            "Epoch 1 Batch 1400 Loss 1.4071 Accuracy 0.4183\n",
            "Epoch 1 Batch 1450 Loss 1.4052 Accuracy 0.4188\n",
            "Epoch 1 Batch 1500 Loss 1.4032 Accuracy 0.4195\n",
            "Epoch 1 Batch 1550 Loss 1.4007 Accuracy 0.42\n",
            "Epoch 1 Batch 1600 Loss 1.3993 Accuracy 0.4206\n",
            "Epoch 1 Batch 1650 Loss 1.3975 Accuracy 0.4213\n",
            "Epoch 1 Batch 1700 Loss 1.3958 Accuracy 0.4219\n",
            "Epoch 1 Batch 1750 Loss 1.3940 Accuracy 0.4226\n",
            "Epoch 1 Batch 1800 Loss 1.3927 Accuracy 0.4233\n",
            "Epoch 1 Batch 1850 Loss 1.3899 Accuracy 0.424\n",
            "Epoch 1 Batch 1900 Loss 1.3882 Accuracy 0.4247\n",
            "Epoch 1 Batch 1950 Loss 1.3867 Accuracy 0.4255\n",
            "Epoch 1 Batch 2000 Loss 1.3842 Accuracy 0.426\n",
            "Epoch 1 Batch 2050 Loss 1.3827 Accuracy 0.4264\n",
            "Epoch 1 Batch 2100 Loss 1.3800 Accuracy 0.4267\n",
            "Epoch 1 Batch 2150 Loss 1.3771 Accuracy 0.427\n",
            "Epoch 1 Batch 2200 Loss 1.3737 Accuracy 0.4272\n",
            "Epoch 1 Batch 2250 Loss 1.3701 Accuracy 0.4275\n",
            "Epoch 1 Batch 2300 Loss 1.3670 Accuracy 0.4276\n",
            "Epoch 1 Batch 2350 Loss 1.3634 Accuracy 0.4278\n",
            "Epoch 1 Batch 2400 Loss 1.3605 Accuracy 0.428\n",
            "Epoch 1 Batch 2450 Loss 1.3579 Accuracy 0.4283\n",
            "Epoch 1 Batch 2500 Loss 1.3548 Accuracy 0.4287\n",
            "Epoch 1 Batch 2550 Loss 1.3513 Accuracy 0.4291\n",
            "Epoch 1 Batch 2600 Loss 1.3484 Accuracy 0.4295\n",
            "Epoch 1 Batch 2650 Loss 1.3454 Accuracy 0.4299\n",
            "Epoch 1 Batch 2700 Loss 1.3429 Accuracy 0.4303\n",
            "Epoch 1 Batch 2750 Loss 1.3405 Accuracy 0.4307\n",
            "Epoch 1 Batch 2800 Loss 1.3380 Accuracy 0.4311\n",
            "Epoch 1 Batch 2850 Loss 1.3353 Accuracy 0.4314\n",
            "Epoch 1 Batch 2900 Loss 1.3328 Accuracy 0.4317\n",
            "Epoch 1 Batch 2950 Loss 1.3305 Accuracy 0.432\n",
            "Epoch 1 Batch 3000 Loss 1.3280 Accuracy 0.4323\n",
            "Epoch 1 Batch 3050 Loss 1.3259 Accuracy 0.4326\n",
            "Epoch 1 Batch 3100 Loss 1.3237 Accuracy 0.4329\n",
            "Epoch 1 Batch 3150 Loss 1.3212 Accuracy 0.4332\n",
            "Epoch 1 Batch 3200 Loss 1.3191 Accuracy 0.4334\n",
            "Epoch 1 Batch 3250 Loss 1.3169 Accuracy 0.4337\n",
            "Epoch 1 Batch 3300 Loss 1.3144 Accuracy 0.4339\n",
            "Epoch 1 Batch 3350 Loss 1.3121 Accuracy 0.4342\n",
            "Epoch 1 Batch 3400 Loss 1.3099 Accuracy 0.4345\n",
            "Epoch 1 Batch 3450 Loss 1.3077 Accuracy 0.4349\n",
            "Epoch 1 Batch 3500 Loss 1.3056 Accuracy 0.4352\n",
            "Epoch 1 Batch 3550 Loss 1.3037 Accuracy 0.4355\n",
            "Epoch 1 Batch 3600 Loss 1.3015 Accuracy 0.4359\n",
            "Epoch 1 Batch 3650 Loss 1.2994 Accuracy 0.4362\n",
            "Epoch 1 Batch 3700 Loss 1.2978 Accuracy 0.4365\n",
            "Epoch 1 Batch 3750 Loss 1.2958 Accuracy 0.4369\n",
            "Epoch 1 Batch 3800 Loss 1.2944 Accuracy 0.4372\n",
            "Epoch 1 Batch 3850 Loss 1.2928 Accuracy 0.4375\n",
            "Epoch 1 Batch 3900 Loss 1.2908 Accuracy 0.4379\n",
            "Epoch 1 Batch 3950 Loss 1.2891 Accuracy 0.4383\n",
            "Epoch 1 Batch 4000 Loss 1.2874 Accuracy 0.4386\n",
            "Epoch 1 Batch 4050 Loss 1.2862 Accuracy 0.4389\n",
            "Epoch 1 Batch 4100 Loss 1.2850 Accuracy 0.4391\n",
            "Epoch 1 Batch 4150 Loss 1.2843 Accuracy 0.4393\n",
            "Epoch 1 Batch 4200 Loss 1.2843 Accuracy 0.4394\n",
            "Epoch 1 Batch 4250 Loss 1.2844 Accuracy 0.4394\n",
            "Epoch 1 Batch 4300 Loss 1.2852 Accuracy 0.4393\n",
            "Epoch 1 Batch 4350 Loss 1.2859 Accuracy 0.4392\n",
            "Epoch 1 Batch 4400 Loss 1.2870 Accuracy 0.4391\n",
            "Epoch 1 Batch 4450 Loss 1.2877 Accuracy 0.4389\n",
            "Epoch 1 Batch 4500 Loss 1.2888 Accuracy 0.4388\n",
            "Epoch 1 Batch 4550 Loss 1.2895 Accuracy 0.4387\n",
            "Epoch 1 Batch 4600 Loss 1.2907 Accuracy 0.4386\n",
            "Epoch 1 Batch 4650 Loss 1.2920 Accuracy 0.4384\n",
            "Epoch 1 Batch 4700 Loss 1.2929 Accuracy 0.4383\n",
            "Epoch 1 Batch 4750 Loss 1.2940 Accuracy 0.4381\n",
            "Epoch 1 Batch 4800 Loss 1.2945 Accuracy 0.4381\n",
            "Epoch 1 Batch 4850 Loss 1.2959 Accuracy 0.4379\n",
            "Epoch 1 Batch 4900 Loss 1.2967 Accuracy 0.4378\n",
            "Epoch 1 Batch 4950 Loss 1.2978 Accuracy 0.4377\n",
            "Epoch 1 Batch 5000 Loss 1.2992 Accuracy 0.4375\n",
            "Epoch 1 Batch 5050 Loss 1.3003 Accuracy 0.4373\n",
            "Epoch 1 Batch 5100 Loss 1.3013 Accuracy 0.437\n",
            "Epoch 1 Batch 5150 Loss 1.3021 Accuracy 0.4369\n",
            "Epoch 1 Batch 5200 Loss 1.3032 Accuracy 0.4366\n",
            "Epoch 1 Batch 5250 Loss 1.3043 Accuracy 0.4364\n",
            "Epoch 1 Batch 5300 Loss 1.3053 Accuracy 0.4362\n",
            "Epoch 1 Batch 5350 Loss 1.3060 Accuracy 0.436\n",
            "Epoch 1 Batch 5400 Loss 1.3068 Accuracy 0.4358\n",
            "Epoch 1 Batch 5450 Loss 1.3076 Accuracy 0.4355\n",
            "Epoch 1 Batch 5500 Loss 1.3084 Accuracy 0.4353\n",
            "Epoch 1 Batch 5550 Loss 1.3089 Accuracy 0.4352\n",
            "Epoch 1 Batch 5600 Loss 1.3094 Accuracy 0.435\n",
            "Epoch 1 Batch 5650 Loss 1.3100 Accuracy 0.4348\n",
            "Epoch 1 Batch 5700 Loss 1.3105 Accuracy 0.4347\n",
            "Saving checkpoint for epoch 1 at ./drive/My Drive/transformer/ckpt/ckpt-4\n",
            "Time taken for 1 epoch: 1274.7527432441711 secs\n",
            "\n",
            "Start of epocch 2\n",
            "Epoch 2 Batch 0 Loss 1.3976 Accuracy 0.4375\n",
            "Epoch 2 Batch 50 Loss 1.3868 Accuracy 0.4205\n",
            "Epoch 2 Batch 100 Loss 1.3870 Accuracy 0.4229\n",
            "Epoch 2 Batch 150 Loss 1.3839 Accuracy 0.4212\n",
            "Epoch 2 Batch 200 Loss 1.3795 Accuracy 0.4219\n",
            "Epoch 2 Batch 250 Loss 1.3800 Accuracy 0.4231\n",
            "Epoch 2 Batch 300 Loss 1.3810 Accuracy 0.4229\n",
            "Epoch 2 Batch 350 Loss 1.3820 Accuracy 0.423\n",
            "Epoch 2 Batch 400 Loss 1.3808 Accuracy 0.4229\n",
            "Epoch 2 Batch 450 Loss 1.3800 Accuracy 0.4229\n",
            "Epoch 2 Batch 500 Loss 1.3756 Accuracy 0.4226\n",
            "Epoch 2 Batch 550 Loss 1.3749 Accuracy 0.4222\n",
            "Epoch 2 Batch 600 Loss 1.3757 Accuracy 0.4224\n",
            "Epoch 2 Batch 650 Loss 1.3743 Accuracy 0.4224\n",
            "Epoch 2 Batch 700 Loss 1.3727 Accuracy 0.4227\n",
            "Epoch 2 Batch 750 Loss 1.3716 Accuracy 0.4232\n",
            "Epoch 2 Batch 800 Loss 1.3715 Accuracy 0.4233\n",
            "Epoch 2 Batch 850 Loss 1.3723 Accuracy 0.4238\n",
            "Epoch 2 Batch 900 Loss 1.3720 Accuracy 0.4237\n",
            "Epoch 2 Batch 950 Loss 1.3694 Accuracy 0.424\n",
            "Epoch 2 Batch 1000 Loss 1.3676 Accuracy 0.4239\n",
            "Epoch 2 Batch 1050 Loss 1.3662 Accuracy 0.424\n",
            "Epoch 2 Batch 1100 Loss 1.3662 Accuracy 0.4242\n",
            "Epoch 2 Batch 1150 Loss 1.3654 Accuracy 0.4239\n",
            "Epoch 2 Batch 1200 Loss 1.3640 Accuracy 0.424\n",
            "Epoch 2 Batch 1250 Loss 1.3633 Accuracy 0.4243\n",
            "Epoch 2 Batch 1300 Loss 1.3614 Accuracy 0.4244\n",
            "Epoch 2 Batch 1350 Loss 1.3605 Accuracy 0.4247\n",
            "Epoch 2 Batch 1400 Loss 1.3588 Accuracy 0.4254\n",
            "Epoch 2 Batch 1450 Loss 1.3564 Accuracy 0.4261\n",
            "Epoch 2 Batch 1500 Loss 1.3546 Accuracy 0.4267\n",
            "Epoch 2 Batch 1550 Loss 1.3527 Accuracy 0.4275\n",
            "Epoch 2 Batch 1600 Loss 1.3506 Accuracy 0.4283\n",
            "Epoch 2 Batch 1650 Loss 1.3476 Accuracy 0.429\n",
            "Epoch 2 Batch 1700 Loss 1.3461 Accuracy 0.4296\n",
            "Epoch 2 Batch 1750 Loss 1.3437 Accuracy 0.4303\n",
            "Epoch 2 Batch 1800 Loss 1.3421 Accuracy 0.4309\n",
            "Epoch 2 Batch 1850 Loss 1.3400 Accuracy 0.4315\n",
            "Epoch 2 Batch 1900 Loss 1.3378 Accuracy 0.4321\n",
            "Epoch 2 Batch 1950 Loss 1.3354 Accuracy 0.4327\n",
            "Epoch 2 Batch 2000 Loss 1.3336 Accuracy 0.4334\n",
            "Epoch 2 Batch 2050 Loss 1.3313 Accuracy 0.4337\n",
            "Epoch 2 Batch 2100 Loss 1.3294 Accuracy 0.4341\n",
            "Epoch 2 Batch 2150 Loss 1.3264 Accuracy 0.4345\n",
            "Epoch 2 Batch 2200 Loss 1.3237 Accuracy 0.4347\n",
            "Epoch 2 Batch 2250 Loss 1.3202 Accuracy 0.4349\n",
            "Epoch 2 Batch 2300 Loss 1.3175 Accuracy 0.435\n",
            "Epoch 2 Batch 2350 Loss 1.3147 Accuracy 0.4354\n",
            "Epoch 2 Batch 2400 Loss 1.3113 Accuracy 0.4357\n",
            "Epoch 2 Batch 2450 Loss 1.3083 Accuracy 0.4359\n",
            "Epoch 2 Batch 2500 Loss 1.3046 Accuracy 0.4362\n",
            "Epoch 2 Batch 2550 Loss 1.3013 Accuracy 0.4366\n",
            "Epoch 2 Batch 2600 Loss 1.2980 Accuracy 0.437\n",
            "Epoch 2 Batch 2650 Loss 1.2953 Accuracy 0.4374\n",
            "Epoch 2 Batch 2700 Loss 1.2928 Accuracy 0.4378\n",
            "Epoch 2 Batch 2750 Loss 1.2901 Accuracy 0.4381\n",
            "Epoch 2 Batch 2800 Loss 1.2872 Accuracy 0.4384\n",
            "Epoch 2 Batch 2850 Loss 1.2850 Accuracy 0.4387\n",
            "Epoch 2 Batch 2900 Loss 1.2826 Accuracy 0.439\n",
            "Epoch 2 Batch 2950 Loss 1.2806 Accuracy 0.4394\n",
            "Epoch 2 Batch 3000 Loss 1.2787 Accuracy 0.4397\n",
            "Epoch 2 Batch 3050 Loss 1.2769 Accuracy 0.44\n",
            "Epoch 2 Batch 3100 Loss 1.2747 Accuracy 0.4404\n",
            "Epoch 2 Batch 3150 Loss 1.2725 Accuracy 0.4407\n",
            "Epoch 2 Batch 3200 Loss 1.2704 Accuracy 0.4409\n",
            "Epoch 2 Batch 3250 Loss 1.2680 Accuracy 0.4411\n",
            "Epoch 2 Batch 3300 Loss 1.2658 Accuracy 0.4414\n",
            "Epoch 2 Batch 3350 Loss 1.2635 Accuracy 0.4417\n",
            "Epoch 2 Batch 3400 Loss 1.2613 Accuracy 0.442\n",
            "Epoch 2 Batch 3450 Loss 1.2593 Accuracy 0.4423\n",
            "Epoch 2 Batch 3500 Loss 1.2575 Accuracy 0.4426\n",
            "Epoch 2 Batch 3550 Loss 1.2557 Accuracy 0.443\n",
            "Epoch 2 Batch 3600 Loss 1.2539 Accuracy 0.4434\n",
            "Epoch 2 Batch 3650 Loss 1.2515 Accuracy 0.4438\n",
            "Epoch 2 Batch 3700 Loss 1.2499 Accuracy 0.444\n",
            "Epoch 2 Batch 3750 Loss 1.2479 Accuracy 0.4444\n",
            "Epoch 2 Batch 3800 Loss 1.2462 Accuracy 0.4447\n",
            "Epoch 2 Batch 3850 Loss 1.2443 Accuracy 0.4451\n",
            "Epoch 2 Batch 3900 Loss 1.2429 Accuracy 0.4455\n",
            "Epoch 2 Batch 3950 Loss 1.2411 Accuracy 0.4458\n",
            "Epoch 2 Batch 4000 Loss 1.2396 Accuracy 0.4461\n",
            "Epoch 2 Batch 4050 Loss 1.2383 Accuracy 0.4463\n",
            "Epoch 2 Batch 4100 Loss 1.2368 Accuracy 0.4466\n",
            "Epoch 2 Batch 4150 Loss 1.2362 Accuracy 0.4467\n",
            "Epoch 2 Batch 4200 Loss 1.2360 Accuracy 0.4467\n",
            "Epoch 2 Batch 4250 Loss 1.2364 Accuracy 0.4468\n",
            "Epoch 2 Batch 4300 Loss 1.2371 Accuracy 0.4467\n",
            "Epoch 2 Batch 4350 Loss 1.2379 Accuracy 0.4467\n",
            "Epoch 2 Batch 4400 Loss 1.2390 Accuracy 0.4466\n",
            "Epoch 2 Batch 4450 Loss 1.2400 Accuracy 0.4464\n",
            "Epoch 2 Batch 4500 Loss 1.2413 Accuracy 0.4462\n",
            "Epoch 2 Batch 4550 Loss 1.2427 Accuracy 0.4461\n",
            "Epoch 2 Batch 4600 Loss 1.2441 Accuracy 0.4459\n",
            "Epoch 2 Batch 4650 Loss 1.2454 Accuracy 0.4457\n",
            "Epoch 2 Batch 4700 Loss 1.2464 Accuracy 0.4456\n",
            "Epoch 2 Batch 4750 Loss 1.2474 Accuracy 0.4454\n",
            "Epoch 2 Batch 4800 Loss 1.2485 Accuracy 0.4453\n",
            "Epoch 2 Batch 4850 Loss 1.2496 Accuracy 0.4451\n",
            "Epoch 2 Batch 4900 Loss 1.2507 Accuracy 0.445\n",
            "Epoch 2 Batch 4950 Loss 1.2518 Accuracy 0.4448\n",
            "Epoch 2 Batch 5000 Loss 1.2532 Accuracy 0.4447\n",
            "Epoch 2 Batch 5050 Loss 1.2543 Accuracy 0.4445\n",
            "Epoch 2 Batch 5100 Loss 1.2553 Accuracy 0.4443\n",
            "Epoch 2 Batch 5150 Loss 1.2565 Accuracy 0.444\n",
            "Epoch 2 Batch 5200 Loss 1.2576 Accuracy 0.4438\n",
            "Epoch 2 Batch 5250 Loss 1.2589 Accuracy 0.4436\n",
            "Epoch 2 Batch 5300 Loss 1.2598 Accuracy 0.4433\n",
            "Epoch 2 Batch 5350 Loss 1.2607 Accuracy 0.4431\n",
            "Epoch 2 Batch 5400 Loss 1.2618 Accuracy 0.4429\n",
            "Epoch 2 Batch 5450 Loss 1.2625 Accuracy 0.4427\n",
            "Epoch 2 Batch 5500 Loss 1.2634 Accuracy 0.4425\n",
            "Epoch 2 Batch 5550 Loss 1.2641 Accuracy 0.4423\n",
            "Epoch 2 Batch 5600 Loss 1.2647 Accuracy 0.4421\n",
            "Epoch 2 Batch 5650 Loss 1.2656 Accuracy 0.4419\n",
            "Epoch 2 Batch 5700 Loss 1.2662 Accuracy 0.4417\n",
            "Saving checkpoint for epoch 2 at ./drive/My Drive/transformer/ckpt/ckpt-5\n",
            "Time taken for 1 epoch: 1275.2579867839813 secs\n",
            "\n",
            "Start of epocch 3\n",
            "Epoch 3 Batch 0 Loss 1.4961 Accuracy 0.4317\n",
            "Epoch 3 Batch 50 Loss 1.3621 Accuracy 0.4216\n",
            "Epoch 3 Batch 100 Loss 1.3643 Accuracy 0.4264\n",
            "Epoch 3 Batch 150 Loss 1.3602 Accuracy 0.428\n",
            "Epoch 3 Batch 200 Loss 1.3599 Accuracy 0.4281\n",
            "Epoch 3 Batch 250 Loss 1.3582 Accuracy 0.4279\n",
            "Epoch 3 Batch 300 Loss 1.3538 Accuracy 0.4285\n",
            "Epoch 3 Batch 350 Loss 1.3529 Accuracy 0.4286\n",
            "Epoch 3 Batch 400 Loss 1.3479 Accuracy 0.429\n",
            "Epoch 3 Batch 450 Loss 1.3468 Accuracy 0.4294\n",
            "Epoch 3 Batch 500 Loss 1.3434 Accuracy 0.4289\n",
            "Epoch 3 Batch 550 Loss 1.3436 Accuracy 0.429\n",
            "Epoch 3 Batch 600 Loss 1.3431 Accuracy 0.4294\n",
            "Epoch 3 Batch 650 Loss 1.3400 Accuracy 0.4294\n",
            "Epoch 3 Batch 700 Loss 1.3407 Accuracy 0.4293\n",
            "Epoch 3 Batch 750 Loss 1.3367 Accuracy 0.4298\n",
            "Epoch 3 Batch 800 Loss 1.3363 Accuracy 0.4299\n",
            "Epoch 3 Batch 850 Loss 1.3351 Accuracy 0.4301\n",
            "Epoch 3 Batch 900 Loss 1.3336 Accuracy 0.4302\n",
            "Epoch 3 Batch 950 Loss 1.3323 Accuracy 0.43\n",
            "Epoch 3 Batch 1000 Loss 1.3309 Accuracy 0.4301\n",
            "Epoch 3 Batch 1050 Loss 1.3306 Accuracy 0.4302\n",
            "Epoch 3 Batch 1100 Loss 1.3290 Accuracy 0.4301\n",
            "Epoch 3 Batch 1150 Loss 1.3284 Accuracy 0.4302\n",
            "Epoch 3 Batch 1200 Loss 1.3288 Accuracy 0.4301\n",
            "Epoch 3 Batch 1250 Loss 1.3268 Accuracy 0.4304\n",
            "Epoch 3 Batch 1300 Loss 1.3245 Accuracy 0.4306\n",
            "Epoch 3 Batch 1350 Loss 1.3227 Accuracy 0.4308\n",
            "Epoch 3 Batch 1400 Loss 1.3215 Accuracy 0.4313\n",
            "Epoch 3 Batch 1450 Loss 1.3200 Accuracy 0.4318\n",
            "Epoch 3 Batch 1500 Loss 1.3176 Accuracy 0.4325\n",
            "Epoch 3 Batch 1550 Loss 1.3155 Accuracy 0.4331\n",
            "Epoch 3 Batch 1600 Loss 1.3128 Accuracy 0.4338\n",
            "Epoch 3 Batch 1650 Loss 1.3108 Accuracy 0.4345\n",
            "Epoch 3 Batch 1700 Loss 1.3094 Accuracy 0.435\n",
            "Epoch 3 Batch 1750 Loss 1.3076 Accuracy 0.4359\n",
            "Epoch 3 Batch 1800 Loss 1.3062 Accuracy 0.4366\n",
            "Epoch 3 Batch 1850 Loss 1.3043 Accuracy 0.4372\n",
            "Epoch 3 Batch 1900 Loss 1.3023 Accuracy 0.4379\n",
            "Epoch 3 Batch 1950 Loss 1.3006 Accuracy 0.4387\n",
            "Epoch 3 Batch 2000 Loss 1.2986 Accuracy 0.4391\n",
            "Epoch 3 Batch 2050 Loss 1.2972 Accuracy 0.4394\n",
            "Epoch 3 Batch 2100 Loss 1.2952 Accuracy 0.4398\n",
            "Epoch 3 Batch 2150 Loss 1.2923 Accuracy 0.4401\n",
            "Epoch 3 Batch 2200 Loss 1.2889 Accuracy 0.4403\n",
            "Epoch 3 Batch 2250 Loss 1.2853 Accuracy 0.4405\n",
            "Epoch 3 Batch 2300 Loss 1.2822 Accuracy 0.4406\n",
            "Epoch 3 Batch 2350 Loss 1.2792 Accuracy 0.4408\n",
            "Epoch 3 Batch 2400 Loss 1.2763 Accuracy 0.4411\n",
            "Epoch 3 Batch 2450 Loss 1.2723 Accuracy 0.4414\n",
            "Epoch 3 Batch 2500 Loss 1.2690 Accuracy 0.4418\n",
            "Epoch 3 Batch 2550 Loss 1.2661 Accuracy 0.4421\n",
            "Epoch 3 Batch 2600 Loss 1.2636 Accuracy 0.4424\n",
            "Epoch 3 Batch 2650 Loss 1.2604 Accuracy 0.4427\n",
            "Epoch 3 Batch 2700 Loss 1.2576 Accuracy 0.4431\n",
            "Epoch 3 Batch 2750 Loss 1.2552 Accuracy 0.4434\n",
            "Epoch 3 Batch 2800 Loss 1.2526 Accuracy 0.4439\n",
            "Epoch 3 Batch 2850 Loss 1.2501 Accuracy 0.4442\n",
            "Epoch 3 Batch 2900 Loss 1.2481 Accuracy 0.4445\n",
            "Epoch 3 Batch 2950 Loss 1.2461 Accuracy 0.4449\n",
            "Epoch 3 Batch 3000 Loss 1.2439 Accuracy 0.4453\n",
            "Epoch 3 Batch 3050 Loss 1.2421 Accuracy 0.4455\n",
            "Epoch 3 Batch 3100 Loss 1.2401 Accuracy 0.4458\n",
            "Epoch 3 Batch 3150 Loss 1.2382 Accuracy 0.446\n",
            "Epoch 3 Batch 3200 Loss 1.2361 Accuracy 0.4462\n",
            "Epoch 3 Batch 3250 Loss 1.2337 Accuracy 0.4465\n",
            "Epoch 3 Batch 3300 Loss 1.2312 Accuracy 0.4467\n",
            "Epoch 3 Batch 3350 Loss 1.2288 Accuracy 0.4471\n",
            "Epoch 3 Batch 3400 Loss 1.2270 Accuracy 0.4474\n",
            "Epoch 3 Batch 3450 Loss 1.2250 Accuracy 0.4478\n",
            "Epoch 3 Batch 3500 Loss 1.2227 Accuracy 0.4482\n",
            "Epoch 3 Batch 3550 Loss 1.2212 Accuracy 0.4485\n",
            "Epoch 3 Batch 3600 Loss 1.2192 Accuracy 0.4487\n",
            "Epoch 3 Batch 3650 Loss 1.2171 Accuracy 0.4491\n",
            "Epoch 3 Batch 3700 Loss 1.2153 Accuracy 0.4494\n",
            "Epoch 3 Batch 3750 Loss 1.2134 Accuracy 0.4498\n",
            "Epoch 3 Batch 3800 Loss 1.2123 Accuracy 0.4502\n",
            "Epoch 3 Batch 3850 Loss 1.2109 Accuracy 0.4504\n",
            "Epoch 3 Batch 3900 Loss 1.2095 Accuracy 0.4508\n",
            "Epoch 3 Batch 3950 Loss 1.2080 Accuracy 0.4511\n",
            "Epoch 3 Batch 4000 Loss 1.2065 Accuracy 0.4514\n",
            "Epoch 3 Batch 4050 Loss 1.2054 Accuracy 0.4517\n",
            "Epoch 3 Batch 4100 Loss 1.2042 Accuracy 0.452\n",
            "Epoch 3 Batch 4150 Loss 1.2036 Accuracy 0.4521\n",
            "Epoch 3 Batch 4200 Loss 1.2038 Accuracy 0.4521\n",
            "Epoch 3 Batch 4250 Loss 1.2038 Accuracy 0.4521\n",
            "Epoch 3 Batch 4300 Loss 1.2048 Accuracy 0.452\n",
            "Epoch 3 Batch 4350 Loss 1.2054 Accuracy 0.4519\n",
            "Epoch 3 Batch 4400 Loss 1.2066 Accuracy 0.4518\n",
            "Epoch 3 Batch 4450 Loss 1.2075 Accuracy 0.4516\n",
            "Epoch 3 Batch 4500 Loss 1.2087 Accuracy 0.4514\n",
            "Epoch 3 Batch 4550 Loss 1.2103 Accuracy 0.4513\n",
            "Epoch 3 Batch 4600 Loss 1.2115 Accuracy 0.4511\n",
            "Epoch 3 Batch 4650 Loss 1.2129 Accuracy 0.451\n",
            "Epoch 3 Batch 4700 Loss 1.2142 Accuracy 0.4508\n",
            "Epoch 3 Batch 4750 Loss 1.2156 Accuracy 0.4506\n",
            "Epoch 3 Batch 4800 Loss 1.2166 Accuracy 0.4505\n",
            "Epoch 3 Batch 4850 Loss 1.2175 Accuracy 0.4504\n",
            "Epoch 3 Batch 4900 Loss 1.2187 Accuracy 0.4502\n",
            "Epoch 3 Batch 4950 Loss 1.2199 Accuracy 0.45\n",
            "Epoch 3 Batch 5000 Loss 1.2212 Accuracy 0.4499\n",
            "Epoch 3 Batch 5050 Loss 1.2224 Accuracy 0.4497\n",
            "Epoch 3 Batch 5100 Loss 1.2237 Accuracy 0.4495\n",
            "Epoch 3 Batch 5150 Loss 1.2247 Accuracy 0.4493\n",
            "Epoch 3 Batch 5200 Loss 1.2259 Accuracy 0.4491\n",
            "Epoch 3 Batch 5250 Loss 1.2268 Accuracy 0.4489\n",
            "Epoch 3 Batch 5300 Loss 1.2279 Accuracy 0.4486\n",
            "Epoch 3 Batch 5350 Loss 1.2291 Accuracy 0.4483\n",
            "Epoch 3 Batch 5400 Loss 1.2299 Accuracy 0.4481\n",
            "Epoch 3 Batch 5450 Loss 1.2305 Accuracy 0.4479\n",
            "Epoch 3 Batch 5500 Loss 1.2313 Accuracy 0.4476\n",
            "Epoch 3 Batch 5550 Loss 1.2322 Accuracy 0.4474\n",
            "Epoch 3 Batch 5600 Loss 1.2329 Accuracy 0.4472\n",
            "Epoch 3 Batch 5650 Loss 1.2337 Accuracy 0.447\n",
            "Epoch 3 Batch 5700 Loss 1.2346 Accuracy 0.4467\n",
            "Saving checkpoint for epoch 3 at ./drive/My Drive/transformer/ckpt/ckpt-6\n",
            "Time taken for 1 epoch: 1285.0590379238129 secs\n",
            "\n",
            "Start of epocch 4\n",
            "Epoch 4 Batch 0 Loss 1.1809 Accuracy 0.4038\n",
            "Epoch 4 Batch 50 Loss 1.3544 Accuracy 0.4277\n",
            "Epoch 4 Batch 100 Loss 1.3468 Accuracy 0.4297\n",
            "Epoch 4 Batch 150 Loss 1.3393 Accuracy 0.4323\n",
            "Epoch 4 Batch 200 Loss 1.3310 Accuracy 0.4325\n",
            "Epoch 4 Batch 250 Loss 1.3257 Accuracy 0.4323\n",
            "Epoch 4 Batch 300 Loss 1.3244 Accuracy 0.4319\n",
            "Epoch 4 Batch 350 Loss 1.3229 Accuracy 0.4318\n",
            "Epoch 4 Batch 400 Loss 1.3186 Accuracy 0.4317\n",
            "Epoch 4 Batch 450 Loss 1.3131 Accuracy 0.4321\n",
            "Epoch 4 Batch 500 Loss 1.3101 Accuracy 0.432\n",
            "Epoch 4 Batch 550 Loss 1.3108 Accuracy 0.4319\n",
            "Epoch 4 Batch 600 Loss 1.3096 Accuracy 0.432\n",
            "Epoch 4 Batch 650 Loss 1.3084 Accuracy 0.4323\n",
            "Epoch 4 Batch 700 Loss 1.3090 Accuracy 0.4328\n",
            "Epoch 4 Batch 750 Loss 1.3085 Accuracy 0.433\n",
            "Epoch 4 Batch 800 Loss 1.3086 Accuracy 0.4334\n",
            "Epoch 4 Batch 850 Loss 1.3085 Accuracy 0.4335\n",
            "Epoch 4 Batch 900 Loss 1.3081 Accuracy 0.4333\n",
            "Epoch 4 Batch 950 Loss 1.3079 Accuracy 0.4335\n",
            "Epoch 4 Batch 1000 Loss 1.3065 Accuracy 0.4337\n",
            "Epoch 4 Batch 1050 Loss 1.3065 Accuracy 0.4338\n",
            "Epoch 4 Batch 1100 Loss 1.3067 Accuracy 0.4338\n",
            "Epoch 4 Batch 1150 Loss 1.3061 Accuracy 0.4337\n",
            "Epoch 4 Batch 1200 Loss 1.3049 Accuracy 0.4339\n",
            "Epoch 4 Batch 1250 Loss 1.3031 Accuracy 0.434\n",
            "Epoch 4 Batch 1300 Loss 1.3010 Accuracy 0.4345\n",
            "Epoch 4 Batch 1350 Loss 1.2989 Accuracy 0.4349\n",
            "Epoch 4 Batch 1400 Loss 1.2973 Accuracy 0.4356\n",
            "Epoch 4 Batch 1450 Loss 1.2951 Accuracy 0.4362\n",
            "Epoch 4 Batch 1500 Loss 1.2922 Accuracy 0.4367\n",
            "Epoch 4 Batch 1550 Loss 1.2904 Accuracy 0.4374\n",
            "Epoch 4 Batch 1600 Loss 1.2890 Accuracy 0.438\n",
            "Epoch 4 Batch 1650 Loss 1.2865 Accuracy 0.4388\n",
            "Epoch 4 Batch 1700 Loss 1.2841 Accuracy 0.4395\n",
            "Epoch 4 Batch 1750 Loss 1.2821 Accuracy 0.4403\n",
            "Epoch 4 Batch 1800 Loss 1.2802 Accuracy 0.4408\n",
            "Epoch 4 Batch 1850 Loss 1.2783 Accuracy 0.4416\n",
            "Epoch 4 Batch 1900 Loss 1.2768 Accuracy 0.4422\n",
            "Epoch 4 Batch 1950 Loss 1.2744 Accuracy 0.4429\n",
            "Epoch 4 Batch 2000 Loss 1.2730 Accuracy 0.4434\n",
            "Epoch 4 Batch 2050 Loss 1.2706 Accuracy 0.4439\n",
            "Epoch 4 Batch 2100 Loss 1.2680 Accuracy 0.4444\n",
            "Epoch 4 Batch 2150 Loss 1.2654 Accuracy 0.4446\n",
            "Epoch 4 Batch 2200 Loss 1.2619 Accuracy 0.4449\n",
            "Epoch 4 Batch 2250 Loss 1.2587 Accuracy 0.4449\n",
            "Epoch 4 Batch 2300 Loss 1.2561 Accuracy 0.4451\n",
            "Epoch 4 Batch 2350 Loss 1.2527 Accuracy 0.4453\n",
            "Epoch 4 Batch 2400 Loss 1.2499 Accuracy 0.4455\n",
            "Epoch 4 Batch 2450 Loss 1.2475 Accuracy 0.4458\n",
            "Epoch 4 Batch 2500 Loss 1.2444 Accuracy 0.4461\n",
            "Epoch 4 Batch 2550 Loss 1.2416 Accuracy 0.4465\n",
            "Epoch 4 Batch 2600 Loss 1.2389 Accuracy 0.4467\n",
            "Epoch 4 Batch 2650 Loss 1.2361 Accuracy 0.4471\n",
            "Epoch 4 Batch 2700 Loss 1.2331 Accuracy 0.4475\n",
            "Epoch 4 Batch 2750 Loss 1.2304 Accuracy 0.4478\n",
            "Epoch 4 Batch 2800 Loss 1.2277 Accuracy 0.4482\n",
            "Epoch 4 Batch 2850 Loss 1.2255 Accuracy 0.4485\n",
            "Epoch 4 Batch 2900 Loss 1.2229 Accuracy 0.4489\n",
            "Epoch 4 Batch 2950 Loss 1.2204 Accuracy 0.4492\n",
            "Epoch 4 Batch 3000 Loss 1.2184 Accuracy 0.4496\n",
            "Epoch 4 Batch 3050 Loss 1.2166 Accuracy 0.4499\n",
            "Epoch 4 Batch 3100 Loss 1.2148 Accuracy 0.4502\n",
            "Epoch 4 Batch 3150 Loss 1.2127 Accuracy 0.4504\n",
            "Epoch 4 Batch 3200 Loss 1.2103 Accuracy 0.4506\n",
            "Epoch 4 Batch 3250 Loss 1.2083 Accuracy 0.4508\n",
            "Epoch 4 Batch 3300 Loss 1.2066 Accuracy 0.4511\n",
            "Epoch 4 Batch 3350 Loss 1.2042 Accuracy 0.4513\n",
            "Epoch 4 Batch 3400 Loss 1.2021 Accuracy 0.4516\n",
            "Epoch 4 Batch 3450 Loss 1.2003 Accuracy 0.452\n",
            "Epoch 4 Batch 3500 Loss 1.1985 Accuracy 0.4522\n",
            "Epoch 4 Batch 3550 Loss 1.1968 Accuracy 0.4526\n",
            "Epoch 4 Batch 3600 Loss 1.1949 Accuracy 0.4528\n",
            "Epoch 4 Batch 3650 Loss 1.1929 Accuracy 0.4531\n",
            "Epoch 4 Batch 3700 Loss 1.1911 Accuracy 0.4535\n",
            "Epoch 4 Batch 3750 Loss 1.1895 Accuracy 0.4538\n",
            "Epoch 4 Batch 3800 Loss 1.1880 Accuracy 0.4541\n",
            "Epoch 4 Batch 3850 Loss 1.1866 Accuracy 0.4544\n",
            "Epoch 4 Batch 3900 Loss 1.1850 Accuracy 0.4547\n",
            "Epoch 4 Batch 3950 Loss 1.1837 Accuracy 0.455\n",
            "Epoch 4 Batch 4000 Loss 1.1823 Accuracy 0.4554\n",
            "Epoch 4 Batch 4050 Loss 1.1807 Accuracy 0.4557\n",
            "Epoch 4 Batch 4100 Loss 1.1794 Accuracy 0.456\n",
            "Epoch 4 Batch 4150 Loss 1.1790 Accuracy 0.4561\n",
            "Epoch 4 Batch 4200 Loss 1.1795 Accuracy 0.4562\n",
            "Epoch 4 Batch 4250 Loss 1.1798 Accuracy 0.4562\n",
            "Epoch 4 Batch 4300 Loss 1.1804 Accuracy 0.456\n",
            "Epoch 4 Batch 4350 Loss 1.1813 Accuracy 0.4559\n",
            "Epoch 4 Batch 4400 Loss 1.1820 Accuracy 0.4558\n",
            "Epoch 4 Batch 4450 Loss 1.1831 Accuracy 0.4556\n",
            "Epoch 4 Batch 4500 Loss 1.1846 Accuracy 0.4555\n",
            "Epoch 4 Batch 4550 Loss 1.1861 Accuracy 0.4553\n",
            "Epoch 4 Batch 4600 Loss 1.1875 Accuracy 0.4551\n",
            "Epoch 4 Batch 4650 Loss 1.1887 Accuracy 0.455\n",
            "Epoch 4 Batch 4700 Loss 1.1900 Accuracy 0.4548\n",
            "Epoch 4 Batch 4750 Loss 1.1912 Accuracy 0.4546\n",
            "Epoch 4 Batch 4800 Loss 1.1921 Accuracy 0.4544\n",
            "Epoch 4 Batch 4850 Loss 1.1935 Accuracy 0.4543\n",
            "Epoch 4 Batch 4900 Loss 1.1945 Accuracy 0.4542\n",
            "Epoch 4 Batch 4950 Loss 1.1957 Accuracy 0.454\n",
            "Epoch 4 Batch 5000 Loss 1.1970 Accuracy 0.4539\n",
            "Epoch 4 Batch 5050 Loss 1.1982 Accuracy 0.4537\n",
            "Epoch 4 Batch 5100 Loss 1.1994 Accuracy 0.4535\n",
            "Epoch 4 Batch 5150 Loss 1.2007 Accuracy 0.4532\n",
            "Epoch 4 Batch 5200 Loss 1.2020 Accuracy 0.453\n",
            "Epoch 4 Batch 5250 Loss 1.2034 Accuracy 0.4527\n",
            "Epoch 4 Batch 5300 Loss 1.2044 Accuracy 0.4525\n",
            "Epoch 4 Batch 5350 Loss 1.2053 Accuracy 0.4522\n",
            "Epoch 4 Batch 5400 Loss 1.2061 Accuracy 0.452\n",
            "Epoch 4 Batch 5450 Loss 1.2070 Accuracy 0.4517\n",
            "Epoch 4 Batch 5500 Loss 1.2079 Accuracy 0.4515\n",
            "Epoch 4 Batch 5550 Loss 1.2088 Accuracy 0.4512\n",
            "Epoch 4 Batch 5600 Loss 1.2095 Accuracy 0.451\n",
            "Epoch 4 Batch 5650 Loss 1.2104 Accuracy 0.4508\n",
            "Epoch 4 Batch 5700 Loss 1.2113 Accuracy 0.4506\n",
            "Saving checkpoint for epoch 4 at ./drive/My Drive/transformer/ckpt/ckpt-7\n",
            "Time taken for 1 epoch: 1288.3182117938995 secs\n",
            "\n",
            "Start of epocch 5\n",
            "Epoch 5 Batch 0 Loss 1.3134 Accuracy 0.4293\n",
            "Epoch 5 Batch 50 Loss 1.3112 Accuracy 0.4318\n",
            "Epoch 5 Batch 100 Loss 1.3102 Accuracy 0.4347\n",
            "Epoch 5 Batch 150 Loss 1.3045 Accuracy 0.4351\n",
            "Epoch 5 Batch 200 Loss 1.2982 Accuracy 0.4363\n",
            "Epoch 5 Batch 250 Loss 1.2940 Accuracy 0.4361\n",
            "Epoch 5 Batch 300 Loss 1.2911 Accuracy 0.4362\n",
            "Epoch 5 Batch 350 Loss 1.2918 Accuracy 0.4354\n",
            "Epoch 5 Batch 400 Loss 1.2910 Accuracy 0.4358\n",
            "Epoch 5 Batch 450 Loss 1.2900 Accuracy 0.436\n",
            "Epoch 5 Batch 500 Loss 1.2879 Accuracy 0.436\n",
            "Epoch 5 Batch 550 Loss 1.2896 Accuracy 0.4356\n",
            "Epoch 5 Batch 600 Loss 1.2896 Accuracy 0.4359\n",
            "Epoch 5 Batch 650 Loss 1.2905 Accuracy 0.4362\n",
            "Epoch 5 Batch 700 Loss 1.2899 Accuracy 0.4365\n",
            "Epoch 5 Batch 750 Loss 1.2892 Accuracy 0.4365\n",
            "Epoch 5 Batch 800 Loss 1.2873 Accuracy 0.4371\n",
            "Epoch 5 Batch 850 Loss 1.2887 Accuracy 0.437\n",
            "Epoch 5 Batch 900 Loss 1.2887 Accuracy 0.4369\n",
            "Epoch 5 Batch 950 Loss 1.2871 Accuracy 0.4371\n",
            "Epoch 5 Batch 1000 Loss 1.2861 Accuracy 0.4372\n",
            "Epoch 5 Batch 1050 Loss 1.2848 Accuracy 0.4371\n",
            "Epoch 5 Batch 1100 Loss 1.2841 Accuracy 0.4373\n",
            "Epoch 5 Batch 1150 Loss 1.2835 Accuracy 0.4375\n",
            "Epoch 5 Batch 1200 Loss 1.2832 Accuracy 0.4375\n",
            "Epoch 5 Batch 1250 Loss 1.2809 Accuracy 0.4376\n",
            "Epoch 5 Batch 1300 Loss 1.2789 Accuracy 0.4378\n",
            "Epoch 5 Batch 1350 Loss 1.2768 Accuracy 0.4384\n",
            "Epoch 5 Batch 1400 Loss 1.2750 Accuracy 0.439\n",
            "Epoch 5 Batch 1450 Loss 1.2731 Accuracy 0.4397\n",
            "Epoch 5 Batch 1500 Loss 1.2714 Accuracy 0.4404\n",
            "Epoch 5 Batch 1550 Loss 1.2693 Accuracy 0.441\n",
            "Epoch 5 Batch 1600 Loss 1.2669 Accuracy 0.4416\n",
            "Epoch 5 Batch 1650 Loss 1.2650 Accuracy 0.4424\n",
            "Epoch 5 Batch 1700 Loss 1.2642 Accuracy 0.443\n",
            "Epoch 5 Batch 1750 Loss 1.2630 Accuracy 0.4436\n",
            "Epoch 5 Batch 1800 Loss 1.2609 Accuracy 0.4444\n",
            "Epoch 5 Batch 1850 Loss 1.2587 Accuracy 0.4451\n",
            "Epoch 5 Batch 1900 Loss 1.2569 Accuracy 0.4458\n",
            "Epoch 5 Batch 1950 Loss 1.2546 Accuracy 0.4465\n",
            "Epoch 5 Batch 2000 Loss 1.2524 Accuracy 0.447\n",
            "Epoch 5 Batch 2050 Loss 1.2498 Accuracy 0.4475\n",
            "Epoch 5 Batch 2100 Loss 1.2469 Accuracy 0.4478\n",
            "Epoch 5 Batch 2150 Loss 1.2442 Accuracy 0.448\n",
            "Epoch 5 Batch 2200 Loss 1.2406 Accuracy 0.4483\n",
            "Epoch 5 Batch 2250 Loss 1.2374 Accuracy 0.4483\n",
            "Epoch 5 Batch 2300 Loss 1.2339 Accuracy 0.4485\n",
            "Epoch 5 Batch 2350 Loss 1.2310 Accuracy 0.4488\n",
            "Epoch 5 Batch 2400 Loss 1.2279 Accuracy 0.449\n",
            "Epoch 5 Batch 2450 Loss 1.2256 Accuracy 0.4493\n",
            "Epoch 5 Batch 2500 Loss 1.2223 Accuracy 0.4496\n",
            "Epoch 5 Batch 2550 Loss 1.2192 Accuracy 0.4499\n",
            "Epoch 5 Batch 2600 Loss 1.2168 Accuracy 0.4502\n",
            "Epoch 5 Batch 2650 Loss 1.2144 Accuracy 0.4505\n",
            "Epoch 5 Batch 2700 Loss 1.2115 Accuracy 0.4509\n",
            "Epoch 5 Batch 2750 Loss 1.2092 Accuracy 0.4513\n",
            "Epoch 5 Batch 2800 Loss 1.2067 Accuracy 0.4516\n",
            "Epoch 5 Batch 2850 Loss 1.2048 Accuracy 0.4519\n",
            "Epoch 5 Batch 2900 Loss 1.2027 Accuracy 0.4523\n",
            "Epoch 5 Batch 2950 Loss 1.2005 Accuracy 0.4526\n",
            "Epoch 5 Batch 3000 Loss 1.1987 Accuracy 0.4527\n",
            "Epoch 5 Batch 3050 Loss 1.1963 Accuracy 0.453\n",
            "Epoch 5 Batch 3100 Loss 1.1943 Accuracy 0.4533\n",
            "Epoch 5 Batch 3150 Loss 1.1929 Accuracy 0.4536\n",
            "Epoch 5 Batch 3200 Loss 1.1908 Accuracy 0.4538\n",
            "Epoch 5 Batch 3250 Loss 1.1885 Accuracy 0.4541\n",
            "Epoch 5 Batch 3300 Loss 1.1863 Accuracy 0.4543\n",
            "Epoch 5 Batch 3350 Loss 1.1841 Accuracy 0.4546\n",
            "Epoch 5 Batch 3400 Loss 1.1819 Accuracy 0.4549\n",
            "Epoch 5 Batch 3450 Loss 1.1799 Accuracy 0.4551\n",
            "Epoch 5 Batch 3500 Loss 1.1784 Accuracy 0.4555\n",
            "Epoch 5 Batch 3550 Loss 1.1764 Accuracy 0.4558\n",
            "Epoch 5 Batch 3600 Loss 1.1745 Accuracy 0.4561\n",
            "Epoch 5 Batch 3650 Loss 1.1725 Accuracy 0.4565\n",
            "Epoch 5 Batch 3700 Loss 1.1709 Accuracy 0.4568\n",
            "Epoch 5 Batch 3750 Loss 1.1694 Accuracy 0.4572\n",
            "Epoch 5 Batch 3800 Loss 1.1678 Accuracy 0.4575\n",
            "Epoch 5 Batch 3850 Loss 1.1665 Accuracy 0.4578\n",
            "Epoch 5 Batch 3900 Loss 1.1649 Accuracy 0.4581\n",
            "Epoch 5 Batch 3950 Loss 1.1635 Accuracy 0.4584\n",
            "Epoch 5 Batch 4000 Loss 1.1623 Accuracy 0.4588\n",
            "Epoch 5 Batch 4050 Loss 1.1611 Accuracy 0.459\n",
            "Epoch 5 Batch 4100 Loss 1.1600 Accuracy 0.4592\n",
            "Epoch 5 Batch 4150 Loss 1.1596 Accuracy 0.4593\n",
            "Epoch 5 Batch 4200 Loss 1.1595 Accuracy 0.4594\n",
            "Epoch 5 Batch 4250 Loss 1.1599 Accuracy 0.4594\n",
            "Epoch 5 Batch 4300 Loss 1.1607 Accuracy 0.4593\n",
            "Epoch 5 Batch 4350 Loss 1.1616 Accuracy 0.4592\n",
            "Epoch 5 Batch 4400 Loss 1.1624 Accuracy 0.4591\n",
            "Epoch 5 Batch 4450 Loss 1.1634 Accuracy 0.459\n",
            "Epoch 5 Batch 4500 Loss 1.1646 Accuracy 0.4588\n",
            "Epoch 5 Batch 4550 Loss 1.1659 Accuracy 0.4586\n",
            "Epoch 5 Batch 4600 Loss 1.1673 Accuracy 0.4584\n",
            "Epoch 5 Batch 4650 Loss 1.1687 Accuracy 0.4582\n",
            "Epoch 5 Batch 4700 Loss 1.1702 Accuracy 0.458\n",
            "Epoch 5 Batch 4750 Loss 1.1713 Accuracy 0.4579\n",
            "Epoch 5 Batch 4800 Loss 1.1728 Accuracy 0.4577\n",
            "Epoch 5 Batch 4850 Loss 1.1739 Accuracy 0.4576\n",
            "Epoch 5 Batch 4900 Loss 1.1747 Accuracy 0.4574\n",
            "Epoch 5 Batch 4950 Loss 1.1760 Accuracy 0.4572\n",
            "Epoch 5 Batch 5000 Loss 1.1771 Accuracy 0.457\n",
            "Epoch 5 Batch 5050 Loss 1.1785 Accuracy 0.4568\n",
            "Epoch 5 Batch 5100 Loss 1.1799 Accuracy 0.4566\n",
            "Epoch 5 Batch 5150 Loss 1.1810 Accuracy 0.4564\n",
            "Epoch 5 Batch 5200 Loss 1.1824 Accuracy 0.4561\n",
            "Epoch 5 Batch 5250 Loss 1.1833 Accuracy 0.4559\n",
            "Epoch 5 Batch 5300 Loss 1.1842 Accuracy 0.4556\n",
            "Epoch 5 Batch 5350 Loss 1.1854 Accuracy 0.4554\n",
            "Epoch 5 Batch 5400 Loss 1.1865 Accuracy 0.4551\n",
            "Epoch 5 Batch 5450 Loss 1.1875 Accuracy 0.4548\n",
            "Epoch 5 Batch 5500 Loss 1.1885 Accuracy 0.4545\n",
            "Epoch 5 Batch 5550 Loss 1.1895 Accuracy 0.4543\n",
            "Epoch 5 Batch 5600 Loss 1.1904 Accuracy 0.454\n",
            "Epoch 5 Batch 5650 Loss 1.1913 Accuracy 0.4538\n",
            "Epoch 5 Batch 5700 Loss 1.1922 Accuracy 0.4536\n",
            "Saving checkpoint for epoch 5 at ./drive/My Drive/transformer/ckpt/ckpt-8\n",
            "Time taken for 1 epoch: 1299.4042274951935 secs\n",
            "\n",
            "Start of epocch 6\n",
            "Epoch 6 Batch 0 Loss 1.4442 Accuracy 0.4194\n",
            "Epoch 6 Batch 50 Loss 1.3134 Accuracy 0.4378\n",
            "Epoch 6 Batch 100 Loss 1.3019 Accuracy 0.4374\n",
            "Epoch 6 Batch 150 Loss 1.2920 Accuracy 0.437\n",
            "Epoch 6 Batch 200 Loss 1.2870 Accuracy 0.4384\n",
            "Epoch 6 Batch 250 Loss 1.2826 Accuracy 0.4386\n",
            "Epoch 6 Batch 300 Loss 1.2814 Accuracy 0.4385\n",
            "Epoch 6 Batch 350 Loss 1.2832 Accuracy 0.438\n",
            "Epoch 6 Batch 400 Loss 1.2785 Accuracy 0.4382\n",
            "Epoch 6 Batch 450 Loss 1.2779 Accuracy 0.4389\n",
            "Epoch 6 Batch 500 Loss 1.2767 Accuracy 0.4388\n",
            "Epoch 6 Batch 550 Loss 1.2766 Accuracy 0.4391\n",
            "Epoch 6 Batch 600 Loss 1.2752 Accuracy 0.4394\n",
            "Epoch 6 Batch 650 Loss 1.2745 Accuracy 0.4394\n",
            "Epoch 6 Batch 700 Loss 1.2760 Accuracy 0.4395\n",
            "Epoch 6 Batch 750 Loss 1.2751 Accuracy 0.44\n",
            "Epoch 6 Batch 800 Loss 1.2746 Accuracy 0.44\n",
            "Epoch 6 Batch 850 Loss 1.2742 Accuracy 0.4398\n",
            "Epoch 6 Batch 900 Loss 1.2728 Accuracy 0.4396\n",
            "Epoch 6 Batch 950 Loss 1.2717 Accuracy 0.4396\n",
            "Epoch 6 Batch 1000 Loss 1.2685 Accuracy 0.4395\n",
            "Epoch 6 Batch 1050 Loss 1.2680 Accuracy 0.4397\n",
            "Epoch 6 Batch 1100 Loss 1.2669 Accuracy 0.4396\n",
            "Epoch 6 Batch 1150 Loss 1.2677 Accuracy 0.4398\n",
            "Epoch 6 Batch 1200 Loss 1.2656 Accuracy 0.4398\n",
            "Epoch 6 Batch 1250 Loss 1.2645 Accuracy 0.4399\n",
            "Epoch 6 Batch 1300 Loss 1.2625 Accuracy 0.4403\n",
            "Epoch 6 Batch 1350 Loss 1.2617 Accuracy 0.4406\n",
            "Epoch 6 Batch 1400 Loss 1.2589 Accuracy 0.4413\n",
            "Epoch 6 Batch 1450 Loss 1.2570 Accuracy 0.4418\n",
            "Epoch 6 Batch 1500 Loss 1.2551 Accuracy 0.4425\n",
            "Epoch 6 Batch 1550 Loss 1.2525 Accuracy 0.4433\n",
            "Epoch 6 Batch 1600 Loss 1.2510 Accuracy 0.4441\n",
            "Epoch 6 Batch 1650 Loss 1.2483 Accuracy 0.4448\n",
            "Epoch 6 Batch 1700 Loss 1.2472 Accuracy 0.4455\n",
            "Epoch 6 Batch 1750 Loss 1.2447 Accuracy 0.4463\n",
            "Epoch 6 Batch 1800 Loss 1.2428 Accuracy 0.4469\n",
            "Epoch 6 Batch 1850 Loss 1.2406 Accuracy 0.4476\n",
            "Epoch 6 Batch 1900 Loss 1.2388 Accuracy 0.4482\n",
            "Epoch 6 Batch 1950 Loss 1.2373 Accuracy 0.4488\n",
            "Epoch 6 Batch 2000 Loss 1.2358 Accuracy 0.4496\n",
            "Epoch 6 Batch 2050 Loss 1.2333 Accuracy 0.4501\n",
            "Epoch 6 Batch 2100 Loss 1.2311 Accuracy 0.4504\n",
            "Epoch 6 Batch 2150 Loss 1.2279 Accuracy 0.4508\n",
            "Epoch 6 Batch 2200 Loss 1.2248 Accuracy 0.451\n",
            "Epoch 6 Batch 2250 Loss 1.2218 Accuracy 0.4511\n",
            "Epoch 6 Batch 2300 Loss 1.2185 Accuracy 0.4514\n",
            "Epoch 6 Batch 2350 Loss 1.2155 Accuracy 0.4516\n",
            "Epoch 6 Batch 2400 Loss 1.2125 Accuracy 0.4518\n",
            "Epoch 6 Batch 2450 Loss 1.2094 Accuracy 0.4521\n",
            "Epoch 6 Batch 2500 Loss 1.2062 Accuracy 0.4523\n",
            "Epoch 6 Batch 2550 Loss 1.2034 Accuracy 0.4527\n",
            "Epoch 6 Batch 2600 Loss 1.2003 Accuracy 0.4531\n",
            "Epoch 6 Batch 2650 Loss 1.1973 Accuracy 0.4534\n",
            "Epoch 6 Batch 2700 Loss 1.1947 Accuracy 0.4537\n",
            "Epoch 6 Batch 2750 Loss 1.1922 Accuracy 0.454\n",
            "Epoch 6 Batch 2800 Loss 1.1902 Accuracy 0.4543\n",
            "Epoch 6 Batch 2850 Loss 1.1880 Accuracy 0.4546\n",
            "Epoch 6 Batch 2900 Loss 1.1858 Accuracy 0.4549\n",
            "Epoch 6 Batch 2950 Loss 1.1835 Accuracy 0.4552\n",
            "Epoch 6 Batch 3000 Loss 1.1817 Accuracy 0.4554\n",
            "Epoch 6 Batch 3050 Loss 1.1795 Accuracy 0.4556\n",
            "Epoch 6 Batch 3100 Loss 1.1771 Accuracy 0.456\n",
            "Epoch 6 Batch 3150 Loss 1.1755 Accuracy 0.4563\n",
            "Epoch 6 Batch 3200 Loss 1.1733 Accuracy 0.4565\n",
            "Epoch 6 Batch 3250 Loss 1.1716 Accuracy 0.4567\n",
            "Epoch 6 Batch 3300 Loss 1.1690 Accuracy 0.457\n",
            "Epoch 6 Batch 3350 Loss 1.1671 Accuracy 0.4573\n",
            "Epoch 6 Batch 3400 Loss 1.1648 Accuracy 0.4577\n",
            "Epoch 6 Batch 3450 Loss 1.1629 Accuracy 0.458\n",
            "Epoch 6 Batch 3500 Loss 1.1611 Accuracy 0.4584\n",
            "Epoch 6 Batch 3550 Loss 1.1592 Accuracy 0.4587\n",
            "Epoch 6 Batch 3600 Loss 1.1572 Accuracy 0.4591\n",
            "Epoch 6 Batch 3650 Loss 1.1554 Accuracy 0.4594\n",
            "Epoch 6 Batch 3700 Loss 1.1537 Accuracy 0.4598\n",
            "Epoch 6 Batch 3750 Loss 1.1518 Accuracy 0.4601\n",
            "Epoch 6 Batch 3800 Loss 1.1501 Accuracy 0.4605\n",
            "Epoch 6 Batch 3850 Loss 1.1487 Accuracy 0.4608\n",
            "Epoch 6 Batch 3900 Loss 1.1474 Accuracy 0.461\n",
            "Epoch 6 Batch 3950 Loss 1.1460 Accuracy 0.4613\n",
            "Epoch 6 Batch 4000 Loss 1.1447 Accuracy 0.4616\n",
            "Epoch 6 Batch 4050 Loss 1.1437 Accuracy 0.4619\n",
            "Epoch 6 Batch 4100 Loss 1.1430 Accuracy 0.4621\n",
            "Epoch 6 Batch 4150 Loss 1.1423 Accuracy 0.4622\n",
            "Epoch 6 Batch 4200 Loss 1.1425 Accuracy 0.4622\n",
            "Epoch 6 Batch 4250 Loss 1.1429 Accuracy 0.4622\n",
            "Epoch 6 Batch 4300 Loss 1.1438 Accuracy 0.4621\n",
            "Epoch 6 Batch 4350 Loss 1.1447 Accuracy 0.462\n",
            "Epoch 6 Batch 4400 Loss 1.1460 Accuracy 0.4619\n",
            "Epoch 6 Batch 4450 Loss 1.1472 Accuracy 0.4617\n",
            "Epoch 6 Batch 4500 Loss 1.1485 Accuracy 0.4615\n",
            "Epoch 6 Batch 4550 Loss 1.1497 Accuracy 0.4614\n",
            "Epoch 6 Batch 4600 Loss 1.1508 Accuracy 0.4612\n",
            "Epoch 6 Batch 4650 Loss 1.1523 Accuracy 0.461\n",
            "Epoch 6 Batch 4700 Loss 1.1536 Accuracy 0.4609\n",
            "Epoch 6 Batch 4750 Loss 1.1550 Accuracy 0.4606\n",
            "Epoch 6 Batch 4800 Loss 1.1565 Accuracy 0.4605\n",
            "Epoch 6 Batch 4850 Loss 1.1577 Accuracy 0.4603\n",
            "Epoch 6 Batch 4900 Loss 1.1587 Accuracy 0.46\n",
            "Epoch 6 Batch 4950 Loss 1.1600 Accuracy 0.4599\n",
            "Epoch 6 Batch 5000 Loss 1.1616 Accuracy 0.4596\n",
            "Epoch 6 Batch 5050 Loss 1.1627 Accuracy 0.4595\n",
            "Epoch 6 Batch 5100 Loss 1.1641 Accuracy 0.4592\n",
            "Epoch 6 Batch 5150 Loss 1.1655 Accuracy 0.459\n",
            "Epoch 6 Batch 5200 Loss 1.1666 Accuracy 0.4587\n",
            "Epoch 6 Batch 5250 Loss 1.1676 Accuracy 0.4585\n",
            "Epoch 6 Batch 5300 Loss 1.1688 Accuracy 0.4582\n",
            "Epoch 6 Batch 5350 Loss 1.1697 Accuracy 0.4579\n",
            "Epoch 6 Batch 5400 Loss 1.1707 Accuracy 0.4577\n",
            "Epoch 6 Batch 5450 Loss 1.1715 Accuracy 0.4574\n",
            "Epoch 6 Batch 5500 Loss 1.1723 Accuracy 0.4572\n",
            "Epoch 6 Batch 5550 Loss 1.1733 Accuracy 0.4569\n",
            "Epoch 6 Batch 5600 Loss 1.1742 Accuracy 0.4567\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Phoub0kF5Yhf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(input_sentence):\n",
        "  input_sentence = \\\n",
        "        [VOCAB_SIZE_EN - 2] + tokenizer_en.encode(inp_sentence) + [VOCAB_SIZE_EN - 1]\n",
        "\n",
        "  enc_input = tf.expand_dims(input_sentence, axis = 0) #Batch dims\n",
        "\n",
        "  output = tf.expand_dims([VOCAB_SIZE_FR - 2], axis = 0)  #we have only start tag now in predictions, enable batchDims\n",
        "\n",
        "  for _ in range(MAXLEN):\n",
        "    predictions = transformer(enc_input, output, False)  #Note here that the output is dec op which is also dec ip, but the dec ip has a start tag and text is shifted right, but dec op wll be shifted left rel to the dec ip and will have the extra final predicted word\n",
        "\n",
        "    prediction = predictions[:, -1:, :]   #size of predictions is (1, seq_length, vocab_size_fr), taking the last element from seq which is at each stage the predicted word, each word is a array of preds of size vocab_size_fr\n",
        "\n",
        "    predicted_id = tf.cast(tf.argmax(prediction, axis = -1), tf.int32)  #argmax along vocab_size arg to pull out highest of softmax preds\n",
        "\n",
        "    if predicted_id == VOCAB_SIZE_FR - 1:\n",
        "      return tf.squeeze(output, axis = 0)\n",
        "\n",
        "    output = tf.concat([output, predicted_id], axis = -1)\n",
        "\n",
        "  return tf.squeeze(output, axis = 0)   \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9CtXYbP5raG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(sentence):\n",
        "  output = evaluate(sentence).numpy()  #Numpy array is easier to handle than tensor\n",
        "\n",
        "  predicted_ans = tokenizer_fr.decode(\n",
        "      [i for i in output if i < VOCAB_SIZE_FR - 2]    #Do npt decode the start and end tag\n",
        "  )\n",
        "\n",
        "  print(\"Input : {}\".format(sentence))\n",
        "  print(\"Output: {}\".format(predicted_ans))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fi5j4gzg5rFV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "translate(\"This is a really powerful tool\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}