{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Transformer_for_NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9JJ7FBw84tG",
        "colab_type": "text"
      },
      "source": [
        "# Stage 1: Importing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbcvtPlp3YWu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import re\n",
        "import time\n",
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6o_cpZz3y_-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except:\n",
        "    pass\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3n5aFVXGW33b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "666fd586-bf08-49a2-8f3a-4a22fc3ec42f"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQN8jwx48_yU",
        "colab_type": "text"
      },
      "source": [
        "# Stage 2: Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPlOT-2mlw0r",
        "colab_type": "text"
      },
      "source": [
        "## Loading files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCD9jwXsLwS_",
        "colab_type": "text"
      },
      "source": [
        "We import files from our personal google drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQpbl1pXCR0p",
        "colab_type": "code",
        "outputId": "89fab715-eb76-4ada-ba0a-968f2ce59736",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8Or0sLV5b8t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open (\"/content/drive/My Drive/transformer/data/europarl-v7.fr-en.en\",\n",
        "           mode = \"r\",\n",
        "           encoding = \"utf-8\") as f:\n",
        "  europarl_en = f.read()\n",
        "\n",
        "with open (\"/content/drive/My Drive/transformer/data/europarl-v7.fr-en.fr\",\n",
        "           mode = \"r\",\n",
        "           encoding = \"utf-8\") as f:\n",
        "  europarl_fr = f.read()\n",
        "\n",
        "\n",
        "with open (\"/content/drive/My Drive/transformer/data/nonbreaking_prefix.en\",\n",
        "           mode = \"r\",\n",
        "           encoding = \"utf-8\") as f:\n",
        "  non_breaking_prefix_en = f.read()    \n",
        "\n",
        "\n",
        "with open (\"/content/drive/My Drive/transformer/data/nonbreaking_prefix.fr\",\n",
        "           mode = \"r\",\n",
        "           encoding = \"utf-8\") as f:\n",
        "  non_breaking_prefix_fr = f.read()  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEFw0D2vP_Dl",
        "colab_type": "text"
      },
      "source": [
        "## Cleaning data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwIBeGXn7LIJ",
        "colab_type": "text"
      },
      "source": [
        "Getting the non_breaking_prefixes as a clean list of words with a point at the end so it is easier to use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_TeuktU40Cb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "non_breaking_prefix_en = non_breaking_prefix_en.split(\"\\n\")\n",
        "non_breaking_prefix_en = [' ' + pref + '.' for pref in non_breaking_prefix_en]\n",
        "non_breaking_prefix_fr = non_breaking_prefix_fr.split(\"\\n\")\n",
        "non_breaking_prefix_fr = [' ' + pref + '.' for pref in non_breaking_prefix_fr]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9x4mZfKMaxD",
        "colab_type": "text"
      },
      "source": [
        "We will need each word and other symbol that we want to keep to be in lower case and separated by spaces so we can \"tokenize\" them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qg-8LLK-WdFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_en = europarl_en\n",
        "# Add $$$ after non ending sentence points\n",
        "for prefix in non_breaking_prefix_en:\n",
        "    corpus_en = corpus_en.replace(prefix, prefix + '$$$')\n",
        "corpus_en = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_en)\n",
        "# Remove $$$ markers\n",
        "corpus_en = re.sub(r\".\\$\\$\\$\", '', corpus_en)\n",
        "# Clear multiple spaces\n",
        "corpus_en = re.sub(r\"  +\", \" \", corpus_en)\n",
        "corpus_en = corpus_en.split('\\n')\n",
        "\n",
        "corpus_fr = europarl_fr\n",
        "for prefix in non_breaking_prefix_fr:\n",
        "    corpus_fr = corpus_fr.replace(prefix, prefix + '$$$')\n",
        "corpus_fr = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_fr)\n",
        "corpus_fr = re.sub(r\".\\$\\$\\$\", '', corpus_fr)\n",
        "corpus_fr = re.sub(r\"  +\", \" \", corpus_fr)\n",
        "corpus_fr = corpus_fr.split('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-Y9v8-Tozl2",
        "colab_type": "text"
      },
      "source": [
        "## Tokenizing text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5YXanmOd_xK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    corpus_en, target_vocab_size=2**13)\n",
        "tokenizer_fr = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    corpus_fr, target_vocab_size=2**13)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftIbPzIwCtwL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCAB_SIZE_EN = tokenizer_en.vocab_size + 2 # = 8190\n",
        "VOCAB_SIZE_FR = tokenizer_fr.vocab_size + 2 # = 8171"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPFe2YJDC9jw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = [[VOCAB_SIZE_EN-2] + tokenizer_en.encode(sentence) + [VOCAB_SIZE_EN-1]\n",
        "          for sentence in corpus_en]\n",
        "outputs = [[VOCAB_SIZE_FR-2] + tokenizer_fr.encode(sentence) + [VOCAB_SIZE_FR-1]\n",
        "           for sentence in corpus_fr]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bG6AlcFMpC5C",
        "colab_type": "text"
      },
      "source": [
        "## Remove too long sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6CD6PLGyQWy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = 20\n",
        "idx_to_remove = [count for count, sent in enumerate(inputs)\n",
        "                 if len(sent) > MAX_LENGTH]\n",
        "for idx in reversed(idx_to_remove):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]\n",
        "idx_to_remove = [count for count, sent in enumerate(outputs)\n",
        "                 if len(sent) > MAX_LENGTH]\n",
        "for idx in reversed(idx_to_remove):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ypm8h5aZQTZ1",
        "colab_type": "text"
      },
      "source": [
        "## Inputs/outputs creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FP0WPsdM8hl",
        "colab_type": "text"
      },
      "source": [
        "As we train with batches, we need each input to have the same length. We pad with the appropriate token, and we will make sure this padding token doesn't interfere with our training later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvDfLDWUONlE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
        "                                                       value=0,\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=MAX_LENGTH)\n",
        "outputs = tf.keras.preprocessing.sequence.pad_sequences(outputs,\n",
        "                                                        value=0,\n",
        "                                                        padding='post',\n",
        "                                                        maxlen=MAX_LENGTH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFxMp3TOIYff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 20000\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n",
        "\n",
        "dataset = dataset.cache()\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycT0YqydRcUd",
        "colab_type": "text"
      },
      "source": [
        "# Stage 3: Model building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SBoH8G4XyR9",
        "colab_type": "text"
      },
      "source": [
        "## Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7G9C3ucmJ86I",
        "colab_type": "text"
      },
      "source": [
        "Positional encoding formulae:\n",
        "\n",
        "$PE_{(pos,2i)} =\\sin(pos/10000^{2i/dmodel})$\n",
        "\n",
        "$PE_{(pos,2i+1)} =\\cos(pos/10000^{2i/dmodel})$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2wc6sYlX0dr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionalEncoding(layers.Layer):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "    \n",
        "    def get_angles(self, pos, i, d_model):\n",
        "        angles = 1 / np.power(10000., (2*(i//2)) / np.float32(d_model))\n",
        "        return pos * angles\n",
        "\n",
        "    def call(self, inputs):\n",
        "        seq_length = inputs.shape.as_list()[-2]\n",
        "        d_model = inputs.shape.as_list()[-1]\n",
        "        angles = self.get_angles(np.arange(seq_length)[:, np.newaxis],\n",
        "                                 np.arange(d_model)[np.newaxis, :],\n",
        "                                 d_model)\n",
        "        angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
        "        angles[:, 1::2] = np.cos(angles[:, 1::2])\n",
        "        pos_encoding = angles[np.newaxis, ...]\n",
        "        return inputs + tf.cast(pos_encoding, tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcw8YIQqRhOJ",
        "colab_type": "text"
      },
      "source": [
        "## Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sffhwwvX-wj",
        "colab_type": "text"
      },
      "source": [
        "### Attention computation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VBuW6lESLDX",
        "colab_type": "text"
      },
      "source": [
        "$Attention(Q, K, V ) = \\text{softmax}\\left(\\dfrac{QK^T}{\\sqrt{d_k}}\\right)V $"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rEoCNJURbrT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scaled_dot_product_attention(queries, keys, values, mask):\n",
        "    product = tf.matmul(queries, keys, transpose_b=True)\n",
        "    \n",
        "    keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32)\n",
        "    scaled_product = product / tf.math.sqrt(keys_dim)\n",
        "    \n",
        "    if mask is not None:\n",
        "        scaled_product += (mask * -1e9)\n",
        "    \n",
        "    attention = tf.matmul(tf.nn.softmax(scaled_product, axis=-1), values)\n",
        "    \n",
        "    return attention"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MjtvXrfYEx7",
        "colab_type": "text"
      },
      "source": [
        "### Multi-head attention sublayer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvq4I9uTX5p7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttention(layers.Layer):\n",
        "    \n",
        "    def __init__(self, nb_proj):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.nb_proj = nb_proj\n",
        "        \n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        assert self.d_model % self.nb_proj == 0\n",
        "        \n",
        "        self.d_proj = self.d_model // self.nb_proj\n",
        "        \n",
        "        self.query_lin = layers.Dense(units=self.d_model)\n",
        "        self.key_lin = layers.Dense(units=self.d_model)\n",
        "        self.value_lin = layers.Dense(units=self.d_model)\n",
        "        \n",
        "        self.final_lin = layers.Dense(units=self.d_model)\n",
        "        \n",
        "    def split_proj(self, inputs, batch_size): # inputs: (batch_size, seq_length, d_model)\n",
        "        shape = (batch_size,\n",
        "                 -1,\n",
        "                 self.nb_proj,\n",
        "                 self.d_proj)\n",
        "        splited_inputs = tf.reshape(inputs, shape=shape) # (batch_size, seq_length, nb_proj, d_proj)\n",
        "        return tf.transpose(splited_inputs, perm=[0, 2, 1, 3]) # (batch_size, nb_proj, seq_length, d_proj)\n",
        "    \n",
        "    def call(self, queries, keys, values, mask):\n",
        "        batch_size = tf.shape(queries)[0]\n",
        "        \n",
        "        queries = self.query_lin(queries)\n",
        "        keys = self.key_lin(keys)\n",
        "        values = self.value_lin(values)\n",
        "        \n",
        "        queries = self.split_proj(queries, batch_size)\n",
        "        keys = self.split_proj(keys, batch_size)\n",
        "        values = self.split_proj(values, batch_size)\n",
        "        \n",
        "        attention = scaled_dot_product_attention(queries, keys, values, mask)\n",
        "        \n",
        "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
        "        \n",
        "        concat_attention = tf.reshape(attention,\n",
        "                                      shape=(batch_size, -1, self.d_model))\n",
        "        \n",
        "        outputs = self.final_lin(concat_attention)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiyuHe1OeT5N",
        "colab_type": "text"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UV0ZMH7KT_KZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(layers.Layer):\n",
        "    \n",
        "    def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.FFN_units = FFN_units\n",
        "        self.nb_proj = nb_proj\n",
        "        self.dropout_rate = dropout_rate\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        \n",
        "        self.multi_head_attention = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        self.dense_1 = layers.Dense(units=self.FFN_units, activation=\"relu\")\n",
        "        self.dense_2 = layers.Dense(units=self.d_model)\n",
        "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "    def call(self, inputs, mask, training):\n",
        "        attention = self.multi_head_attention(inputs,\n",
        "                                              inputs,\n",
        "                                              inputs,\n",
        "                                              mask)\n",
        "        attention = self.dropout_1(attention, training=training)\n",
        "        attention = self.norm_1(attention + inputs)\n",
        "        \n",
        "        outputs = self.dense_1(attention)\n",
        "        outputs = self.dense_2(outputs)\n",
        "        outputs = self.dropout_2(outputs, training=training)\n",
        "        outputs = self.norm_2(outputs + attention)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-P92KeZih60",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(layers.Layer):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 nb_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout_rate,\n",
        "                 vocab_size,\n",
        "                 d_model,\n",
        "                 name=\"encoder\"):\n",
        "        super(Encoder, self).__init__(name=name)\n",
        "        self.nb_layers = nb_layers\n",
        "        self.d_model = d_model\n",
        "        \n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "        self.enc_layers = [EncoderLayer(FFN_units,\n",
        "                                        nb_proj,\n",
        "                                        dropout_rate) \n",
        "                           for _ in range(nb_layers)]\n",
        "    \n",
        "    def call(self, inputs, mask, training):\n",
        "        outputs = self.embedding(inputs)\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        outputs = self.pos_encoding(outputs)\n",
        "        outputs = self.dropout(outputs, training)\n",
        "        \n",
        "        for i in range(self.nb_layers):\n",
        "            outputs = self.enc_layers[i](outputs, mask, training)\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DthraBEwuvl",
        "colab_type": "text"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZWZyFBnwy8u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderLayer(layers.Layer):\n",
        "    \n",
        "    def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.FFN_units = FFN_units\n",
        "        self.nb_proj = nb_proj\n",
        "        self.dropout_rate = dropout_rate\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        \n",
        "        # Self multi head attention\n",
        "        self.multi_head_attention_1 = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        # Multi head attention combined with encoder output\n",
        "        self.multi_head_attention_2 = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        # Feed foward\n",
        "        self.dense_1 = layers.Dense(units=self.FFN_units,\n",
        "                                    activation=\"relu\")\n",
        "        self.dense_2 = layers.Dense(units=self.d_model)\n",
        "        self.dropout_3 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_3 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "        attention = self.multi_head_attention_1(inputs,\n",
        "                                                inputs,\n",
        "                                                inputs,\n",
        "                                                mask_1)\n",
        "        attention = self.dropout_1(attention, training)\n",
        "        attention = self.norm_1(attention + inputs)\n",
        "        \n",
        "        attention_2 = self.multi_head_attention_2(attention,\n",
        "                                                  enc_outputs,\n",
        "                                                  enc_outputs,\n",
        "                                                  mask_2)\n",
        "        attention_2 = self.dropout_2(attention_2, training)\n",
        "        attention_2 = self.norm_2(attention_2 + attention)\n",
        "        \n",
        "        outputs = self.dense_1(attention_2)\n",
        "        outputs = self.dense_2(outputs)\n",
        "        outputs = self.dropout_3(outputs, training)\n",
        "        outputs = self.norm_3(outputs + attention_2)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpzdiWHiwywF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(layers.Layer):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 nb_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout_rate,\n",
        "                 vocab_size,\n",
        "                 d_model,\n",
        "                 name=\"decoder\"):\n",
        "        super(Decoder, self).__init__(name=name)\n",
        "        self.d_model = d_model\n",
        "        self.nb_layers = nb_layers\n",
        "        \n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "        \n",
        "        self.dec_layers = [DecoderLayer(FFN_units,\n",
        "                                        nb_proj,\n",
        "                                        dropout_rate) \n",
        "                           for i in range(nb_layers)]\n",
        "    \n",
        "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "        outputs = self.embedding(inputs)\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        outputs = self.pos_encoding(outputs)\n",
        "        outputs = self.dropout(outputs, training)\n",
        "        \n",
        "        for i in range(self.nb_layers):\n",
        "            outputs = self.dec_layers[i](outputs,\n",
        "                                         enc_outputs,\n",
        "                                         mask_1,\n",
        "                                         mask_2,\n",
        "                                         training)\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5sJYkjbz5DD",
        "colab_type": "text"
      },
      "source": [
        "## Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqvqNjJPwyh-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 vocab_size_enc,\n",
        "                 vocab_size_dec,\n",
        "                 d_model,\n",
        "                 nb_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout_rate,\n",
        "                 name=\"transformer\"):\n",
        "        super(Transformer, self).__init__(name=name)\n",
        "        \n",
        "        self.encoder = Encoder(nb_layers,\n",
        "                               FFN_units,\n",
        "                               nb_proj,\n",
        "                               dropout_rate,\n",
        "                               vocab_size_enc,\n",
        "                               d_model)\n",
        "        self.decoder = Decoder(nb_layers,\n",
        "                               FFN_units,\n",
        "                               nb_proj,\n",
        "                               dropout_rate,\n",
        "                               vocab_size_dec,\n",
        "                               d_model)\n",
        "        self.last_linear = layers.Dense(units=vocab_size_dec, name=\"lin_ouput\")\n",
        "    \n",
        "    def create_padding_mask(self, seq):\n",
        "        mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "        return mask[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "    def create_look_ahead_mask(self, seq):\n",
        "        seq_len = tf.shape(seq)[1]\n",
        "        look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "        return look_ahead_mask\n",
        "    \n",
        "    def call(self, enc_inputs, dec_inputs, training):\n",
        "        enc_mask = self.create_padding_mask(enc_inputs)\n",
        "        dec_mask_1 = tf.maximum(\n",
        "            self.create_padding_mask(dec_inputs),\n",
        "            self.create_look_ahead_mask(dec_inputs)\n",
        "        )\n",
        "        dec_mask_2 = self.create_padding_mask(enc_inputs)\n",
        "        \n",
        "        enc_outputs = self.encoder(enc_inputs, enc_mask, training)\n",
        "        dec_outputs = self.decoder(dec_inputs,\n",
        "                                   enc_outputs,\n",
        "                                   dec_mask_1,\n",
        "                                   dec_mask_2,\n",
        "                                   training)\n",
        "        \n",
        "        outputs = self.last_linear(dec_outputs)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-c-LRThUPrso",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiOdqQ5qPs8z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Hyper-parameters\n",
        "D_MODEL = 128 # 512\n",
        "NB_LAYERS = 4 # 6\n",
        "FFN_UNITS = 512 # 2048\n",
        "NB_PROJ = 8 # 8\n",
        "DROPOUT_RATE = 0.1 # 0.1\n",
        "\n",
        "transformer = Transformer(vocab_size_enc=VOCAB_SIZE_EN,\n",
        "                          vocab_size_dec=VOCAB_SIZE_FR,\n",
        "                          d_model=D_MODEL,\n",
        "                          nb_layers=NB_LAYERS,\n",
        "                          FFN_units=FFN_UNITS,\n",
        "                          nb_proj=NB_PROJ,\n",
        "                          dropout_rate=DROPOUT_RATE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46xg4Wrg1Wgl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                            reduction=\"none\")\n",
        "\n",
        "def loss_function(target, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(target, 0))\n",
        "    loss_ = loss_object(target, pred)\n",
        "    \n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    \n",
        "    return tf.reduce_mean(loss_)\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Goque362343",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    \n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "        \n",
        "        self.d_model = tf.cast(d_model, tf.float32)\n",
        "        self.warmup_steps = warmup_steps\n",
        "    \n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps**-1.5)\n",
        "        \n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "leaning_rate = CustomSchedule(D_MODEL)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(leaning_rate,\n",
        "                                     beta_1=0.9,\n",
        "                                     beta_2=0.98,\n",
        "                                     epsilon=1e-9)\n",
        "\n",
        "\n",
        "#All these specs are according to the paper by Google Research\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nb_32PIU5Zkh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_path = \"./drive/My Drive/projects/transformer/ckpt/\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Latest checkpoint restored!!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhFK5kUx602K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dde5d384-bf27-4fb1-ce12-99dd9eca7b99"
      },
      "source": [
        "EPOCHS = 10\n",
        "for epoch in range(EPOCHS):\n",
        "    print(\"Start of epoch {}\".format(epoch+1))\n",
        "    start = time.time()\n",
        "    \n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    \n",
        "    for (batch, (enc_inputs, targets)) in enumerate(dataset):\n",
        "        dec_inputs = targets[:, :-1]\n",
        "        dec_outputs_real = targets[:, 1:]\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = transformer(enc_inputs, dec_inputs, True)\n",
        "            loss = loss_function(dec_outputs_real, predictions)\n",
        "        \n",
        "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "        \n",
        "        train_loss(loss)\n",
        "        train_accuracy(dec_outputs_real, predictions)\n",
        "        \n",
        "        if batch % 50 == 0:\n",
        "            print(\"Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}\".format(\n",
        "                epoch+1, batch, train_loss.result(), train_accuracy.result()))\n",
        "            \n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print(\"Saving checkpoint for epoch {} at {}\".format(epoch+1,\n",
        "                                                        ckpt_save_path))\n",
        "    print(\"Time taken for 1 epoch: {} secs\\n\".format(time.time() - start))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start of epoch 1\n",
            "Epoch 1 Batch 0 Loss 5.7881 Accuracy 0.0526\n",
            "Epoch 1 Batch 50 Loss 5.3025 Accuracy 0.0526\n",
            "Epoch 1 Batch 100 Loss 5.1148 Accuracy 0.0530\n",
            "Epoch 1 Batch 150 Loss 4.9854 Accuracy 0.0611\n",
            "Epoch 1 Batch 200 Loss 4.8710 Accuracy 0.0691\n",
            "Epoch 1 Batch 250 Loss 4.7812 Accuracy 0.0763\n",
            "Epoch 1 Batch 300 Loss 4.6925 Accuracy 0.0835\n",
            "Epoch 1 Batch 350 Loss 4.6059 Accuracy 0.0910\n",
            "Epoch 1 Batch 400 Loss 4.5208 Accuracy 0.0977\n",
            "Epoch 1 Batch 450 Loss 4.4397 Accuracy 0.1040\n",
            "Epoch 1 Batch 500 Loss 4.3577 Accuracy 0.1097\n",
            "Epoch 1 Batch 550 Loss 4.2856 Accuracy 0.1156\n",
            "Epoch 1 Batch 600 Loss 4.2215 Accuracy 0.1211\n",
            "Epoch 1 Batch 650 Loss 4.1570 Accuracy 0.1266\n",
            "Epoch 1 Batch 700 Loss 4.0945 Accuracy 0.1318\n",
            "Epoch 1 Batch 750 Loss 4.0405 Accuracy 0.1367\n",
            "Epoch 1 Batch 800 Loss 3.9913 Accuracy 0.1413\n",
            "Epoch 1 Batch 850 Loss 3.9407 Accuracy 0.1456\n",
            "Epoch 1 Batch 900 Loss 3.8912 Accuracy 0.1496\n",
            "Epoch 1 Batch 950 Loss 3.8450 Accuracy 0.1533\n",
            "Epoch 1 Batch 1000 Loss 3.8004 Accuracy 0.1571\n",
            "Epoch 1 Batch 1050 Loss 3.7598 Accuracy 0.1603\n",
            "Epoch 1 Batch 1100 Loss 3.7223 Accuracy 0.1633\n",
            "Epoch 1 Batch 1150 Loss 3.6869 Accuracy 0.1661\n",
            "Epoch 1 Batch 1200 Loss 3.6542 Accuracy 0.1688\n",
            "Epoch 1 Batch 1250 Loss 3.6224 Accuracy 0.1714\n",
            "Epoch 1 Batch 1300 Loss 3.5939 Accuracy 0.1739\n",
            "Epoch 1 Batch 1350 Loss 3.5650 Accuracy 0.1762\n",
            "Epoch 1 Batch 1400 Loss 3.5378 Accuracy 0.1787\n",
            "Epoch 1 Batch 1450 Loss 3.5116 Accuracy 0.1812\n",
            "Epoch 1 Batch 1500 Loss 3.4861 Accuracy 0.1836\n",
            "Epoch 1 Batch 1550 Loss 3.4611 Accuracy 0.1859\n",
            "Epoch 1 Batch 1600 Loss 3.4370 Accuracy 0.1882\n",
            "Epoch 1 Batch 1650 Loss 3.4140 Accuracy 0.1905\n",
            "Epoch 1 Batch 1700 Loss 3.3914 Accuracy 0.1928\n",
            "Epoch 1 Batch 1750 Loss 3.3700 Accuracy 0.1949\n",
            "Epoch 1 Batch 1800 Loss 3.3484 Accuracy 0.1971\n",
            "Epoch 1 Batch 1850 Loss 3.3284 Accuracy 0.1992\n",
            "Epoch 1 Batch 1900 Loss 3.3082 Accuracy 0.2013\n",
            "Epoch 1 Batch 1950 Loss 3.2886 Accuracy 0.2033\n",
            "Epoch 1 Batch 2000 Loss 3.2695 Accuracy 0.2051\n",
            "Epoch 1 Batch 2050 Loss 3.2509 Accuracy 0.2068\n",
            "Epoch 1 Batch 2100 Loss 3.2320 Accuracy 0.2083\n",
            "Epoch 1 Batch 2150 Loss 3.2133 Accuracy 0.2097\n",
            "Epoch 1 Batch 2200 Loss 3.1937 Accuracy 0.2112\n",
            "Epoch 1 Batch 2250 Loss 3.1758 Accuracy 0.2128\n",
            "Epoch 1 Batch 2300 Loss 3.1573 Accuracy 0.2142\n",
            "Epoch 1 Batch 2350 Loss 3.1402 Accuracy 0.2156\n",
            "Epoch 1 Batch 2400 Loss 3.1233 Accuracy 0.2171\n",
            "Epoch 1 Batch 2450 Loss 3.1053 Accuracy 0.2186\n",
            "Epoch 1 Batch 2500 Loss 3.0885 Accuracy 0.2202\n",
            "Epoch 1 Batch 2550 Loss 3.0721 Accuracy 0.2218\n",
            "Epoch 1 Batch 2600 Loss 3.0552 Accuracy 0.2234\n",
            "Epoch 1 Batch 2650 Loss 3.0385 Accuracy 0.2250\n",
            "Epoch 1 Batch 2700 Loss 3.0217 Accuracy 0.2266\n",
            "Epoch 1 Batch 2750 Loss 3.0065 Accuracy 0.2282\n",
            "Epoch 1 Batch 2800 Loss 2.9916 Accuracy 0.2297\n",
            "Epoch 1 Batch 2850 Loss 2.9763 Accuracy 0.2313\n",
            "Epoch 1 Batch 2900 Loss 2.9614 Accuracy 0.2329\n",
            "Epoch 1 Batch 2950 Loss 2.9463 Accuracy 0.2344\n",
            "Epoch 1 Batch 3000 Loss 2.9320 Accuracy 0.2359\n",
            "Epoch 1 Batch 3050 Loss 2.9173 Accuracy 0.2374\n",
            "Epoch 1 Batch 3100 Loss 2.9034 Accuracy 0.2390\n",
            "Epoch 1 Batch 3150 Loss 2.8891 Accuracy 0.2405\n",
            "Epoch 1 Batch 3200 Loss 2.8743 Accuracy 0.2420\n",
            "Epoch 1 Batch 3250 Loss 2.8606 Accuracy 0.2435\n",
            "Epoch 1 Batch 3300 Loss 2.8459 Accuracy 0.2450\n",
            "Epoch 1 Batch 3350 Loss 2.8325 Accuracy 0.2465\n",
            "Epoch 1 Batch 3400 Loss 2.8190 Accuracy 0.2480\n",
            "Epoch 1 Batch 3450 Loss 2.8059 Accuracy 0.2496\n",
            "Epoch 1 Batch 3500 Loss 2.7929 Accuracy 0.2511\n",
            "Epoch 1 Batch 3550 Loss 2.7797 Accuracy 0.2526\n",
            "Epoch 1 Batch 3600 Loss 2.7667 Accuracy 0.2542\n",
            "Epoch 1 Batch 3650 Loss 2.7541 Accuracy 0.2557\n",
            "Epoch 1 Batch 3700 Loss 2.7413 Accuracy 0.2573\n",
            "Epoch 1 Batch 3750 Loss 2.7291 Accuracy 0.2587\n",
            "Epoch 1 Batch 3800 Loss 2.7171 Accuracy 0.2603\n",
            "Epoch 1 Batch 3850 Loss 2.7052 Accuracy 0.2618\n",
            "Epoch 1 Batch 3900 Loss 2.6936 Accuracy 0.2633\n",
            "Epoch 1 Batch 3950 Loss 2.6824 Accuracy 0.2647\n",
            "Epoch 1 Batch 4000 Loss 2.6705 Accuracy 0.2662\n",
            "Epoch 1 Batch 4050 Loss 2.6594 Accuracy 0.2676\n",
            "Epoch 1 Batch 4100 Loss 2.6482 Accuracy 0.2691\n",
            "Epoch 1 Batch 4150 Loss 2.6373 Accuracy 0.2704\n",
            "Epoch 1 Batch 4200 Loss 2.6276 Accuracy 0.2716\n",
            "Epoch 1 Batch 4250 Loss 2.6180 Accuracy 0.2728\n",
            "Epoch 1 Batch 4300 Loss 2.6092 Accuracy 0.2739\n",
            "Epoch 1 Batch 4350 Loss 2.6006 Accuracy 0.2750\n",
            "Epoch 1 Batch 4400 Loss 2.5919 Accuracy 0.2761\n",
            "Epoch 1 Batch 4450 Loss 2.5833 Accuracy 0.2771\n",
            "Epoch 1 Batch 4500 Loss 2.5749 Accuracy 0.2781\n",
            "Epoch 1 Batch 4550 Loss 2.5668 Accuracy 0.2791\n",
            "Epoch 1 Batch 4600 Loss 2.5591 Accuracy 0.2800\n",
            "Epoch 1 Batch 4650 Loss 2.5510 Accuracy 0.2810\n",
            "Epoch 1 Batch 4700 Loss 2.5431 Accuracy 0.2819\n",
            "Epoch 1 Batch 4750 Loss 2.5356 Accuracy 0.2829\n",
            "Epoch 1 Batch 4800 Loss 2.5280 Accuracy 0.2838\n",
            "Epoch 1 Batch 4850 Loss 2.5204 Accuracy 0.2848\n",
            "Epoch 1 Batch 4900 Loss 2.5128 Accuracy 0.2857\n",
            "Epoch 1 Batch 4950 Loss 2.5056 Accuracy 0.2866\n",
            "Epoch 1 Batch 5000 Loss 2.4983 Accuracy 0.2875\n",
            "Epoch 1 Batch 5050 Loss 2.4913 Accuracy 0.2883\n",
            "Epoch 1 Batch 5100 Loss 2.4837 Accuracy 0.2891\n",
            "Epoch 1 Batch 5150 Loss 2.4768 Accuracy 0.2900\n",
            "Epoch 1 Batch 5200 Loss 2.4696 Accuracy 0.2908\n",
            "Epoch 1 Batch 5250 Loss 2.4628 Accuracy 0.2916\n",
            "Epoch 1 Batch 5300 Loss 2.4555 Accuracy 0.2923\n",
            "Epoch 1 Batch 5350 Loss 2.4485 Accuracy 0.2930\n",
            "Epoch 1 Batch 5400 Loss 2.4418 Accuracy 0.2938\n",
            "Epoch 1 Batch 5450 Loss 2.4352 Accuracy 0.2945\n",
            "Epoch 1 Batch 5500 Loss 2.4282 Accuracy 0.2953\n",
            "Epoch 1 Batch 5550 Loss 2.4213 Accuracy 0.2960\n",
            "Epoch 1 Batch 5600 Loss 2.4147 Accuracy 0.2968\n",
            "Epoch 1 Batch 5650 Loss 2.4079 Accuracy 0.2975\n",
            "Epoch 1 Batch 5700 Loss 2.4013 Accuracy 0.2982\n",
            "Saving checkpoint for epoch 1 at ./drive/My Drive/projects/transformer/ckpt/ckpt-1\n",
            "Time taken for 1 epoch: 1428.4687807559967 secs\n",
            "\n",
            "Start of epoch 2\n",
            "Epoch 2 Batch 0 Loss 1.7990 Accuracy 0.3684\n",
            "Epoch 2 Batch 50 Loss 1.7046 Accuracy 0.3833\n",
            "Epoch 2 Batch 100 Loss 1.6965 Accuracy 0.3857\n",
            "Epoch 2 Batch 150 Loss 1.6909 Accuracy 0.3864\n",
            "Epoch 2 Batch 200 Loss 1.6750 Accuracy 0.3883\n",
            "Epoch 2 Batch 250 Loss 1.6631 Accuracy 0.3892\n",
            "Epoch 2 Batch 300 Loss 1.6555 Accuracy 0.3895\n",
            "Epoch 2 Batch 350 Loss 1.6607 Accuracy 0.3904\n",
            "Epoch 2 Batch 400 Loss 1.6557 Accuracy 0.3911\n",
            "Epoch 2 Batch 450 Loss 1.6487 Accuracy 0.3918\n",
            "Epoch 2 Batch 500 Loss 1.6445 Accuracy 0.3919\n",
            "Epoch 2 Batch 550 Loss 1.6397 Accuracy 0.3922\n",
            "Epoch 2 Batch 600 Loss 1.6361 Accuracy 0.3925\n",
            "Epoch 2 Batch 650 Loss 1.6313 Accuracy 0.3930\n",
            "Epoch 2 Batch 700 Loss 1.6287 Accuracy 0.3935\n",
            "Epoch 2 Batch 750 Loss 1.6251 Accuracy 0.3938\n",
            "Epoch 2 Batch 800 Loss 1.6227 Accuracy 0.3943\n",
            "Epoch 2 Batch 850 Loss 1.6188 Accuracy 0.3946\n",
            "Epoch 2 Batch 900 Loss 1.6165 Accuracy 0.3949\n",
            "Epoch 2 Batch 950 Loss 1.6130 Accuracy 0.3950\n",
            "Epoch 2 Batch 1000 Loss 1.6080 Accuracy 0.3952\n",
            "Epoch 2 Batch 1050 Loss 1.6040 Accuracy 0.3955\n",
            "Epoch 2 Batch 1100 Loss 1.6020 Accuracy 0.3957\n",
            "Epoch 2 Batch 1150 Loss 1.5997 Accuracy 0.3960\n",
            "Epoch 2 Batch 1200 Loss 1.5977 Accuracy 0.3963\n",
            "Epoch 2 Batch 1250 Loss 1.5945 Accuracy 0.3967\n",
            "Epoch 2 Batch 1300 Loss 1.5909 Accuracy 0.3971\n",
            "Epoch 2 Batch 1350 Loss 1.5886 Accuracy 0.3978\n",
            "Epoch 2 Batch 1400 Loss 1.5848 Accuracy 0.3983\n",
            "Epoch 2 Batch 1450 Loss 1.5809 Accuracy 0.3991\n",
            "Epoch 2 Batch 1500 Loss 1.5761 Accuracy 0.4001\n",
            "Epoch 2 Batch 1550 Loss 1.5717 Accuracy 0.4012\n",
            "Epoch 2 Batch 1600 Loss 1.5677 Accuracy 0.4022\n",
            "Epoch 2 Batch 1650 Loss 1.5640 Accuracy 0.4031\n",
            "Epoch 2 Batch 1700 Loss 1.5604 Accuracy 0.4041\n",
            "Epoch 2 Batch 1750 Loss 1.5561 Accuracy 0.4050\n",
            "Epoch 2 Batch 1800 Loss 1.5529 Accuracy 0.4060\n",
            "Epoch 2 Batch 1850 Loss 1.5496 Accuracy 0.4070\n",
            "Epoch 2 Batch 1900 Loss 1.5461 Accuracy 0.4079\n",
            "Epoch 2 Batch 1950 Loss 1.5429 Accuracy 0.4088\n",
            "Epoch 2 Batch 2000 Loss 1.5396 Accuracy 0.4096\n",
            "Epoch 2 Batch 2050 Loss 1.5359 Accuracy 0.4103\n",
            "Epoch 2 Batch 2100 Loss 1.5327 Accuracy 0.4109\n",
            "Epoch 2 Batch 2150 Loss 1.5276 Accuracy 0.4114\n",
            "Epoch 2 Batch 2200 Loss 1.5230 Accuracy 0.4118\n",
            "Epoch 2 Batch 2250 Loss 1.5183 Accuracy 0.4122\n",
            "Epoch 2 Batch 2300 Loss 1.5132 Accuracy 0.4126\n",
            "Epoch 2 Batch 2350 Loss 1.5083 Accuracy 0.4130\n",
            "Epoch 2 Batch 2400 Loss 1.5041 Accuracy 0.4134\n",
            "Epoch 2 Batch 2450 Loss 1.4995 Accuracy 0.4139\n",
            "Epoch 2 Batch 2500 Loss 1.4945 Accuracy 0.4145\n",
            "Epoch 2 Batch 2550 Loss 1.4898 Accuracy 0.4150\n",
            "Epoch 2 Batch 2600 Loss 1.4854 Accuracy 0.4155\n",
            "Epoch 2 Batch 2650 Loss 1.4805 Accuracy 0.4162\n",
            "Epoch 2 Batch 2700 Loss 1.4762 Accuracy 0.4168\n",
            "Epoch 2 Batch 2750 Loss 1.4720 Accuracy 0.4173\n",
            "Epoch 2 Batch 2800 Loss 1.4678 Accuracy 0.4179\n",
            "Epoch 2 Batch 2850 Loss 1.4640 Accuracy 0.4185\n",
            "Epoch 2 Batch 2900 Loss 1.4603 Accuracy 0.4190\n",
            "Epoch 2 Batch 2950 Loss 1.4564 Accuracy 0.4196\n",
            "Epoch 2 Batch 3000 Loss 1.4525 Accuracy 0.4201\n",
            "Epoch 2 Batch 3050 Loss 1.4486 Accuracy 0.4206\n",
            "Epoch 2 Batch 3100 Loss 1.4450 Accuracy 0.4211\n",
            "Epoch 2 Batch 3150 Loss 1.4416 Accuracy 0.4217\n",
            "Epoch 2 Batch 3200 Loss 1.4377 Accuracy 0.4223\n",
            "Epoch 2 Batch 3250 Loss 1.4342 Accuracy 0.4227\n",
            "Epoch 2 Batch 3300 Loss 1.4308 Accuracy 0.4232\n",
            "Epoch 2 Batch 3350 Loss 1.4266 Accuracy 0.4237\n",
            "Epoch 2 Batch 3400 Loss 1.4234 Accuracy 0.4242\n",
            "Epoch 2 Batch 3450 Loss 1.4200 Accuracy 0.4247\n",
            "Epoch 2 Batch 3500 Loss 1.4164 Accuracy 0.4252\n",
            "Epoch 2 Batch 3550 Loss 1.4132 Accuracy 0.4257\n",
            "Epoch 2 Batch 3600 Loss 1.4096 Accuracy 0.4264\n",
            "Epoch 2 Batch 3650 Loss 1.4062 Accuracy 0.4270\n",
            "Epoch 2 Batch 3700 Loss 1.4028 Accuracy 0.4275\n",
            "Epoch 2 Batch 3750 Loss 1.3996 Accuracy 0.4281\n",
            "Epoch 2 Batch 3800 Loss 1.3962 Accuracy 0.4286\n",
            "Epoch 2 Batch 3850 Loss 1.3933 Accuracy 0.4291\n",
            "Epoch 2 Batch 3900 Loss 1.3908 Accuracy 0.4297\n",
            "Epoch 2 Batch 3950 Loss 1.3878 Accuracy 0.4303\n",
            "Epoch 2 Batch 4000 Loss 1.3846 Accuracy 0.4309\n",
            "Epoch 2 Batch 4050 Loss 1.3819 Accuracy 0.4314\n",
            "Epoch 2 Batch 4100 Loss 1.3793 Accuracy 0.4318\n",
            "Epoch 2 Batch 4150 Loss 1.3775 Accuracy 0.4321\n",
            "Epoch 2 Batch 4200 Loss 1.3766 Accuracy 0.4324\n",
            "Epoch 2 Batch 4250 Loss 1.3755 Accuracy 0.4326\n",
            "Epoch 2 Batch 4300 Loss 1.3749 Accuracy 0.4327\n",
            "Epoch 2 Batch 4350 Loss 1.3743 Accuracy 0.4329\n",
            "Epoch 2 Batch 4400 Loss 1.3740 Accuracy 0.4329\n",
            "Epoch 2 Batch 4450 Loss 1.3737 Accuracy 0.4329\n",
            "Epoch 2 Batch 4500 Loss 1.3739 Accuracy 0.4329\n",
            "Epoch 2 Batch 4550 Loss 1.3740 Accuracy 0.4330\n",
            "Epoch 2 Batch 4600 Loss 1.3740 Accuracy 0.4330\n",
            "Epoch 2 Batch 4650 Loss 1.3742 Accuracy 0.4331\n",
            "Epoch 2 Batch 4700 Loss 1.3741 Accuracy 0.4331\n",
            "Epoch 2 Batch 4750 Loss 1.3741 Accuracy 0.4331\n",
            "Epoch 2 Batch 4800 Loss 1.3741 Accuracy 0.4331\n",
            "Epoch 2 Batch 4850 Loss 1.3742 Accuracy 0.4332\n",
            "Epoch 2 Batch 4900 Loss 1.3741 Accuracy 0.4332\n",
            "Epoch 2 Batch 4950 Loss 1.3741 Accuracy 0.4332\n",
            "Epoch 2 Batch 5000 Loss 1.3742 Accuracy 0.4331\n",
            "Epoch 2 Batch 5050 Loss 1.3741 Accuracy 0.4331\n",
            "Epoch 2 Batch 5100 Loss 1.3738 Accuracy 0.4331\n",
            "Epoch 2 Batch 5150 Loss 1.3740 Accuracy 0.4331\n",
            "Epoch 2 Batch 5200 Loss 1.3738 Accuracy 0.4331\n",
            "Epoch 2 Batch 5250 Loss 1.3736 Accuracy 0.4330\n",
            "Epoch 2 Batch 5300 Loss 1.3735 Accuracy 0.4329\n",
            "Epoch 2 Batch 5350 Loss 1.3733 Accuracy 0.4328\n",
            "Epoch 2 Batch 5400 Loss 1.3731 Accuracy 0.4328\n",
            "Epoch 2 Batch 5450 Loss 1.3727 Accuracy 0.4327\n",
            "Epoch 2 Batch 5500 Loss 1.3727 Accuracy 0.4327\n",
            "Epoch 2 Batch 5550 Loss 1.3723 Accuracy 0.4326\n",
            "Epoch 2 Batch 5600 Loss 1.3719 Accuracy 0.4326\n",
            "Epoch 2 Batch 5650 Loss 1.3718 Accuracy 0.4325\n",
            "Epoch 2 Batch 5700 Loss 1.3714 Accuracy 0.4324\n",
            "Saving checkpoint for epoch 2 at ./drive/My Drive/projects/transformer/ckpt/ckpt-2\n",
            "Time taken for 1 epoch: 1458.2823185920715 secs\n",
            "\n",
            "Start of epoch 3\n",
            "Epoch 3 Batch 0 Loss 1.2167 Accuracy 0.4712\n",
            "Epoch 3 Batch 50 Loss 1.3539 Accuracy 0.4370\n",
            "Epoch 3 Batch 100 Loss 1.3557 Accuracy 0.4378\n",
            "Epoch 3 Batch 150 Loss 1.3413 Accuracy 0.4367\n",
            "Epoch 3 Batch 200 Loss 1.3395 Accuracy 0.4357\n",
            "Epoch 3 Batch 250 Loss 1.3357 Accuracy 0.4373\n",
            "Epoch 3 Batch 300 Loss 1.3288 Accuracy 0.4369\n",
            "Epoch 3 Batch 350 Loss 1.3256 Accuracy 0.4375\n",
            "Epoch 3 Batch 400 Loss 1.3208 Accuracy 0.4375\n",
            "Epoch 3 Batch 450 Loss 1.3196 Accuracy 0.4381\n",
            "Epoch 3 Batch 500 Loss 1.3179 Accuracy 0.4379\n",
            "Epoch 3 Batch 550 Loss 1.3173 Accuracy 0.4380\n",
            "Epoch 3 Batch 600 Loss 1.3152 Accuracy 0.4381\n",
            "Epoch 3 Batch 650 Loss 1.3153 Accuracy 0.4388\n",
            "Epoch 3 Batch 700 Loss 1.3155 Accuracy 0.4389\n",
            "Epoch 3 Batch 750 Loss 1.3156 Accuracy 0.4389\n",
            "Epoch 3 Batch 800 Loss 1.3135 Accuracy 0.4391\n",
            "Epoch 3 Batch 850 Loss 1.3111 Accuracy 0.4394\n",
            "Epoch 3 Batch 900 Loss 1.3101 Accuracy 0.4396\n",
            "Epoch 3 Batch 950 Loss 1.3094 Accuracy 0.4398\n",
            "Epoch 3 Batch 1000 Loss 1.3066 Accuracy 0.4399\n",
            "Epoch 3 Batch 1050 Loss 1.3053 Accuracy 0.4400\n",
            "Epoch 3 Batch 1100 Loss 1.3029 Accuracy 0.4402\n",
            "Epoch 3 Batch 1150 Loss 1.3009 Accuracy 0.4405\n",
            "Epoch 3 Batch 1200 Loss 1.2975 Accuracy 0.4409\n",
            "Epoch 3 Batch 1250 Loss 1.2957 Accuracy 0.4414\n",
            "Epoch 3 Batch 1300 Loss 1.2928 Accuracy 0.4419\n",
            "Epoch 3 Batch 1350 Loss 1.2910 Accuracy 0.4424\n",
            "Epoch 3 Batch 1400 Loss 1.2889 Accuracy 0.4430\n",
            "Epoch 3 Batch 1450 Loss 1.2859 Accuracy 0.4437\n",
            "Epoch 3 Batch 1500 Loss 1.2832 Accuracy 0.4445\n",
            "Epoch 3 Batch 1550 Loss 1.2796 Accuracy 0.4453\n",
            "Epoch 3 Batch 1600 Loss 1.2768 Accuracy 0.4460\n",
            "Epoch 3 Batch 1650 Loss 1.2746 Accuracy 0.4468\n",
            "Epoch 3 Batch 1700 Loss 1.2717 Accuracy 0.4477\n",
            "Epoch 3 Batch 1750 Loss 1.2686 Accuracy 0.4485\n",
            "Epoch 3 Batch 1800 Loss 1.2664 Accuracy 0.4493\n",
            "Epoch 3 Batch 1850 Loss 1.2638 Accuracy 0.4502\n",
            "Epoch 3 Batch 1900 Loss 1.2616 Accuracy 0.4510\n",
            "Epoch 3 Batch 1950 Loss 1.2587 Accuracy 0.4518\n",
            "Epoch 3 Batch 2000 Loss 1.2567 Accuracy 0.4525\n",
            "Epoch 3 Batch 2050 Loss 1.2542 Accuracy 0.4528\n",
            "Epoch 3 Batch 2100 Loss 1.2518 Accuracy 0.4531\n",
            "Epoch 3 Batch 2150 Loss 1.2490 Accuracy 0.4535\n",
            "Epoch 3 Batch 2200 Loss 1.2451 Accuracy 0.4538\n",
            "Epoch 3 Batch 2250 Loss 1.2415 Accuracy 0.4540\n",
            "Epoch 3 Batch 2300 Loss 1.2379 Accuracy 0.4544\n",
            "Epoch 3 Batch 2350 Loss 1.2343 Accuracy 0.4547\n",
            "Epoch 3 Batch 2400 Loss 1.2310 Accuracy 0.4550\n",
            "Epoch 3 Batch 2450 Loss 1.2276 Accuracy 0.4554\n",
            "Epoch 3 Batch 2500 Loss 1.2243 Accuracy 0.4558\n",
            "Epoch 3 Batch 2550 Loss 1.2211 Accuracy 0.4561\n",
            "Epoch 3 Batch 2600 Loss 1.2176 Accuracy 0.4566\n",
            "Epoch 3 Batch 2650 Loss 1.2143 Accuracy 0.4570\n",
            "Epoch 3 Batch 2700 Loss 1.2116 Accuracy 0.4574\n",
            "Epoch 3 Batch 2750 Loss 1.2083 Accuracy 0.4578\n",
            "Epoch 3 Batch 2800 Loss 1.2063 Accuracy 0.4582\n",
            "Epoch 3 Batch 2850 Loss 1.2037 Accuracy 0.4586\n",
            "Epoch 3 Batch 2900 Loss 1.2008 Accuracy 0.4589\n",
            "Epoch 3 Batch 2950 Loss 1.1980 Accuracy 0.4593\n",
            "Epoch 3 Batch 3000 Loss 1.1955 Accuracy 0.4597\n",
            "Epoch 3 Batch 3050 Loss 1.1931 Accuracy 0.4600\n",
            "Epoch 3 Batch 3100 Loss 1.1907 Accuracy 0.4603\n",
            "Epoch 3 Batch 3150 Loss 1.1883 Accuracy 0.4605\n",
            "Epoch 3 Batch 3200 Loss 1.1858 Accuracy 0.4608\n",
            "Epoch 3 Batch 3250 Loss 1.1835 Accuracy 0.4612\n",
            "Epoch 3 Batch 3300 Loss 1.1807 Accuracy 0.4616\n",
            "Epoch 3 Batch 3350 Loss 1.1779 Accuracy 0.4620\n",
            "Epoch 3 Batch 3400 Loss 1.1758 Accuracy 0.4623\n",
            "Epoch 3 Batch 3450 Loss 1.1732 Accuracy 0.4627\n",
            "Epoch 3 Batch 3500 Loss 1.1711 Accuracy 0.4631\n",
            "Epoch 3 Batch 3550 Loss 1.1691 Accuracy 0.4634\n",
            "Epoch 3 Batch 3600 Loss 1.1667 Accuracy 0.4638\n",
            "Epoch 3 Batch 3650 Loss 1.1644 Accuracy 0.4642\n",
            "Epoch 3 Batch 3700 Loss 1.1622 Accuracy 0.4645\n",
            "Epoch 3 Batch 3750 Loss 1.1601 Accuracy 0.4649\n",
            "Epoch 3 Batch 3800 Loss 1.1580 Accuracy 0.4654\n",
            "Epoch 3 Batch 3850 Loss 1.1560 Accuracy 0.4657\n",
            "Epoch 3 Batch 3900 Loss 1.1541 Accuracy 0.4661\n",
            "Epoch 3 Batch 3950 Loss 1.1526 Accuracy 0.4665\n",
            "Epoch 3 Batch 4000 Loss 1.1507 Accuracy 0.4670\n",
            "Epoch 3 Batch 4050 Loss 1.1491 Accuracy 0.4674\n",
            "Epoch 3 Batch 4100 Loss 1.1475 Accuracy 0.4677\n",
            "Epoch 3 Batch 4150 Loss 1.1465 Accuracy 0.4679\n",
            "Epoch 3 Batch 4200 Loss 1.1463 Accuracy 0.4681\n",
            "Epoch 3 Batch 4250 Loss 1.1464 Accuracy 0.4681\n",
            "Epoch 3 Batch 4300 Loss 1.1469 Accuracy 0.4682\n",
            "Epoch 3 Batch 4350 Loss 1.1474 Accuracy 0.4681\n",
            "Epoch 3 Batch 4400 Loss 1.1479 Accuracy 0.4680\n",
            "Epoch 3 Batch 4450 Loss 1.1485 Accuracy 0.4679\n",
            "Epoch 3 Batch 4500 Loss 1.1493 Accuracy 0.4678\n",
            "Epoch 3 Batch 4550 Loss 1.1501 Accuracy 0.4676\n",
            "Epoch 3 Batch 4600 Loss 1.1512 Accuracy 0.4674\n",
            "Epoch 3 Batch 4650 Loss 1.1522 Accuracy 0.4673\n",
            "Epoch 3 Batch 4700 Loss 1.1530 Accuracy 0.4673\n",
            "Epoch 3 Batch 4750 Loss 1.1542 Accuracy 0.4672\n",
            "Epoch 3 Batch 4800 Loss 1.1548 Accuracy 0.4671\n",
            "Epoch 3 Batch 4850 Loss 1.1555 Accuracy 0.4670\n",
            "Epoch 3 Batch 4900 Loss 1.1564 Accuracy 0.4668\n",
            "Epoch 3 Batch 4950 Loss 1.1573 Accuracy 0.4667\n",
            "Epoch 3 Batch 5000 Loss 1.1581 Accuracy 0.4666\n",
            "Epoch 3 Batch 5050 Loss 1.1590 Accuracy 0.4665\n",
            "Epoch 3 Batch 5100 Loss 1.1599 Accuracy 0.4664\n",
            "Epoch 3 Batch 5150 Loss 1.1607 Accuracy 0.4661\n",
            "Epoch 3 Batch 5200 Loss 1.1615 Accuracy 0.4659\n",
            "Epoch 3 Batch 5250 Loss 1.1623 Accuracy 0.4657\n",
            "Epoch 3 Batch 5300 Loss 1.1629 Accuracy 0.4654\n",
            "Epoch 3 Batch 5350 Loss 1.1635 Accuracy 0.4652\n",
            "Epoch 3 Batch 5400 Loss 1.1640 Accuracy 0.4649\n",
            "Epoch 3 Batch 5450 Loss 1.1646 Accuracy 0.4647\n",
            "Epoch 3 Batch 5500 Loss 1.1655 Accuracy 0.4645\n",
            "Epoch 3 Batch 5550 Loss 1.1660 Accuracy 0.4644\n",
            "Epoch 3 Batch 5600 Loss 1.1668 Accuracy 0.4642\n",
            "Epoch 3 Batch 5650 Loss 1.1672 Accuracy 0.4641\n",
            "Epoch 3 Batch 5700 Loss 1.1677 Accuracy 0.4640\n",
            "Saving checkpoint for epoch 3 at ./drive/My Drive/projects/transformer/ckpt/ckpt-3\n",
            "Time taken for 1 epoch: 1475.743899345398 secs\n",
            "\n",
            "Start of epoch 4\n",
            "Epoch 4 Batch 0 Loss 1.2792 Accuracy 0.4778\n",
            "Epoch 4 Batch 50 Loss 1.2181 Accuracy 0.4544\n",
            "Epoch 4 Batch 100 Loss 1.2147 Accuracy 0.4567\n",
            "Epoch 4 Batch 150 Loss 1.2141 Accuracy 0.4567\n",
            "Epoch 4 Batch 200 Loss 1.2157 Accuracy 0.4569\n",
            "Epoch 4 Batch 250 Loss 1.2154 Accuracy 0.4562\n",
            "Epoch 4 Batch 300 Loss 1.2153 Accuracy 0.4558\n",
            "Epoch 4 Batch 350 Loss 1.2101 Accuracy 0.4560\n",
            "Epoch 4 Batch 400 Loss 1.2071 Accuracy 0.4563\n",
            "Epoch 4 Batch 450 Loss 1.2048 Accuracy 0.4562\n",
            "Epoch 4 Batch 500 Loss 1.2007 Accuracy 0.4564\n",
            "Epoch 4 Batch 550 Loss 1.1990 Accuracy 0.4564\n",
            "Epoch 4 Batch 600 Loss 1.1996 Accuracy 0.4568\n",
            "Epoch 4 Batch 650 Loss 1.2012 Accuracy 0.4566\n",
            "Epoch 4 Batch 700 Loss 1.2029 Accuracy 0.4571\n",
            "Epoch 4 Batch 750 Loss 1.2032 Accuracy 0.4573\n",
            "Epoch 4 Batch 800 Loss 1.2023 Accuracy 0.4575\n",
            "Epoch 4 Batch 850 Loss 1.2005 Accuracy 0.4579\n",
            "Epoch 4 Batch 900 Loss 1.1988 Accuracy 0.4579\n",
            "Epoch 4 Batch 950 Loss 1.1957 Accuracy 0.4580\n",
            "Epoch 4 Batch 1000 Loss 1.1932 Accuracy 0.4582\n",
            "Epoch 4 Batch 1050 Loss 1.1910 Accuracy 0.4584\n",
            "Epoch 4 Batch 1100 Loss 1.1898 Accuracy 0.4579\n",
            "Epoch 4 Batch 1150 Loss 1.1889 Accuracy 0.4582\n",
            "Epoch 4 Batch 1200 Loss 1.1871 Accuracy 0.4583\n",
            "Epoch 4 Batch 1250 Loss 1.1851 Accuracy 0.4585\n",
            "Epoch 4 Batch 1300 Loss 1.1828 Accuracy 0.4589\n",
            "Epoch 4 Batch 1350 Loss 1.1800 Accuracy 0.4595\n",
            "Epoch 4 Batch 1400 Loss 1.1788 Accuracy 0.4604\n",
            "Epoch 4 Batch 1450 Loss 1.1763 Accuracy 0.4610\n",
            "Epoch 4 Batch 1500 Loss 1.1735 Accuracy 0.4618\n",
            "Epoch 4 Batch 1550 Loss 1.1716 Accuracy 0.4627\n",
            "Epoch 4 Batch 1600 Loss 1.1695 Accuracy 0.4633\n",
            "Epoch 4 Batch 1650 Loss 1.1665 Accuracy 0.4642\n",
            "Epoch 4 Batch 1700 Loss 1.1639 Accuracy 0.4651\n",
            "Epoch 4 Batch 1750 Loss 1.1609 Accuracy 0.4660\n",
            "Epoch 4 Batch 1800 Loss 1.1584 Accuracy 0.4668\n",
            "Epoch 4 Batch 1850 Loss 1.1555 Accuracy 0.4675\n",
            "Epoch 4 Batch 1900 Loss 1.1533 Accuracy 0.4685\n",
            "Epoch 4 Batch 1950 Loss 1.1509 Accuracy 0.4692\n",
            "Epoch 4 Batch 2000 Loss 1.1493 Accuracy 0.4698\n",
            "Epoch 4 Batch 2050 Loss 1.1467 Accuracy 0.4702\n",
            "Epoch 4 Batch 2100 Loss 1.1447 Accuracy 0.4704\n",
            "Epoch 4 Batch 2150 Loss 1.1418 Accuracy 0.4707\n",
            "Epoch 4 Batch 2200 Loss 1.1383 Accuracy 0.4709\n",
            "Epoch 4 Batch 2250 Loss 1.1352 Accuracy 0.4710\n",
            "Epoch 4 Batch 2300 Loss 1.1319 Accuracy 0.4713\n",
            "Epoch 4 Batch 2350 Loss 1.1287 Accuracy 0.4715\n",
            "Epoch 4 Batch 2400 Loss 1.1263 Accuracy 0.4719\n",
            "Epoch 4 Batch 2450 Loss 1.1232 Accuracy 0.4722\n",
            "Epoch 4 Batch 2500 Loss 1.1205 Accuracy 0.4724\n",
            "Epoch 4 Batch 2550 Loss 1.1176 Accuracy 0.4728\n",
            "Epoch 4 Batch 2600 Loss 1.1142 Accuracy 0.4732\n",
            "Epoch 4 Batch 2650 Loss 1.1119 Accuracy 0.4735\n",
            "Epoch 4 Batch 2700 Loss 1.1090 Accuracy 0.4738\n",
            "Epoch 4 Batch 2750 Loss 1.1063 Accuracy 0.4741\n",
            "Epoch 4 Batch 2800 Loss 1.1037 Accuracy 0.4745\n",
            "Epoch 4 Batch 2850 Loss 1.1013 Accuracy 0.4749\n",
            "Epoch 4 Batch 2900 Loss 1.0989 Accuracy 0.4752\n",
            "Epoch 4 Batch 2950 Loss 1.0962 Accuracy 0.4755\n",
            "Epoch 4 Batch 3000 Loss 1.0939 Accuracy 0.4758\n",
            "Epoch 4 Batch 3050 Loss 1.0920 Accuracy 0.4762\n",
            "Epoch 4 Batch 3100 Loss 1.0897 Accuracy 0.4765\n",
            "Epoch 4 Batch 3150 Loss 1.0877 Accuracy 0.4769\n",
            "Epoch 4 Batch 3200 Loss 1.0855 Accuracy 0.4771\n",
            "Epoch 4 Batch 3250 Loss 1.0831 Accuracy 0.4774\n",
            "Epoch 4 Batch 3300 Loss 1.0809 Accuracy 0.4777\n",
            "Epoch 4 Batch 3350 Loss 1.0788 Accuracy 0.4781\n",
            "Epoch 4 Batch 3400 Loss 1.0766 Accuracy 0.4784\n",
            "Epoch 4 Batch 3450 Loss 1.0748 Accuracy 0.4787\n",
            "Epoch 4 Batch 3500 Loss 1.0726 Accuracy 0.4790\n",
            "Epoch 4 Batch 3550 Loss 1.0703 Accuracy 0.4794\n",
            "Epoch 4 Batch 3600 Loss 1.0688 Accuracy 0.4798\n",
            "Epoch 4 Batch 3650 Loss 1.0670 Accuracy 0.4801\n",
            "Epoch 4 Batch 3700 Loss 1.0650 Accuracy 0.4804\n",
            "Epoch 4 Batch 3750 Loss 1.0631 Accuracy 0.4808\n",
            "Epoch 4 Batch 3800 Loss 1.0615 Accuracy 0.4811\n",
            "Epoch 4 Batch 3850 Loss 1.0600 Accuracy 0.4815\n",
            "Epoch 4 Batch 3900 Loss 1.0582 Accuracy 0.4818\n",
            "Epoch 4 Batch 3950 Loss 1.0567 Accuracy 0.4822\n",
            "Epoch 4 Batch 4000 Loss 1.0552 Accuracy 0.4826\n",
            "Epoch 4 Batch 4050 Loss 1.0536 Accuracy 0.4829\n",
            "Epoch 4 Batch 4100 Loss 1.0524 Accuracy 0.4831\n",
            "Epoch 4 Batch 4150 Loss 1.0519 Accuracy 0.4833\n",
            "Epoch 4 Batch 4200 Loss 1.0520 Accuracy 0.4833\n",
            "Epoch 4 Batch 4250 Loss 1.0523 Accuracy 0.4833\n",
            "Epoch 4 Batch 4300 Loss 1.0530 Accuracy 0.4833\n",
            "Epoch 4 Batch 4350 Loss 1.0537 Accuracy 0.4832\n",
            "Epoch 4 Batch 4400 Loss 1.0545 Accuracy 0.4831\n",
            "Epoch 4 Batch 4450 Loss 1.0555 Accuracy 0.4829\n",
            "Epoch 4 Batch 4500 Loss 1.0567 Accuracy 0.4828\n",
            "Epoch 4 Batch 4550 Loss 1.0577 Accuracy 0.4827\n",
            "Epoch 4 Batch 4600 Loss 1.0590 Accuracy 0.4825\n",
            "Epoch 4 Batch 4650 Loss 1.0604 Accuracy 0.4822\n",
            "Epoch 4 Batch 4700 Loss 1.0614 Accuracy 0.4821\n",
            "Epoch 4 Batch 4750 Loss 1.0624 Accuracy 0.4819\n",
            "Epoch 4 Batch 4800 Loss 1.0633 Accuracy 0.4818\n",
            "Epoch 4 Batch 4850 Loss 1.0645 Accuracy 0.4816\n",
            "Epoch 4 Batch 4900 Loss 1.0656 Accuracy 0.4814\n",
            "Epoch 4 Batch 4950 Loss 1.0666 Accuracy 0.4813\n",
            "Epoch 4 Batch 5000 Loss 1.0678 Accuracy 0.4811\n",
            "Epoch 4 Batch 5050 Loss 1.0689 Accuracy 0.4809\n",
            "Epoch 4 Batch 5100 Loss 1.0702 Accuracy 0.4808\n",
            "Epoch 4 Batch 5150 Loss 1.0716 Accuracy 0.4805\n",
            "Epoch 4 Batch 5200 Loss 1.0727 Accuracy 0.4803\n",
            "Epoch 4 Batch 5250 Loss 1.0737 Accuracy 0.4800\n",
            "Epoch 4 Batch 5300 Loss 1.0746 Accuracy 0.4798\n",
            "Epoch 4 Batch 5350 Loss 1.0754 Accuracy 0.4796\n",
            "Epoch 4 Batch 5400 Loss 1.0761 Accuracy 0.4794\n",
            "Epoch 4 Batch 5450 Loss 1.0770 Accuracy 0.4792\n",
            "Epoch 4 Batch 5500 Loss 1.0777 Accuracy 0.4789\n",
            "Epoch 4 Batch 5550 Loss 1.0784 Accuracy 0.4787\n",
            "Epoch 4 Batch 5600 Loss 1.0791 Accuracy 0.4785\n",
            "Epoch 4 Batch 5650 Loss 1.0798 Accuracy 0.4783\n",
            "Epoch 4 Batch 5700 Loss 1.0805 Accuracy 0.4781\n",
            "Saving checkpoint for epoch 4 at ./drive/My Drive/projects/transformer/ckpt/ckpt-4\n",
            "Time taken for 1 epoch: 1476.0324211120605 secs\n",
            "\n",
            "Start of epoch 5\n",
            "Epoch 5 Batch 0 Loss 1.2179 Accuracy 0.4359\n",
            "Epoch 5 Batch 50 Loss 1.1313 Accuracy 0.4635\n",
            "Epoch 5 Batch 100 Loss 1.1450 Accuracy 0.4645\n",
            "Epoch 5 Batch 150 Loss 1.1590 Accuracy 0.4630\n",
            "Epoch 5 Batch 200 Loss 1.1575 Accuracy 0.4639\n",
            "Epoch 5 Batch 250 Loss 1.1546 Accuracy 0.4653\n",
            "Epoch 5 Batch 300 Loss 1.1526 Accuracy 0.4657\n",
            "Epoch 5 Batch 350 Loss 1.1518 Accuracy 0.4655\n",
            "Epoch 5 Batch 400 Loss 1.1507 Accuracy 0.4657\n",
            "Epoch 5 Batch 450 Loss 1.1472 Accuracy 0.4656\n",
            "Epoch 5 Batch 500 Loss 1.1448 Accuracy 0.4655\n",
            "Epoch 5 Batch 550 Loss 1.1424 Accuracy 0.4656\n",
            "Epoch 5 Batch 600 Loss 1.1417 Accuracy 0.4658\n",
            "Epoch 5 Batch 650 Loss 1.1405 Accuracy 0.4658\n",
            "Epoch 5 Batch 700 Loss 1.1383 Accuracy 0.4665\n",
            "Epoch 5 Batch 750 Loss 1.1372 Accuracy 0.4671\n",
            "Epoch 5 Batch 800 Loss 1.1362 Accuracy 0.4669\n",
            "Epoch 5 Batch 850 Loss 1.1348 Accuracy 0.4674\n",
            "Epoch 5 Batch 900 Loss 1.1345 Accuracy 0.4673\n",
            "Epoch 5 Batch 950 Loss 1.1320 Accuracy 0.4670\n",
            "Epoch 5 Batch 1000 Loss 1.1304 Accuracy 0.4671\n",
            "Epoch 5 Batch 1050 Loss 1.1305 Accuracy 0.4671\n",
            "Epoch 5 Batch 1100 Loss 1.1305 Accuracy 0.4670\n",
            "Epoch 5 Batch 1150 Loss 1.1294 Accuracy 0.4673\n",
            "Epoch 5 Batch 1200 Loss 1.1277 Accuracy 0.4675\n",
            "Epoch 5 Batch 1250 Loss 1.1255 Accuracy 0.4678\n",
            "Epoch 5 Batch 1300 Loss 1.1230 Accuracy 0.4684\n",
            "Epoch 5 Batch 1350 Loss 1.1209 Accuracy 0.4693\n",
            "Epoch 5 Batch 1400 Loss 1.1194 Accuracy 0.4699\n",
            "Epoch 5 Batch 1450 Loss 1.1174 Accuracy 0.4704\n",
            "Epoch 5 Batch 1500 Loss 1.1145 Accuracy 0.4713\n",
            "Epoch 5 Batch 1550 Loss 1.1109 Accuracy 0.4720\n",
            "Epoch 5 Batch 1600 Loss 1.1081 Accuracy 0.4729\n",
            "Epoch 5 Batch 1650 Loss 1.1053 Accuracy 0.4737\n",
            "Epoch 5 Batch 1700 Loss 1.1032 Accuracy 0.4746\n",
            "Epoch 5 Batch 1750 Loss 1.1006 Accuracy 0.4754\n",
            "Epoch 5 Batch 1800 Loss 1.0988 Accuracy 0.4762\n",
            "Epoch 5 Batch 1850 Loss 1.0967 Accuracy 0.4770\n",
            "Epoch 5 Batch 1900 Loss 1.0948 Accuracy 0.4777\n",
            "Epoch 5 Batch 1950 Loss 1.0921 Accuracy 0.4784\n",
            "Epoch 5 Batch 2000 Loss 1.0898 Accuracy 0.4791\n",
            "Epoch 5 Batch 2050 Loss 1.0873 Accuracy 0.4797\n",
            "Epoch 5 Batch 2100 Loss 1.0847 Accuracy 0.4800\n",
            "Epoch 5 Batch 2150 Loss 1.0816 Accuracy 0.4803\n",
            "Epoch 5 Batch 2200 Loss 1.0784 Accuracy 0.4805\n",
            "Epoch 5 Batch 2250 Loss 1.0753 Accuracy 0.4807\n",
            "Epoch 5 Batch 2300 Loss 1.0725 Accuracy 0.4808\n",
            "Epoch 5 Batch 2350 Loss 1.0698 Accuracy 0.4812\n",
            "Epoch 5 Batch 2400 Loss 1.0669 Accuracy 0.4815\n",
            "Epoch 5 Batch 2450 Loss 1.0635 Accuracy 0.4818\n",
            "Epoch 5 Batch 2500 Loss 1.0609 Accuracy 0.4820\n",
            "Epoch 5 Batch 2550 Loss 1.0581 Accuracy 0.4824\n",
            "Epoch 5 Batch 2600 Loss 1.0556 Accuracy 0.4828\n",
            "Epoch 5 Batch 2650 Loss 1.0529 Accuracy 0.4831\n",
            "Epoch 5 Batch 2700 Loss 1.0504 Accuracy 0.4834\n",
            "Epoch 5 Batch 2750 Loss 1.0474 Accuracy 0.4838\n",
            "Epoch 5 Batch 2800 Loss 1.0452 Accuracy 0.4840\n",
            "Epoch 5 Batch 2850 Loss 1.0430 Accuracy 0.4843\n",
            "Epoch 5 Batch 2900 Loss 1.0408 Accuracy 0.4845\n",
            "Epoch 5 Batch 2950 Loss 1.0386 Accuracy 0.4848\n",
            "Epoch 5 Batch 3000 Loss 1.0368 Accuracy 0.4850\n",
            "Epoch 5 Batch 3050 Loss 1.0349 Accuracy 0.4854\n",
            "Epoch 5 Batch 3100 Loss 1.0330 Accuracy 0.4857\n",
            "Epoch 5 Batch 3150 Loss 1.0313 Accuracy 0.4859\n",
            "Epoch 5 Batch 3200 Loss 1.0290 Accuracy 0.4862\n",
            "Epoch 5 Batch 3250 Loss 1.0268 Accuracy 0.4865\n",
            "Epoch 5 Batch 3300 Loss 1.0247 Accuracy 0.4867\n",
            "Epoch 5 Batch 3350 Loss 1.0223 Accuracy 0.4871\n",
            "Epoch 5 Batch 3400 Loss 1.0204 Accuracy 0.4874\n",
            "Epoch 5 Batch 3450 Loss 1.0186 Accuracy 0.4878\n",
            "Epoch 5 Batch 3500 Loss 1.0169 Accuracy 0.4881\n",
            "Epoch 5 Batch 3550 Loss 1.0150 Accuracy 0.4884\n",
            "Epoch 5 Batch 3600 Loss 1.0131 Accuracy 0.4887\n",
            "Epoch 5 Batch 3650 Loss 1.0113 Accuracy 0.4890\n",
            "Epoch 5 Batch 3700 Loss 1.0093 Accuracy 0.4894\n",
            "Epoch 5 Batch 3750 Loss 1.0074 Accuracy 0.4898\n",
            "Epoch 5 Batch 3800 Loss 1.0057 Accuracy 0.4901\n",
            "Epoch 5 Batch 3850 Loss 1.0045 Accuracy 0.4904\n",
            "Epoch 5 Batch 3900 Loss 1.0028 Accuracy 0.4907\n",
            "Epoch 5 Batch 3950 Loss 1.0016 Accuracy 0.4911\n",
            "Epoch 5 Batch 4000 Loss 0.9999 Accuracy 0.4914\n",
            "Epoch 5 Batch 4050 Loss 0.9987 Accuracy 0.4917\n",
            "Epoch 5 Batch 4100 Loss 0.9974 Accuracy 0.4919\n",
            "Epoch 5 Batch 4150 Loss 0.9971 Accuracy 0.4920\n",
            "Epoch 5 Batch 4200 Loss 0.9973 Accuracy 0.4920\n",
            "Epoch 5 Batch 4250 Loss 0.9977 Accuracy 0.4921\n",
            "Epoch 5 Batch 4300 Loss 0.9982 Accuracy 0.4920\n",
            "Epoch 5 Batch 4350 Loss 0.9992 Accuracy 0.4919\n",
            "Epoch 5 Batch 4400 Loss 1.0002 Accuracy 0.4918\n",
            "Epoch 5 Batch 4450 Loss 1.0015 Accuracy 0.4916\n",
            "Epoch 5 Batch 4500 Loss 1.0027 Accuracy 0.4914\n",
            "Epoch 5 Batch 4550 Loss 1.0038 Accuracy 0.4912\n",
            "Epoch 5 Batch 4600 Loss 1.0048 Accuracy 0.4911\n",
            "Epoch 5 Batch 4650 Loss 1.0061 Accuracy 0.4909\n",
            "Epoch 5 Batch 4700 Loss 1.0073 Accuracy 0.4907\n",
            "Epoch 5 Batch 4750 Loss 1.0085 Accuracy 0.4905\n",
            "Epoch 5 Batch 4800 Loss 1.0098 Accuracy 0.4903\n",
            "Epoch 5 Batch 4850 Loss 1.0108 Accuracy 0.4901\n",
            "Epoch 5 Batch 4900 Loss 1.0117 Accuracy 0.4900\n",
            "Epoch 5 Batch 4950 Loss 1.0129 Accuracy 0.4898\n",
            "Epoch 5 Batch 5000 Loss 1.0142 Accuracy 0.4896\n",
            "Epoch 5 Batch 5050 Loss 1.0155 Accuracy 0.4894\n",
            "Epoch 5 Batch 5100 Loss 1.0169 Accuracy 0.4892\n",
            "Epoch 5 Batch 5150 Loss 1.0182 Accuracy 0.4890\n",
            "Epoch 5 Batch 5200 Loss 1.0194 Accuracy 0.4887\n",
            "Epoch 5 Batch 5250 Loss 1.0208 Accuracy 0.4885\n",
            "Epoch 5 Batch 5300 Loss 1.0217 Accuracy 0.4883\n",
            "Epoch 5 Batch 5350 Loss 1.0227 Accuracy 0.4880\n",
            "Epoch 5 Batch 5400 Loss 1.0238 Accuracy 0.4878\n",
            "Epoch 5 Batch 5450 Loss 1.0249 Accuracy 0.4875\n",
            "Epoch 5 Batch 5500 Loss 1.0257 Accuracy 0.4872\n",
            "Epoch 5 Batch 5550 Loss 1.0265 Accuracy 0.4870\n",
            "Epoch 5 Batch 5600 Loss 1.0272 Accuracy 0.4868\n",
            "Epoch 5 Batch 5650 Loss 1.0279 Accuracy 0.4866\n",
            "Epoch 5 Batch 5700 Loss 1.0285 Accuracy 0.4864\n",
            "Saving checkpoint for epoch 5 at ./drive/My Drive/projects/transformer/ckpt/ckpt-5\n",
            "Time taken for 1 epoch: 1452.4816522598267 secs\n",
            "\n",
            "Start of epoch 6\n",
            "Epoch 6 Batch 0 Loss 1.3672 Accuracy 0.4498\n",
            "Epoch 6 Batch 50 Loss 1.1069 Accuracy 0.4732\n",
            "Epoch 6 Batch 100 Loss 1.1192 Accuracy 0.4705\n",
            "Epoch 6 Batch 150 Loss 1.1165 Accuracy 0.4713\n",
            "Epoch 6 Batch 200 Loss 1.1137 Accuracy 0.4713\n",
            "Epoch 6 Batch 250 Loss 1.1117 Accuracy 0.4723\n",
            "Epoch 6 Batch 300 Loss 1.1114 Accuracy 0.4717\n",
            "Epoch 6 Batch 350 Loss 1.1097 Accuracy 0.4718\n",
            "Epoch 6 Batch 400 Loss 1.1081 Accuracy 0.4725\n",
            "Epoch 6 Batch 450 Loss 1.1045 Accuracy 0.4726\n",
            "Epoch 6 Batch 500 Loss 1.1030 Accuracy 0.4725\n",
            "Epoch 6 Batch 550 Loss 1.1017 Accuracy 0.4725\n",
            "Epoch 6 Batch 600 Loss 1.1012 Accuracy 0.4723\n",
            "Epoch 6 Batch 650 Loss 1.0999 Accuracy 0.4723\n",
            "Epoch 6 Batch 700 Loss 1.0989 Accuracy 0.4727\n",
            "Epoch 6 Batch 750 Loss 1.0971 Accuracy 0.4729\n",
            "Epoch 6 Batch 800 Loss 1.0963 Accuracy 0.4736\n",
            "Epoch 6 Batch 850 Loss 1.0950 Accuracy 0.4739\n",
            "Epoch 6 Batch 900 Loss 1.0941 Accuracy 0.4740\n",
            "Epoch 6 Batch 950 Loss 1.0935 Accuracy 0.4740\n",
            "Epoch 6 Batch 1000 Loss 1.0922 Accuracy 0.4740\n",
            "Epoch 6 Batch 1050 Loss 1.0896 Accuracy 0.4739\n",
            "Epoch 6 Batch 1100 Loss 1.0875 Accuracy 0.4743\n",
            "Epoch 6 Batch 1150 Loss 1.0867 Accuracy 0.4747\n",
            "Epoch 6 Batch 1200 Loss 1.0853 Accuracy 0.4748\n",
            "Epoch 6 Batch 1250 Loss 1.0831 Accuracy 0.4750\n",
            "Epoch 6 Batch 1300 Loss 1.0802 Accuracy 0.4753\n",
            "Epoch 6 Batch 1350 Loss 1.0788 Accuracy 0.4759\n",
            "Epoch 6 Batch 1400 Loss 1.0775 Accuracy 0.4763\n",
            "Epoch 6 Batch 1450 Loss 1.0757 Accuracy 0.4770\n",
            "Epoch 6 Batch 1500 Loss 1.0731 Accuracy 0.4778\n",
            "Epoch 6 Batch 1550 Loss 1.0701 Accuracy 0.4786\n",
            "Epoch 6 Batch 1600 Loss 1.0679 Accuracy 0.4793\n",
            "Epoch 6 Batch 1650 Loss 1.0652 Accuracy 0.4802\n",
            "Epoch 6 Batch 1700 Loss 1.0629 Accuracy 0.4812\n",
            "Epoch 6 Batch 1750 Loss 1.0598 Accuracy 0.4821\n",
            "Epoch 6 Batch 1800 Loss 1.0579 Accuracy 0.4827\n",
            "Epoch 6 Batch 1850 Loss 1.0564 Accuracy 0.4836\n",
            "Epoch 6 Batch 1900 Loss 1.0541 Accuracy 0.4843\n",
            "Epoch 6 Batch 1950 Loss 1.0520 Accuracy 0.4851\n",
            "Epoch 6 Batch 2000 Loss 1.0499 Accuracy 0.4857\n",
            "Epoch 6 Batch 2050 Loss 1.0476 Accuracy 0.4862\n",
            "Epoch 6 Batch 2100 Loss 1.0455 Accuracy 0.4865\n",
            "Epoch 6 Batch 2150 Loss 1.0424 Accuracy 0.4868\n",
            "Epoch 6 Batch 2200 Loss 1.0393 Accuracy 0.4870\n",
            "Epoch 6 Batch 2250 Loss 1.0358 Accuracy 0.4872\n",
            "Epoch 6 Batch 2300 Loss 1.0330 Accuracy 0.4874\n",
            "Epoch 6 Batch 2350 Loss 1.0303 Accuracy 0.4877\n",
            "Epoch 6 Batch 2400 Loss 1.0275 Accuracy 0.4879\n",
            "Epoch 6 Batch 2450 Loss 1.0244 Accuracy 0.4882\n",
            "Epoch 6 Batch 2500 Loss 1.0218 Accuracy 0.4885\n",
            "Epoch 6 Batch 2550 Loss 1.0193 Accuracy 0.4889\n",
            "Epoch 6 Batch 2600 Loss 1.0161 Accuracy 0.4891\n",
            "Epoch 6 Batch 2650 Loss 1.0131 Accuracy 0.4896\n",
            "Epoch 6 Batch 2700 Loss 1.0110 Accuracy 0.4899\n",
            "Epoch 6 Batch 2750 Loss 1.0088 Accuracy 0.4902\n",
            "Epoch 6 Batch 2800 Loss 1.0065 Accuracy 0.4904\n",
            "Epoch 6 Batch 2850 Loss 1.0040 Accuracy 0.4907\n",
            "Epoch 6 Batch 2900 Loss 1.0016 Accuracy 0.4909\n",
            "Epoch 6 Batch 2950 Loss 0.9992 Accuracy 0.4912\n",
            "Epoch 6 Batch 3000 Loss 0.9977 Accuracy 0.4916\n",
            "Epoch 6 Batch 3050 Loss 0.9959 Accuracy 0.4919\n",
            "Epoch 6 Batch 3100 Loss 0.9941 Accuracy 0.4922\n",
            "Epoch 6 Batch 3150 Loss 0.9924 Accuracy 0.4925\n",
            "Epoch 6 Batch 3200 Loss 0.9904 Accuracy 0.4927\n",
            "Epoch 6 Batch 3250 Loss 0.9882 Accuracy 0.4929\n",
            "Epoch 6 Batch 3300 Loss 0.9862 Accuracy 0.4931\n",
            "Epoch 6 Batch 3350 Loss 0.9842 Accuracy 0.4934\n",
            "Epoch 6 Batch 3400 Loss 0.9824 Accuracy 0.4937\n",
            "Epoch 6 Batch 3450 Loss 0.9805 Accuracy 0.4941\n",
            "Epoch 6 Batch 3500 Loss 0.9785 Accuracy 0.4943\n",
            "Epoch 6 Batch 3550 Loss 0.9768 Accuracy 0.4946\n",
            "Epoch 6 Batch 3600 Loss 0.9749 Accuracy 0.4949\n",
            "Epoch 6 Batch 3650 Loss 0.9732 Accuracy 0.4952\n",
            "Epoch 6 Batch 3700 Loss 0.9712 Accuracy 0.4955\n",
            "Epoch 6 Batch 3750 Loss 0.9697 Accuracy 0.4959\n",
            "Epoch 6 Batch 3800 Loss 0.9681 Accuracy 0.4963\n",
            "Epoch 6 Batch 3850 Loss 0.9665 Accuracy 0.4966\n",
            "Epoch 6 Batch 3900 Loss 0.9651 Accuracy 0.4969\n",
            "Epoch 6 Batch 3950 Loss 0.9637 Accuracy 0.4972\n",
            "Epoch 6 Batch 4000 Loss 0.9624 Accuracy 0.4976\n",
            "Epoch 6 Batch 4050 Loss 0.9611 Accuracy 0.4979\n",
            "Epoch 6 Batch 4100 Loss 0.9602 Accuracy 0.4980\n",
            "Epoch 6 Batch 4150 Loss 0.9598 Accuracy 0.4982\n",
            "Epoch 6 Batch 4200 Loss 0.9599 Accuracy 0.4982\n",
            "Epoch 6 Batch 4250 Loss 0.9604 Accuracy 0.4982\n",
            "Epoch 6 Batch 4300 Loss 0.9613 Accuracy 0.4982\n",
            "Epoch 6 Batch 4350 Loss 0.9622 Accuracy 0.4980\n",
            "Epoch 6 Batch 4400 Loss 0.9633 Accuracy 0.4979\n",
            "Epoch 6 Batch 4450 Loss 0.9644 Accuracy 0.4978\n",
            "Epoch 6 Batch 4500 Loss 0.9655 Accuracy 0.4976\n",
            "Epoch 6 Batch 4550 Loss 0.9667 Accuracy 0.4974\n",
            "Epoch 6 Batch 4600 Loss 0.9681 Accuracy 0.4972\n",
            "Epoch 6 Batch 4650 Loss 0.9693 Accuracy 0.4970\n",
            "Epoch 6 Batch 4700 Loss 0.9708 Accuracy 0.4968\n",
            "Epoch 6 Batch 4750 Loss 0.9719 Accuracy 0.4966\n",
            "Epoch 6 Batch 4800 Loss 0.9730 Accuracy 0.4965\n",
            "Epoch 6 Batch 4850 Loss 0.9743 Accuracy 0.4963\n",
            "Epoch 6 Batch 4900 Loss 0.9755 Accuracy 0.4962\n",
            "Epoch 6 Batch 4950 Loss 0.9765 Accuracy 0.4959\n",
            "Epoch 6 Batch 5000 Loss 0.9777 Accuracy 0.4957\n",
            "Epoch 6 Batch 5050 Loss 0.9789 Accuracy 0.4955\n",
            "Epoch 6 Batch 5100 Loss 0.9803 Accuracy 0.4952\n",
            "Epoch 6 Batch 5150 Loss 0.9817 Accuracy 0.4950\n",
            "Epoch 6 Batch 5200 Loss 0.9829 Accuracy 0.4947\n",
            "Epoch 6 Batch 5250 Loss 0.9838 Accuracy 0.4945\n",
            "Epoch 6 Batch 5300 Loss 0.9851 Accuracy 0.4942\n",
            "Epoch 6 Batch 5350 Loss 0.9863 Accuracy 0.4940\n",
            "Epoch 6 Batch 5400 Loss 0.9875 Accuracy 0.4937\n",
            "Epoch 6 Batch 5450 Loss 0.9885 Accuracy 0.4934\n",
            "Epoch 6 Batch 5500 Loss 0.9894 Accuracy 0.4931\n",
            "Epoch 6 Batch 5550 Loss 0.9901 Accuracy 0.4929\n",
            "Epoch 6 Batch 5600 Loss 0.9909 Accuracy 0.4927\n",
            "Epoch 6 Batch 5650 Loss 0.9919 Accuracy 0.4925\n",
            "Epoch 6 Batch 5700 Loss 0.9929 Accuracy 0.4922\n",
            "Saving checkpoint for epoch 6 at ./drive/My Drive/projects/transformer/ckpt/ckpt-6\n",
            "Time taken for 1 epoch: 1452.4611160755157 secs\n",
            "\n",
            "Start of epoch 7\n",
            "Epoch 7 Batch 0 Loss 1.1917 Accuracy 0.4844\n",
            "Epoch 7 Batch 50 Loss 1.0975 Accuracy 0.4799\n",
            "Epoch 7 Batch 100 Loss 1.0894 Accuracy 0.4789\n",
            "Epoch 7 Batch 150 Loss 1.0836 Accuracy 0.4796\n",
            "Epoch 7 Batch 200 Loss 1.0753 Accuracy 0.4795\n",
            "Epoch 7 Batch 250 Loss 1.0744 Accuracy 0.4794\n",
            "Epoch 7 Batch 300 Loss 1.0756 Accuracy 0.4783\n",
            "Epoch 7 Batch 350 Loss 1.0757 Accuracy 0.4779\n",
            "Epoch 7 Batch 400 Loss 1.0724 Accuracy 0.4781\n",
            "Epoch 7 Batch 450 Loss 1.0720 Accuracy 0.4778\n",
            "Epoch 7 Batch 500 Loss 1.0696 Accuracy 0.4772\n",
            "Epoch 7 Batch 550 Loss 1.0693 Accuracy 0.4768\n",
            "Epoch 7 Batch 600 Loss 1.0698 Accuracy 0.4765\n",
            "Epoch 7 Batch 650 Loss 1.0686 Accuracy 0.4769\n",
            "Epoch 7 Batch 700 Loss 1.0681 Accuracy 0.4778\n",
            "Epoch 7 Batch 750 Loss 1.0679 Accuracy 0.4781\n",
            "Epoch 7 Batch 800 Loss 1.0674 Accuracy 0.4784\n",
            "Epoch 7 Batch 850 Loss 1.0664 Accuracy 0.4783\n",
            "Epoch 7 Batch 900 Loss 1.0653 Accuracy 0.4784\n",
            "Epoch 7 Batch 950 Loss 1.0633 Accuracy 0.4784\n",
            "Epoch 7 Batch 1000 Loss 1.0618 Accuracy 0.4789\n",
            "Epoch 7 Batch 1050 Loss 1.0606 Accuracy 0.4790\n",
            "Epoch 7 Batch 1100 Loss 1.0595 Accuracy 0.4791\n",
            "Epoch 7 Batch 1150 Loss 1.0572 Accuracy 0.4794\n",
            "Epoch 7 Batch 1200 Loss 1.0560 Accuracy 0.4796\n",
            "Epoch 7 Batch 1250 Loss 1.0535 Accuracy 0.4799\n",
            "Epoch 7 Batch 1300 Loss 1.0515 Accuracy 0.4802\n",
            "Epoch 7 Batch 1350 Loss 1.0505 Accuracy 0.4807\n",
            "Epoch 7 Batch 1400 Loss 1.0482 Accuracy 0.4813\n",
            "Epoch 7 Batch 1450 Loss 1.0464 Accuracy 0.4822\n",
            "Epoch 7 Batch 1500 Loss 1.0434 Accuracy 0.4830\n",
            "Epoch 7 Batch 1550 Loss 1.0410 Accuracy 0.4839\n",
            "Epoch 7 Batch 1600 Loss 1.0385 Accuracy 0.4848\n",
            "Epoch 7 Batch 1650 Loss 1.0367 Accuracy 0.4854\n",
            "Epoch 7 Batch 1700 Loss 1.0345 Accuracy 0.4862\n",
            "Epoch 7 Batch 1750 Loss 1.0319 Accuracy 0.4869\n",
            "Epoch 7 Batch 1800 Loss 1.0293 Accuracy 0.4877\n",
            "Epoch 7 Batch 1850 Loss 1.0273 Accuracy 0.4886\n",
            "Epoch 7 Batch 1900 Loss 1.0254 Accuracy 0.4892\n",
            "Epoch 7 Batch 1950 Loss 1.0230 Accuracy 0.4900\n",
            "Epoch 7 Batch 2000 Loss 1.0211 Accuracy 0.4907\n",
            "Epoch 7 Batch 2050 Loss 1.0192 Accuracy 0.4911\n",
            "Epoch 7 Batch 2100 Loss 1.0166 Accuracy 0.4914\n",
            "Epoch 7 Batch 2150 Loss 1.0138 Accuracy 0.4917\n",
            "Epoch 7 Batch 2200 Loss 1.0104 Accuracy 0.4918\n",
            "Epoch 7 Batch 2250 Loss 1.0077 Accuracy 0.4920\n",
            "Epoch 7 Batch 2300 Loss 1.0047 Accuracy 0.4922\n",
            "Epoch 7 Batch 2350 Loss 1.0018 Accuracy 0.4925\n",
            "Epoch 7 Batch 2400 Loss 0.9987 Accuracy 0.4928\n",
            "Epoch 7 Batch 2450 Loss 0.9961 Accuracy 0.4931\n",
            "Epoch 7 Batch 2500 Loss 0.9931 Accuracy 0.4934\n",
            "Epoch 7 Batch 2550 Loss 0.9904 Accuracy 0.4937\n",
            "Epoch 7 Batch 2600 Loss 0.9873 Accuracy 0.4941\n",
            "Epoch 7 Batch 2650 Loss 0.9846 Accuracy 0.4945\n",
            "Epoch 7 Batch 2700 Loss 0.9820 Accuracy 0.4948\n",
            "Epoch 7 Batch 2750 Loss 0.9799 Accuracy 0.4950\n",
            "Epoch 7 Batch 2800 Loss 0.9780 Accuracy 0.4953\n",
            "Epoch 7 Batch 2850 Loss 0.9756 Accuracy 0.4956\n",
            "Epoch 7 Batch 2900 Loss 0.9734 Accuracy 0.4958\n",
            "Epoch 7 Batch 2950 Loss 0.9714 Accuracy 0.4961\n",
            "Epoch 7 Batch 3000 Loss 0.9695 Accuracy 0.4964\n",
            "Epoch 7 Batch 3050 Loss 0.9675 Accuracy 0.4967\n",
            "Epoch 7 Batch 3100 Loss 0.9658 Accuracy 0.4969\n",
            "Epoch 7 Batch 3150 Loss 0.9638 Accuracy 0.4971\n",
            "Epoch 7 Batch 3200 Loss 0.9621 Accuracy 0.4973\n",
            "Epoch 7 Batch 3250 Loss 0.9600 Accuracy 0.4974\n",
            "Epoch 7 Batch 3300 Loss 0.9582 Accuracy 0.4978\n",
            "Epoch 7 Batch 3350 Loss 0.9562 Accuracy 0.4980\n",
            "Epoch 7 Batch 3400 Loss 0.9543 Accuracy 0.4984\n",
            "Epoch 7 Batch 3450 Loss 0.9523 Accuracy 0.4986\n",
            "Epoch 7 Batch 3500 Loss 0.9504 Accuracy 0.4989\n",
            "Epoch 7 Batch 3550 Loss 0.9489 Accuracy 0.4992\n",
            "Epoch 7 Batch 3600 Loss 0.9469 Accuracy 0.4995\n",
            "Epoch 7 Batch 3650 Loss 0.9454 Accuracy 0.4999\n",
            "Epoch 7 Batch 3700 Loss 0.9439 Accuracy 0.5002\n",
            "Epoch 7 Batch 3750 Loss 0.9424 Accuracy 0.5005\n",
            "Epoch 7 Batch 3800 Loss 0.9409 Accuracy 0.5009\n",
            "Epoch 7 Batch 3850 Loss 0.9397 Accuracy 0.5012\n",
            "Epoch 7 Batch 3900 Loss 0.9382 Accuracy 0.5015\n",
            "Epoch 7 Batch 3950 Loss 0.9369 Accuracy 0.5018\n",
            "Epoch 7 Batch 4000 Loss 0.9357 Accuracy 0.5021\n",
            "Epoch 7 Batch 4050 Loss 0.9345 Accuracy 0.5025\n",
            "Epoch 7 Batch 4100 Loss 0.9334 Accuracy 0.5027\n",
            "Epoch 7 Batch 4150 Loss 0.9329 Accuracy 0.5029\n",
            "Epoch 7 Batch 4200 Loss 0.9332 Accuracy 0.5028\n",
            "Epoch 7 Batch 4250 Loss 0.9337 Accuracy 0.5028\n",
            "Epoch 7 Batch 4300 Loss 0.9342 Accuracy 0.5027\n",
            "Epoch 7 Batch 4350 Loss 0.9351 Accuracy 0.5025\n",
            "Epoch 7 Batch 4400 Loss 0.9362 Accuracy 0.5024\n",
            "Epoch 7 Batch 4450 Loss 0.9374 Accuracy 0.5023\n",
            "Epoch 7 Batch 4500 Loss 0.9383 Accuracy 0.5021\n",
            "Epoch 7 Batch 4550 Loss 0.9400 Accuracy 0.5019\n",
            "Epoch 7 Batch 4600 Loss 0.9413 Accuracy 0.5017\n",
            "Epoch 7 Batch 4650 Loss 0.9426 Accuracy 0.5015\n",
            "Epoch 7 Batch 4700 Loss 0.9439 Accuracy 0.5013\n",
            "Epoch 7 Batch 4750 Loss 0.9453 Accuracy 0.5011\n",
            "Epoch 7 Batch 4800 Loss 0.9463 Accuracy 0.5009\n",
            "Epoch 7 Batch 4850 Loss 0.9474 Accuracy 0.5008\n",
            "Epoch 7 Batch 4900 Loss 0.9489 Accuracy 0.5006\n",
            "Epoch 7 Batch 4950 Loss 0.9499 Accuracy 0.5005\n",
            "Epoch 7 Batch 5000 Loss 0.9513 Accuracy 0.5002\n",
            "Epoch 7 Batch 5050 Loss 0.9529 Accuracy 0.5000\n",
            "Epoch 7 Batch 5100 Loss 0.9540 Accuracy 0.4998\n",
            "Epoch 7 Batch 5150 Loss 0.9555 Accuracy 0.4995\n",
            "Epoch 7 Batch 5200 Loss 0.9566 Accuracy 0.4993\n",
            "Epoch 7 Batch 5250 Loss 0.9577 Accuracy 0.4991\n",
            "Epoch 7 Batch 5300 Loss 0.9587 Accuracy 0.4988\n",
            "Epoch 7 Batch 5350 Loss 0.9600 Accuracy 0.4984\n",
            "Epoch 7 Batch 5400 Loss 0.9610 Accuracy 0.4981\n",
            "Epoch 7 Batch 5450 Loss 0.9620 Accuracy 0.4979\n",
            "Epoch 7 Batch 5500 Loss 0.9630 Accuracy 0.4976\n",
            "Epoch 7 Batch 5550 Loss 0.9640 Accuracy 0.4974\n",
            "Epoch 7 Batch 5600 Loss 0.9650 Accuracy 0.4971\n",
            "Epoch 7 Batch 5650 Loss 0.9659 Accuracy 0.4968\n",
            "Epoch 7 Batch 5700 Loss 0.9667 Accuracy 0.4966\n",
            "Saving checkpoint for epoch 7 at ./drive/My Drive/projects/transformer/ckpt/ckpt-7\n",
            "Time taken for 1 epoch: 1447.543218612671 secs\n",
            "\n",
            "Start of epoch 8\n",
            "Epoch 8 Batch 0 Loss 1.1229 Accuracy 0.4663\n",
            "Epoch 8 Batch 50 Loss 1.0758 Accuracy 0.4815\n",
            "Epoch 8 Batch 100 Loss 1.0576 Accuracy 0.4809\n",
            "Epoch 8 Batch 150 Loss 1.0558 Accuracy 0.4808\n",
            "Epoch 8 Batch 200 Loss 1.0569 Accuracy 0.4813\n",
            "Epoch 8 Batch 250 Loss 1.0537 Accuracy 0.4823\n",
            "Epoch 8 Batch 300 Loss 1.0553 Accuracy 0.4818\n",
            "Epoch 8 Batch 350 Loss 1.0574 Accuracy 0.4808\n",
            "Epoch 8 Batch 400 Loss 1.0527 Accuracy 0.4816\n",
            "Epoch 8 Batch 450 Loss 1.0499 Accuracy 0.4819\n",
            "Epoch 8 Batch 500 Loss 1.0483 Accuracy 0.4815\n",
            "Epoch 8 Batch 550 Loss 1.0470 Accuracy 0.4815\n",
            "Epoch 8 Batch 600 Loss 1.0457 Accuracy 0.4816\n",
            "Epoch 8 Batch 650 Loss 1.0468 Accuracy 0.4817\n",
            "Epoch 8 Batch 700 Loss 1.0476 Accuracy 0.4818\n",
            "Epoch 8 Batch 750 Loss 1.0469 Accuracy 0.4823\n",
            "Epoch 8 Batch 800 Loss 1.0464 Accuracy 0.4824\n",
            "Epoch 8 Batch 850 Loss 1.0456 Accuracy 0.4826\n",
            "Epoch 8 Batch 900 Loss 1.0440 Accuracy 0.4825\n",
            "Epoch 8 Batch 950 Loss 1.0429 Accuracy 0.4823\n",
            "Epoch 8 Batch 1000 Loss 1.0405 Accuracy 0.4823\n",
            "Epoch 8 Batch 1050 Loss 1.0394 Accuracy 0.4827\n",
            "Epoch 8 Batch 1100 Loss 1.0377 Accuracy 0.4827\n",
            "Epoch 8 Batch 1150 Loss 1.0364 Accuracy 0.4830\n",
            "Epoch 8 Batch 1200 Loss 1.0341 Accuracy 0.4833\n",
            "Epoch 8 Batch 1250 Loss 1.0324 Accuracy 0.4836\n",
            "Epoch 8 Batch 1300 Loss 1.0303 Accuracy 0.4842\n",
            "Epoch 8 Batch 1350 Loss 1.0272 Accuracy 0.4847\n",
            "Epoch 8 Batch 1400 Loss 1.0246 Accuracy 0.4854\n",
            "Epoch 8 Batch 1450 Loss 1.0218 Accuracy 0.4862\n",
            "Epoch 8 Batch 1500 Loss 1.0196 Accuracy 0.4870\n",
            "Epoch 8 Batch 1550 Loss 1.0172 Accuracy 0.4878\n",
            "Epoch 8 Batch 1600 Loss 1.0156 Accuracy 0.4885\n",
            "Epoch 8 Batch 1650 Loss 1.0132 Accuracy 0.4894\n",
            "Epoch 8 Batch 1700 Loss 1.0106 Accuracy 0.4900\n",
            "Epoch 8 Batch 1750 Loss 1.0080 Accuracy 0.4906\n",
            "Epoch 8 Batch 1800 Loss 1.0063 Accuracy 0.4914\n",
            "Epoch 8 Batch 1850 Loss 1.0035 Accuracy 0.4922\n",
            "Epoch 8 Batch 1900 Loss 1.0016 Accuracy 0.4930\n",
            "Epoch 8 Batch 1950 Loss 0.9995 Accuracy 0.4937\n",
            "Epoch 8 Batch 2000 Loss 0.9976 Accuracy 0.4942\n",
            "Epoch 8 Batch 2050 Loss 0.9952 Accuracy 0.4948\n",
            "Epoch 8 Batch 2100 Loss 0.9934 Accuracy 0.4951\n",
            "Epoch 8 Batch 2150 Loss 0.9912 Accuracy 0.4954\n",
            "Epoch 8 Batch 2200 Loss 0.9882 Accuracy 0.4958\n",
            "Epoch 8 Batch 2250 Loss 0.9853 Accuracy 0.4960\n",
            "Epoch 8 Batch 2300 Loss 0.9820 Accuracy 0.4963\n",
            "Epoch 8 Batch 2350 Loss 0.9793 Accuracy 0.4965\n",
            "Epoch 8 Batch 2400 Loss 0.9766 Accuracy 0.4968\n",
            "Epoch 8 Batch 2450 Loss 0.9742 Accuracy 0.4970\n",
            "Epoch 8 Batch 2500 Loss 0.9712 Accuracy 0.4973\n",
            "Epoch 8 Batch 2550 Loss 0.9681 Accuracy 0.4976\n",
            "Epoch 8 Batch 2600 Loss 0.9651 Accuracy 0.4979\n",
            "Epoch 8 Batch 2650 Loss 0.9621 Accuracy 0.4983\n",
            "Epoch 8 Batch 2700 Loss 0.9599 Accuracy 0.4985\n",
            "Epoch 8 Batch 2750 Loss 0.9575 Accuracy 0.4988\n",
            "Epoch 8 Batch 2800 Loss 0.9553 Accuracy 0.4990\n",
            "Epoch 8 Batch 2850 Loss 0.9536 Accuracy 0.4993\n",
            "Epoch 8 Batch 2900 Loss 0.9514 Accuracy 0.4997\n",
            "Epoch 8 Batch 2950 Loss 0.9496 Accuracy 0.5000\n",
            "Epoch 8 Batch 3000 Loss 0.9479 Accuracy 0.5003\n",
            "Epoch 8 Batch 3050 Loss 0.9463 Accuracy 0.5005\n",
            "Epoch 8 Batch 3100 Loss 0.9441 Accuracy 0.5007\n",
            "Epoch 8 Batch 3150 Loss 0.9423 Accuracy 0.5009\n",
            "Epoch 8 Batch 3200 Loss 0.9405 Accuracy 0.5011\n",
            "Epoch 8 Batch 3250 Loss 0.9388 Accuracy 0.5014\n",
            "Epoch 8 Batch 3300 Loss 0.9370 Accuracy 0.5017\n",
            "Epoch 8 Batch 3350 Loss 0.9349 Accuracy 0.5020\n",
            "Epoch 8 Batch 3400 Loss 0.9328 Accuracy 0.5022\n",
            "Epoch 8 Batch 3450 Loss 0.9313 Accuracy 0.5024\n",
            "Epoch 8 Batch 3500 Loss 0.9292 Accuracy 0.5028\n",
            "Epoch 8 Batch 3550 Loss 0.9271 Accuracy 0.5031\n",
            "Epoch 8 Batch 3600 Loss 0.9255 Accuracy 0.5033\n",
            "Epoch 8 Batch 3650 Loss 0.9239 Accuracy 0.5036\n",
            "Epoch 8 Batch 3700 Loss 0.9224 Accuracy 0.5039\n",
            "Epoch 8 Batch 3750 Loss 0.9206 Accuracy 0.5042\n",
            "Epoch 8 Batch 3800 Loss 0.9190 Accuracy 0.5046\n",
            "Epoch 8 Batch 3850 Loss 0.9177 Accuracy 0.5049\n",
            "Epoch 8 Batch 3900 Loss 0.9165 Accuracy 0.5052\n",
            "Epoch 8 Batch 3950 Loss 0.9152 Accuracy 0.5056\n",
            "Epoch 8 Batch 4000 Loss 0.9139 Accuracy 0.5059\n",
            "Epoch 8 Batch 4050 Loss 0.9128 Accuracy 0.5062\n",
            "Epoch 8 Batch 4100 Loss 0.9118 Accuracy 0.5064\n",
            "Epoch 8 Batch 4150 Loss 0.9113 Accuracy 0.5065\n",
            "Epoch 8 Batch 4200 Loss 0.9116 Accuracy 0.5065\n",
            "Epoch 8 Batch 4250 Loss 0.9118 Accuracy 0.5065\n",
            "Epoch 8 Batch 4300 Loss 0.9124 Accuracy 0.5064\n",
            "Epoch 8 Batch 4350 Loss 0.9136 Accuracy 0.5063\n",
            "Epoch 8 Batch 4400 Loss 0.9146 Accuracy 0.5061\n",
            "Epoch 8 Batch 4450 Loss 0.9158 Accuracy 0.5060\n",
            "Epoch 8 Batch 4500 Loss 0.9169 Accuracy 0.5058\n",
            "Epoch 8 Batch 4550 Loss 0.9182 Accuracy 0.5056\n",
            "Epoch 8 Batch 4600 Loss 0.9196 Accuracy 0.5054\n",
            "Epoch 8 Batch 4650 Loss 0.9212 Accuracy 0.5052\n",
            "Epoch 8 Batch 4700 Loss 0.9225 Accuracy 0.5050\n",
            "Epoch 8 Batch 4750 Loss 0.9237 Accuracy 0.5048\n",
            "Epoch 8 Batch 4800 Loss 0.9250 Accuracy 0.5046\n",
            "Epoch 8 Batch 4850 Loss 0.9265 Accuracy 0.5044\n",
            "Epoch 8 Batch 4900 Loss 0.9279 Accuracy 0.5042\n",
            "Epoch 8 Batch 4950 Loss 0.9291 Accuracy 0.5041\n",
            "Epoch 8 Batch 5000 Loss 0.9304 Accuracy 0.5039\n",
            "Epoch 8 Batch 5050 Loss 0.9317 Accuracy 0.5036\n",
            "Epoch 8 Batch 5100 Loss 0.9332 Accuracy 0.5034\n",
            "Epoch 8 Batch 5150 Loss 0.9343 Accuracy 0.5032\n",
            "Epoch 8 Batch 5200 Loss 0.9355 Accuracy 0.5030\n",
            "Epoch 8 Batch 5250 Loss 0.9368 Accuracy 0.5026\n",
            "Epoch 8 Batch 5300 Loss 0.9378 Accuracy 0.5023\n",
            "Epoch 8 Batch 5350 Loss 0.9389 Accuracy 0.5020\n",
            "Epoch 8 Batch 5400 Loss 0.9398 Accuracy 0.5018\n",
            "Epoch 8 Batch 5450 Loss 0.9407 Accuracy 0.5015\n",
            "Epoch 8 Batch 5500 Loss 0.9419 Accuracy 0.5012\n",
            "Epoch 8 Batch 5550 Loss 0.9428 Accuracy 0.5010\n",
            "Epoch 8 Batch 5600 Loss 0.9437 Accuracy 0.5007\n",
            "Epoch 8 Batch 5650 Loss 0.9446 Accuracy 0.5005\n",
            "Epoch 8 Batch 5700 Loss 0.9455 Accuracy 0.5003\n",
            "Saving checkpoint for epoch 8 at ./drive/My Drive/projects/transformer/ckpt/ckpt-8\n",
            "Time taken for 1 epoch: 1438.103768825531 secs\n",
            "\n",
            "Start of epoch 9\n",
            "Epoch 9 Batch 0 Loss 1.1412 Accuracy 0.4712\n",
            "Epoch 9 Batch 50 Loss 1.0252 Accuracy 0.4906\n",
            "Epoch 9 Batch 100 Loss 1.0298 Accuracy 0.4876\n",
            "Epoch 9 Batch 150 Loss 1.0337 Accuracy 0.4862\n",
            "Epoch 9 Batch 200 Loss 1.0366 Accuracy 0.4864\n",
            "Epoch 9 Batch 250 Loss 1.0347 Accuracy 0.4864\n",
            "Epoch 9 Batch 300 Loss 1.0366 Accuracy 0.4852\n",
            "Epoch 9 Batch 350 Loss 1.0385 Accuracy 0.4849\n",
            "Epoch 9 Batch 400 Loss 1.0360 Accuracy 0.4842\n",
            "Epoch 9 Batch 450 Loss 1.0352 Accuracy 0.4849\n",
            "Epoch 9 Batch 500 Loss 1.0313 Accuracy 0.4853\n",
            "Epoch 9 Batch 550 Loss 1.0326 Accuracy 0.4848\n",
            "Epoch 9 Batch 600 Loss 1.0296 Accuracy 0.4852\n",
            "Epoch 9 Batch 650 Loss 1.0283 Accuracy 0.4854\n",
            "Epoch 9 Batch 700 Loss 1.0280 Accuracy 0.4858\n",
            "Epoch 9 Batch 750 Loss 1.0265 Accuracy 0.4859\n",
            "Epoch 9 Batch 800 Loss 1.0256 Accuracy 0.4861\n",
            "Epoch 9 Batch 850 Loss 1.0242 Accuracy 0.4861\n",
            "Epoch 9 Batch 900 Loss 1.0226 Accuracy 0.4864\n",
            "Epoch 9 Batch 950 Loss 1.0209 Accuracy 0.4864\n",
            "Epoch 9 Batch 1000 Loss 1.0178 Accuracy 0.4862\n",
            "Epoch 9 Batch 1050 Loss 1.0177 Accuracy 0.4862\n",
            "Epoch 9 Batch 1100 Loss 1.0167 Accuracy 0.4864\n",
            "Epoch 9 Batch 1150 Loss 1.0150 Accuracy 0.4865\n",
            "Epoch 9 Batch 1200 Loss 1.0141 Accuracy 0.4865\n",
            "Epoch 9 Batch 1250 Loss 1.0131 Accuracy 0.4868\n",
            "Epoch 9 Batch 1300 Loss 1.0102 Accuracy 0.4872\n",
            "Epoch 9 Batch 1350 Loss 1.0080 Accuracy 0.4879\n",
            "Epoch 9 Batch 1400 Loss 1.0059 Accuracy 0.4888\n",
            "Epoch 9 Batch 1450 Loss 1.0031 Accuracy 0.4895\n",
            "Epoch 9 Batch 1500 Loss 1.0004 Accuracy 0.4901\n",
            "Epoch 9 Batch 1550 Loss 0.9979 Accuracy 0.4909\n",
            "Epoch 9 Batch 1600 Loss 0.9957 Accuracy 0.4917\n",
            "Epoch 9 Batch 1650 Loss 0.9941 Accuracy 0.4926\n",
            "Epoch 9 Batch 1700 Loss 0.9920 Accuracy 0.4935\n",
            "Epoch 9 Batch 1750 Loss 0.9893 Accuracy 0.4943\n",
            "Epoch 9 Batch 1800 Loss 0.9870 Accuracy 0.4951\n",
            "Epoch 9 Batch 1850 Loss 0.9844 Accuracy 0.4959\n",
            "Epoch 9 Batch 1900 Loss 0.9821 Accuracy 0.4966\n",
            "Epoch 9 Batch 1950 Loss 0.9800 Accuracy 0.4973\n",
            "Epoch 9 Batch 2000 Loss 0.9781 Accuracy 0.4979\n",
            "Epoch 9 Batch 2050 Loss 0.9762 Accuracy 0.4984\n",
            "Epoch 9 Batch 2100 Loss 0.9742 Accuracy 0.4986\n",
            "Epoch 9 Batch 2150 Loss 0.9714 Accuracy 0.4990\n",
            "Epoch 9 Batch 2200 Loss 0.9686 Accuracy 0.4991\n",
            "Epoch 9 Batch 2250 Loss 0.9655 Accuracy 0.4995\n",
            "Epoch 9 Batch 2300 Loss 0.9632 Accuracy 0.4996\n",
            "Epoch 9 Batch 2350 Loss 0.9602 Accuracy 0.4999\n",
            "Epoch 9 Batch 2400 Loss 0.9571 Accuracy 0.5001\n",
            "Epoch 9 Batch 2450 Loss 0.9546 Accuracy 0.5002\n",
            "Epoch 9 Batch 2500 Loss 0.9516 Accuracy 0.5005\n",
            "Epoch 9 Batch 2550 Loss 0.9489 Accuracy 0.5008\n",
            "Epoch 9 Batch 2600 Loss 0.9464 Accuracy 0.5012\n",
            "Epoch 9 Batch 2650 Loss 0.9435 Accuracy 0.5015\n",
            "Epoch 9 Batch 2700 Loss 0.9407 Accuracy 0.5019\n",
            "Epoch 9 Batch 2750 Loss 0.9383 Accuracy 0.5022\n",
            "Epoch 9 Batch 2800 Loss 0.9360 Accuracy 0.5023\n",
            "Epoch 9 Batch 2850 Loss 0.9339 Accuracy 0.5026\n",
            "Epoch 9 Batch 2900 Loss 0.9321 Accuracy 0.5028\n",
            "Epoch 9 Batch 2950 Loss 0.9304 Accuracy 0.5031\n",
            "Epoch 9 Batch 3000 Loss 0.9286 Accuracy 0.5033\n",
            "Epoch 9 Batch 3050 Loss 0.9266 Accuracy 0.5036\n",
            "Epoch 9 Batch 3100 Loss 0.9248 Accuracy 0.5038\n",
            "Epoch 9 Batch 3150 Loss 0.9231 Accuracy 0.5040\n",
            "Epoch 9 Batch 3200 Loss 0.9214 Accuracy 0.5042\n",
            "Epoch 9 Batch 3250 Loss 0.9197 Accuracy 0.5045\n",
            "Epoch 9 Batch 3300 Loss 0.9178 Accuracy 0.5047\n",
            "Epoch 9 Batch 3350 Loss 0.9157 Accuracy 0.5050\n",
            "Epoch 9 Batch 3400 Loss 0.9138 Accuracy 0.5053\n",
            "Epoch 9 Batch 3450 Loss 0.9118 Accuracy 0.5056\n",
            "Epoch 9 Batch 3500 Loss 0.9101 Accuracy 0.5059\n",
            "Epoch 9 Batch 3550 Loss 0.9085 Accuracy 0.5062\n",
            "Epoch 9 Batch 3600 Loss 0.9065 Accuracy 0.5065\n",
            "Epoch 9 Batch 3650 Loss 0.9051 Accuracy 0.5068\n",
            "Epoch 9 Batch 3700 Loss 0.9037 Accuracy 0.5071\n",
            "Epoch 9 Batch 3750 Loss 0.9025 Accuracy 0.5074\n",
            "Epoch 9 Batch 3800 Loss 0.9010 Accuracy 0.5078\n",
            "Epoch 9 Batch 3850 Loss 0.8993 Accuracy 0.5080\n",
            "Epoch 9 Batch 3900 Loss 0.8981 Accuracy 0.5083\n",
            "Epoch 9 Batch 3950 Loss 0.8968 Accuracy 0.5085\n",
            "Epoch 9 Batch 4000 Loss 0.8955 Accuracy 0.5088\n",
            "Epoch 9 Batch 4050 Loss 0.8941 Accuracy 0.5091\n",
            "Epoch 9 Batch 4100 Loss 0.8933 Accuracy 0.5093\n",
            "Epoch 9 Batch 4150 Loss 0.8932 Accuracy 0.5094\n",
            "Epoch 9 Batch 4200 Loss 0.8934 Accuracy 0.5094\n",
            "Epoch 9 Batch 4250 Loss 0.8938 Accuracy 0.5094\n",
            "Epoch 9 Batch 4300 Loss 0.8945 Accuracy 0.5093\n",
            "Epoch 9 Batch 4350 Loss 0.8957 Accuracy 0.5092\n",
            "Epoch 9 Batch 4400 Loss 0.8967 Accuracy 0.5090\n",
            "Epoch 9 Batch 4450 Loss 0.8980 Accuracy 0.5088\n",
            "Epoch 9 Batch 4500 Loss 0.8993 Accuracy 0.5086\n",
            "Epoch 9 Batch 4550 Loss 0.9007 Accuracy 0.5085\n",
            "Epoch 9 Batch 4600 Loss 0.9019 Accuracy 0.5083\n",
            "Epoch 9 Batch 4650 Loss 0.9031 Accuracy 0.5081\n",
            "Epoch 9 Batch 4700 Loss 0.9046 Accuracy 0.5079\n",
            "Epoch 9 Batch 4750 Loss 0.9057 Accuracy 0.5077\n",
            "Epoch 9 Batch 4800 Loss 0.9069 Accuracy 0.5076\n",
            "Epoch 9 Batch 4850 Loss 0.9081 Accuracy 0.5074\n",
            "Epoch 9 Batch 4900 Loss 0.9095 Accuracy 0.5072\n",
            "Epoch 9 Batch 4950 Loss 0.9108 Accuracy 0.5070\n",
            "Epoch 9 Batch 5000 Loss 0.9123 Accuracy 0.5068\n",
            "Epoch 9 Batch 5050 Loss 0.9137 Accuracy 0.5066\n",
            "Epoch 9 Batch 5100 Loss 0.9150 Accuracy 0.5064\n",
            "Epoch 9 Batch 5150 Loss 0.9162 Accuracy 0.5061\n",
            "Epoch 9 Batch 5200 Loss 0.9175 Accuracy 0.5058\n",
            "Epoch 9 Batch 5250 Loss 0.9188 Accuracy 0.5055\n",
            "Epoch 9 Batch 5300 Loss 0.9200 Accuracy 0.5052\n",
            "Epoch 9 Batch 5350 Loss 0.9211 Accuracy 0.5050\n",
            "Epoch 9 Batch 5400 Loss 0.9222 Accuracy 0.5047\n",
            "Epoch 9 Batch 5450 Loss 0.9233 Accuracy 0.5045\n",
            "Epoch 9 Batch 5500 Loss 0.9244 Accuracy 0.5042\n",
            "Epoch 9 Batch 5550 Loss 0.9252 Accuracy 0.5039\n",
            "Epoch 9 Batch 5600 Loss 0.9260 Accuracy 0.5037\n",
            "Epoch 9 Batch 5650 Loss 0.9271 Accuracy 0.5034\n",
            "Epoch 9 Batch 5700 Loss 0.9282 Accuracy 0.5031\n",
            "Saving checkpoint for epoch 9 at ./drive/My Drive/projects/transformer/ckpt/ckpt-9\n",
            "Time taken for 1 epoch: 1440.5985443592072 secs\n",
            "\n",
            "Start of epoch 10\n",
            "Epoch 10 Batch 0 Loss 1.1208 Accuracy 0.4910\n",
            "Epoch 10 Batch 50 Loss 1.0322 Accuracy 0.4850\n",
            "Epoch 10 Batch 100 Loss 1.0233 Accuracy 0.4881\n",
            "Epoch 10 Batch 150 Loss 1.0242 Accuracy 0.4886\n",
            "Epoch 10 Batch 200 Loss 1.0219 Accuracy 0.4879\n",
            "Epoch 10 Batch 250 Loss 1.0244 Accuracy 0.4879\n",
            "Epoch 10 Batch 300 Loss 1.0229 Accuracy 0.4869\n",
            "Epoch 10 Batch 350 Loss 1.0208 Accuracy 0.4866\n",
            "Epoch 10 Batch 400 Loss 1.0213 Accuracy 0.4858\n",
            "Epoch 10 Batch 450 Loss 1.0184 Accuracy 0.4854\n",
            "Epoch 10 Batch 500 Loss 1.0123 Accuracy 0.4856\n",
            "Epoch 10 Batch 550 Loss 1.0108 Accuracy 0.4859\n",
            "Epoch 10 Batch 600 Loss 1.0087 Accuracy 0.4864\n",
            "Epoch 10 Batch 650 Loss 1.0099 Accuracy 0.4864\n",
            "Epoch 10 Batch 700 Loss 1.0102 Accuracy 0.4869\n",
            "Epoch 10 Batch 750 Loss 1.0098 Accuracy 0.4871\n",
            "Epoch 10 Batch 800 Loss 1.0092 Accuracy 0.4872\n",
            "Epoch 10 Batch 850 Loss 1.0098 Accuracy 0.4875\n",
            "Epoch 10 Batch 900 Loss 1.0092 Accuracy 0.4874\n",
            "Epoch 10 Batch 950 Loss 1.0081 Accuracy 0.4873\n",
            "Epoch 10 Batch 1000 Loss 1.0065 Accuracy 0.4877\n",
            "Epoch 10 Batch 1050 Loss 1.0047 Accuracy 0.4879\n",
            "Epoch 10 Batch 1100 Loss 1.0029 Accuracy 0.4883\n",
            "Epoch 10 Batch 1150 Loss 1.0021 Accuracy 0.4885\n",
            "Epoch 10 Batch 1200 Loss 1.0002 Accuracy 0.4888\n",
            "Epoch 10 Batch 1250 Loss 0.9985 Accuracy 0.4890\n",
            "Epoch 10 Batch 1300 Loss 0.9964 Accuracy 0.4895\n",
            "Epoch 10 Batch 1350 Loss 0.9944 Accuracy 0.4903\n",
            "Epoch 10 Batch 1400 Loss 0.9921 Accuracy 0.4909\n",
            "Epoch 10 Batch 1450 Loss 0.9894 Accuracy 0.4914\n",
            "Epoch 10 Batch 1500 Loss 0.9870 Accuracy 0.4923\n",
            "Epoch 10 Batch 1550 Loss 0.9838 Accuracy 0.4931\n",
            "Epoch 10 Batch 1600 Loss 0.9814 Accuracy 0.4940\n",
            "Epoch 10 Batch 1650 Loss 0.9795 Accuracy 0.4947\n",
            "Epoch 10 Batch 1700 Loss 0.9767 Accuracy 0.4956\n",
            "Epoch 10 Batch 1750 Loss 0.9747 Accuracy 0.4964\n",
            "Epoch 10 Batch 1800 Loss 0.9723 Accuracy 0.4971\n",
            "Epoch 10 Batch 1850 Loss 0.9701 Accuracy 0.4979\n",
            "Epoch 10 Batch 1900 Loss 0.9677 Accuracy 0.4986\n",
            "Epoch 10 Batch 1950 Loss 0.9657 Accuracy 0.4993\n",
            "Epoch 10 Batch 2000 Loss 0.9638 Accuracy 0.4999\n",
            "Epoch 10 Batch 2050 Loss 0.9618 Accuracy 0.5004\n",
            "Epoch 10 Batch 2100 Loss 0.9595 Accuracy 0.5006\n",
            "Epoch 10 Batch 2150 Loss 0.9571 Accuracy 0.5009\n",
            "Epoch 10 Batch 2200 Loss 0.9544 Accuracy 0.5011\n",
            "Epoch 10 Batch 2250 Loss 0.9511 Accuracy 0.5013\n",
            "Epoch 10 Batch 2300 Loss 0.9485 Accuracy 0.5015\n",
            "Epoch 10 Batch 2350 Loss 0.9454 Accuracy 0.5018\n",
            "Epoch 10 Batch 2400 Loss 0.9427 Accuracy 0.5020\n",
            "Epoch 10 Batch 2450 Loss 0.9398 Accuracy 0.5022\n",
            "Epoch 10 Batch 2500 Loss 0.9369 Accuracy 0.5025\n",
            "Epoch 10 Batch 2550 Loss 0.9345 Accuracy 0.5029\n",
            "Epoch 10 Batch 2600 Loss 0.9317 Accuracy 0.5032\n",
            "Epoch 10 Batch 2650 Loss 0.9294 Accuracy 0.5036\n",
            "Epoch 10 Batch 2700 Loss 0.9268 Accuracy 0.5039\n",
            "Epoch 10 Batch 2750 Loss 0.9247 Accuracy 0.5041\n",
            "Epoch 10 Batch 2800 Loss 0.9223 Accuracy 0.5044\n",
            "Epoch 10 Batch 2850 Loss 0.9203 Accuracy 0.5047\n",
            "Epoch 10 Batch 2900 Loss 0.9182 Accuracy 0.5050\n",
            "Epoch 10 Batch 2950 Loss 0.9161 Accuracy 0.5053\n",
            "Epoch 10 Batch 3000 Loss 0.9145 Accuracy 0.5056\n",
            "Epoch 10 Batch 3050 Loss 0.9127 Accuracy 0.5058\n",
            "Epoch 10 Batch 3100 Loss 0.9111 Accuracy 0.5061\n",
            "Epoch 10 Batch 3150 Loss 0.9095 Accuracy 0.5063\n",
            "Epoch 10 Batch 3200 Loss 0.9075 Accuracy 0.5065\n",
            "Epoch 10 Batch 3250 Loss 0.9058 Accuracy 0.5068\n",
            "Epoch 10 Batch 3300 Loss 0.9041 Accuracy 0.5070\n",
            "Epoch 10 Batch 3350 Loss 0.9022 Accuracy 0.5072\n",
            "Epoch 10 Batch 3400 Loss 0.9000 Accuracy 0.5075\n",
            "Epoch 10 Batch 3450 Loss 0.8981 Accuracy 0.5078\n",
            "Epoch 10 Batch 3500 Loss 0.8963 Accuracy 0.5080\n",
            "Epoch 10 Batch 3550 Loss 0.8949 Accuracy 0.5083\n",
            "Epoch 10 Batch 3600 Loss 0.8932 Accuracy 0.5087\n",
            "Epoch 10 Batch 3650 Loss 0.8919 Accuracy 0.5090\n",
            "Epoch 10 Batch 3700 Loss 0.8902 Accuracy 0.5093\n",
            "Epoch 10 Batch 3750 Loss 0.8886 Accuracy 0.5097\n",
            "Epoch 10 Batch 3800 Loss 0.8870 Accuracy 0.5101\n",
            "Epoch 10 Batch 3850 Loss 0.8855 Accuracy 0.5104\n",
            "Epoch 10 Batch 3900 Loss 0.8842 Accuracy 0.5108\n",
            "Epoch 10 Batch 3950 Loss 0.8827 Accuracy 0.5110\n",
            "Epoch 10 Batch 4000 Loss 0.8813 Accuracy 0.5113\n",
            "Epoch 10 Batch 4050 Loss 0.8802 Accuracy 0.5116\n",
            "Epoch 10 Batch 4100 Loss 0.8791 Accuracy 0.5118\n",
            "Epoch 10 Batch 4150 Loss 0.8788 Accuracy 0.5118\n",
            "Epoch 10 Batch 4200 Loss 0.8789 Accuracy 0.5119\n",
            "Epoch 10 Batch 4250 Loss 0.8793 Accuracy 0.5119\n",
            "Epoch 10 Batch 4300 Loss 0.8802 Accuracy 0.5117\n",
            "Epoch 10 Batch 4350 Loss 0.8814 Accuracy 0.5116\n",
            "Epoch 10 Batch 4400 Loss 0.8824 Accuracy 0.5114\n",
            "Epoch 10 Batch 4450 Loss 0.8834 Accuracy 0.5112\n",
            "Epoch 10 Batch 4500 Loss 0.8846 Accuracy 0.5110\n",
            "Epoch 10 Batch 4550 Loss 0.8859 Accuracy 0.5108\n",
            "Epoch 10 Batch 4600 Loss 0.8868 Accuracy 0.5107\n",
            "Epoch 10 Batch 4650 Loss 0.8884 Accuracy 0.5105\n",
            "Epoch 10 Batch 4700 Loss 0.8898 Accuracy 0.5103\n",
            "Epoch 10 Batch 4750 Loss 0.8912 Accuracy 0.5101\n",
            "Epoch 10 Batch 4800 Loss 0.8925 Accuracy 0.5100\n",
            "Epoch 10 Batch 4850 Loss 0.8939 Accuracy 0.5097\n",
            "Epoch 10 Batch 4900 Loss 0.8952 Accuracy 0.5096\n",
            "Epoch 10 Batch 4950 Loss 0.8967 Accuracy 0.5095\n",
            "Epoch 10 Batch 5000 Loss 0.8980 Accuracy 0.5092\n",
            "Epoch 10 Batch 5050 Loss 0.8993 Accuracy 0.5090\n",
            "Epoch 10 Batch 5100 Loss 0.9007 Accuracy 0.5088\n",
            "Epoch 10 Batch 5150 Loss 0.9020 Accuracy 0.5086\n",
            "Epoch 10 Batch 5200 Loss 0.9034 Accuracy 0.5083\n",
            "Epoch 10 Batch 5250 Loss 0.9048 Accuracy 0.5081\n",
            "Epoch 10 Batch 5300 Loss 0.9059 Accuracy 0.5077\n",
            "Epoch 10 Batch 5350 Loss 0.9071 Accuracy 0.5074\n",
            "Epoch 10 Batch 5400 Loss 0.9083 Accuracy 0.5071\n",
            "Epoch 10 Batch 5450 Loss 0.9091 Accuracy 0.5069\n",
            "Epoch 10 Batch 5500 Loss 0.9100 Accuracy 0.5066\n",
            "Epoch 10 Batch 5550 Loss 0.9109 Accuracy 0.5063\n",
            "Epoch 10 Batch 5600 Loss 0.9120 Accuracy 0.5061\n",
            "Epoch 10 Batch 5650 Loss 0.9130 Accuracy 0.5058\n",
            "Epoch 10 Batch 5700 Loss 0.9137 Accuracy 0.5055\n",
            "Saving checkpoint for epoch 10 at ./drive/My Drive/projects/transformer/ckpt/ckpt-10\n",
            "Time taken for 1 epoch: 1435.739253282547 secs\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmzyRwDrRGdq",
        "colab_type": "text"
      },
      "source": [
        "# Evaluating"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNHwJJrz3lPB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(inp_sentence):\n",
        "    inp_sentence = \\\n",
        "        [VOCAB_SIZE_EN-2] + tokenizer_en.encode(inp_sentence) + [VOCAB_SIZE_EN-1]\n",
        "    enc_input = tf.expand_dims(inp_sentence, axis=0)\n",
        "    \n",
        "    output = tf.expand_dims([VOCAB_SIZE_FR-2], axis=0)\n",
        "    \n",
        "    for _ in range(MAX_LENGTH):\n",
        "        predictions = transformer(enc_input, output, False)\n",
        "        \n",
        "        prediction = predictions[:, -1:, :]\n",
        "        \n",
        "        predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)\n",
        "        \n",
        "        if predicted_id == VOCAB_SIZE_FR-1:\n",
        "            return tf.squeeze(output, axis=0)\n",
        "        \n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "        \n",
        "    return tf.squeeze(output, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6VeFKrE6Kdx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(sentence):\n",
        "    output = evaluate(sentence).numpy()\n",
        "    \n",
        "    predicted_sentence = tokenizer_fr.decode(\n",
        "        [i for i in output if i < VOCAB_SIZE_FR-2]\n",
        "    )\n",
        "    \n",
        "    print(\"Input: {}\".format(sentence))\n",
        "    print(\"Predicted translation: {}\".format(predicted_sentence))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdoWKbCP7Czs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a0c74050-ddce-48ba-82f6-f5c0fe7dec76"
      },
      "source": [
        "translate(\"This is a really powerful tool!\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: This is a really powerful tool!\n",
            "Predicted translation: C'est un instrument vraiment puissant!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OxLbWafUTeR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}